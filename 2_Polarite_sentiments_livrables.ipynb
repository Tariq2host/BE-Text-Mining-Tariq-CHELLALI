{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU02tbLWlPQp"
      },
      "source": [
        "# __ECL__ 2023-24\n",
        "\n",
        "## BE Travail à faire \"Text Mining\"\n",
        "\n",
        "<font size=\"5\"> Analyse de Polarités +  Classification de phrases</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2H0Y0RyZY2U"
      },
      "source": [
        "**Elève : TARIQ CHELLALI**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHuGCjBhlPQr"
      },
      "source": [
        "\n",
        "- Appliquer de multiples techniques/méthodes et donner un tableau comparatif sur la base de donnée **movie_reviews**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u7CL03vjT-z"
      },
      "source": [
        "Voici un résumé  des scores des différents modèles de classification appliqué à la base **movie_reviews** :\n",
        "\n",
        "| Modèles                                                 | score |\n",
        "|--------------------------------------------------------|-------------|\n",
        "| DecisionTreeClassifier (arbre de décision)                  | 0.7291     |\n",
        "| Random Forest                 |0.8357                         |\n",
        "| Modèle linéaire (LogisticRegression)                   | 0.8379      |\n",
        "| Evaluation : ROC de RF                | AUC = 0.8379     |\n",
        "| Multinomial Bayesian (MNB)                             | 0.8338      |\n",
        "| RandomForest                                           | 0.8396      |\n",
        "| RandomForestClassifier (n_estimators=150, max_depth=90)| 0.8461      |\n",
        "| RandomForestClassifier (n_estimators=100, max_depth=70)| 0.8471      |\n",
        "| Gradient Boost | 0.5688     |\n",
        "| SVM linéaire avec OneVsRestClassifier                  | 0.8968      |\n",
        "| LogisticRegression avec données lemmatisées (bi-gramme)| 0.8987      |\n",
        "| LogisticRegression avec Cross-validation  les données de base            | 0.9162      |\n",
        "| MultinomialNB sur les données de base                  | 0.8862      |\n",
        "| MNB avec Validation Croisée (XV)                       | 0.8868      |\n",
        "| RandomForestClassifier avec SVD                        | 0.7139      |\n",
        "| LogisticRegression avec SVD                            | 0.7944      |\n",
        "| TF-IDF -> SVD -> LogisticRegression                    | 0.8769      |\n",
        "| LogisticRegression sur word2vect  (model gensim)                     | 0.6219      |\n",
        "| LogisticRegression sur word2vect (modele pré-entrainé de google)    | 0.8336      |\n",
        "| RandomForest sur word2vect (modele pré-entrainé de google)    | 0.7934      |\n",
        "| SVM sur word2vect (modele pré-entrainé de google)    | 0.8477      |\n",
        "| MLPClassifier sur word2vect (modele pré-entrainé de google)    | 0.841      |\n",
        "\n",
        "Nous remarquons que :\n",
        "\n",
        "| Description          | Modèle                                     | Performance |\n",
        "|----------------------|--------------------------------------------|-------------|\n",
        "| Performance maximale | LogisticRegression avec Cross-validation les données de base   | **0.9162**      |\n",
        "| Performance minimale | LogisticRegression sur word2vect (model gensim)          | **0.6219**      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFRNFV3GlPQs"
      },
      "source": [
        "<font size=\"5\"> Classification de phrases </font>\n",
        "En fouille de données textuelles, plus le vocabulaire est étendu, plus il faut des données !\n",
        "\n",
        "Le problème traité ici est (assez) classique en Text-Mining, on cherche à catégoriser des phrases en **Polarite positif ou négatif**. Ce pourrait être aussi classer des spams, résumer un texte, .....\n",
        "\n",
        "Le problème est ici simplifié : une donnée = **une phrase** suivie d'un **label**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFWQns0RlPQt"
      },
      "source": [
        "# 1- Les données\n",
        "\n",
        "On utilisera le jeu de données **\"Movie_reviews.csv\"**.\n",
        "\n",
        "Il faut installer (une seule fois), pour avoir ces données (et plein d'autres). **Voir la cellule ci-dessus**\n",
        "* !pip3 install papierstat.datasets\n",
        "* !pip3 install papierstat\n",
        "* !pip3 install dbfread\n",
        "* !pip3 install geopandas\n",
        "* !pip3 install pyensae\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2BpjKHilPQt"
      },
      "source": [
        "## 1.1 - Chargement des données\n",
        "\n",
        "### 1.1.1) Chargement de la BD   __Soit directement par__  pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSp1oSMulPQt",
        "outputId": "41f46172-bf10-4b9c-8a4f-ab7c3f2e30b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting papierstat\n",
            "  Downloading papierstat-0.2.408-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from papierstat) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from papierstat) (1.5.3)\n",
            "Collecting mlinsights (from papierstat)\n",
            "  Downloading mlinsights-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from papierstat) (3.7.1)\n",
            "Collecting pandas-streaming (from papierstat)\n",
            "  Downloading pandas_streaming-0.5.0-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.0->papierstat) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.0->papierstat) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.0->papierstat) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.0->papierstat) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->papierstat) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->papierstat) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->papierstat) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->papierstat) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->papierstat) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->papierstat) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->papierstat) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->papierstat) (2.8.2)\n",
            "Collecting onnx>=1.14.0 (from mlinsights->papierstat)\n",
            "  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->papierstat) (2023.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.14.0->mlinsights->papierstat) (3.20.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->papierstat) (1.16.0)\n",
            "Installing collected packages: onnx, mlinsights, pandas-streaming, papierstat\n",
            "Successfully installed mlinsights-0.5.0 onnx-1.16.0 pandas-streaming-0.5.0 papierstat-0.2.408\n",
            "Collecting pyquickhelper\n",
            "  Downloading pyquickhelper-1.12.3823-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire (from pyquickhelper)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pyquickhelper) (1.25.2)\n",
            "Collecting pyquicksetup>=0.2 (from pyquickhelper)\n",
            "  Downloading pyquicksetup-0.2.4.tar.gz (5.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyquicksetup>=0.2->pyquickhelper) (67.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->pyquickhelper) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->pyquickhelper) (2.4.0)\n",
            "Building wheels for collected packages: pyquicksetup, fire\n",
            "  Building wheel for pyquicksetup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyquicksetup: filename=pyquicksetup-0.2.4-py3-none-any.whl size=6711 sha256=3a623abde345a79dd3d0f5e70ba97626cf15d1586462c68331eebd38b7c5d117\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/be/8e/de24987fe1693d9c54ac697d5dfa8762422350c6d5732abe0f\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=a2b6a8804298abc9dc8b5b1e513987f3e3c16091f2bce33c91ed7061ced73e68\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built pyquicksetup fire\n",
            "Installing collected packages: pyquicksetup, fire, pyquickhelper\n",
            "Successfully installed fire-0.6.0 pyquickhelper-1.12.3823 pyquicksetup-0.2.4\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n",
            "Collecting dbfread\n",
            "  Downloading dbfread-2.0.7-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Downloading dbfread-2.0.7-py2.py3-none-any.whl (20 kB)\n",
            "Installing collected packages: dbfread\n",
            "Successfully installed dbfread-2.0.7\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pyensae\n",
            "  Downloading pyensae-1.3.967-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pyquickhelper>=1.8 in /usr/local/lib/python3.10/dist-packages (from pyensae) (1.12.3823)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from pyensae) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyensae) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from pyensae) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->pyensae) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->pyensae) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->pyensae) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->pyensae) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->pyensae) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->pyensae) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->pyensae) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->pyensae) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->pyensae) (2023.4)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from pyquickhelper>=1.8->pyensae) (0.6.0)\n",
            "Requirement already satisfied: pyquicksetup>=0.2 in /usr/local/lib/python3.10/dist-packages (from pyquickhelper>=1.8->pyensae) (0.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyquicksetup>=0.2->pyquickhelper>=1.8->pyensae) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->pyensae) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->pyquickhelper>=1.8->pyensae) (2.4.0)\n",
            "Downloading pyensae-1.3.967-py3-none-any.whl (493 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.1/493.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyensae\n",
            "Successfully installed pyensae-1.3.967\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "if True : # déjà fait ! Pour refaire, remplacer \"False\" par \"True\"\n",
        "    !pip install papierstat\n",
        "    !pip install pyquickhelper\n",
        "    !pip install --upgrade pip\n",
        "    !pip install dbfread\n",
        "    !pip install pyensae\n",
        "    !pip install chardet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_bhRD9rlPQt"
      },
      "source": [
        "### 1.1.2) Chargement de la BD :\n",
        "\n",
        "Nous allons charger la base de données **movie_reviews.csv**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Gh2XuwhDlPQu",
        "outputId": "f338bd13-4f6f-41f7-c11a-a6c441f534ee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "# csv_txt=\"/content/drive/MyDrive/BE_apprentissage_bayesien/movie_reviews.csv\"\n",
        "csv_txt=\"./data/movie_reviews.csv\"\n",
        "df_movies= pd.read_csv(csv_txt, header=0)\n",
        "df_movies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIcoVaBRlPQu"
      },
      "source": [
        "## 1.2- Préparation des données     \n",
        "### 1.2.1) Transformer la colone sentiment\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaEH1B5E5_N-",
        "outputId": "492fb98c-aa53-4460-c4bd-a3dbd1f1b331"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "review       object\n",
              "sentiment    object\n",
              "dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Displaying the columns and their data types\n",
        "df_movies.dtypes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous remarquons sue la colonne **sentiment** est de type texte et prend deux valeurs  : *positif* ou *négatif*. Nous allons transformer cette colonne de telle sorte que: \n",
        "- positive ---> 1\n",
        "- négative ---> 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IXXCS_-ui_RU",
        "outputId": "bbd75fdb-f244-4c01-a74f-ae75f01d02ac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  One of the other reviewers has mentioned that ...          1\n",
              "1  A wonderful little production. <br /><br />The...          1\n",
              "2  I thought this was a wonderful way to spend ti...          1\n",
              "3  Basically there's a family where a little boy ...          0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Converting the sentiment to a single column with 1 for 'positive' and 0 for 'negative'\n",
        "df_movies['sentiment'] = df_movies['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "\n",
        "df_movies.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKhaEg5ClPQv"
      },
      "source": [
        "Vérifions que nous avon sbien 2 colonnes :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEWOVItKlPQv",
        "outputId": "7c1dfd79-1371-4df6-ef10-08e82987086f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_movies.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg3BaLs_lPQv"
      },
      "source": [
        "### 1.2.2) Renommage  des colonnes (attributs)   \n",
        "Pour plus de clarté, on **renomme** les colonnes des données en 'Avis' et 'Polarite'\n",
        "* __Avis__ est l'avis exprimé\n",
        "* __Polarite__ est l'opinion (sentiment) (0 ou 1 : négatif ou positif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "FE0yWSunlPQv",
        "outputId": "12639804-5f62-4e05-dc99-6a29138c9861"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Avis</th>\n",
              "      <th>Polarite</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Avis  Polarite\n",
              "0      One of the other reviewers has mentioned that ...         1\n",
              "1      A wonderful little production. <br /><br />The...         1\n",
              "2      I thought this was a wonderful way to spend ti...         1\n",
              "3      Basically there's a family where a little boy ...         0\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...         1\n",
              "...                                                  ...       ...\n",
              "49995  I thought this movie did a down right good job...         1\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...         0\n",
              "49997  I am a Catholic taught in parochial elementary...         0\n",
              "49998  I'm going to have to disagree with the previou...         0\n",
              "49999  No one expects the Star Trek movies to be high...         0\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_movies=df_movies.rename(columns = {'review': 'Avis', 'sentiment' : 'Polarite'})\n",
        "df_movies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNr2RKKYlPQv"
      },
      "source": [
        "__Info :__ Les attributs (les colonnes de la matrice) sous 2 formes :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpsZt0JblPQv",
        "outputId": "92786c9d-4eab-4790-e5bf-d0e4b0fb1a3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Avis', 'Polarite'], dtype=object)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Les attributs (colonnes)\n",
        "df_movies.columns.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBTIbcUolPQv",
        "outputId": "2b58d8ba-2983-4a65-d782-0b7ee981cef5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Avis', 'Polarite'], dtype='object')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_movies.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2YVvE7blPQv"
      },
      "source": [
        "__Info : Combien de chaque polarité__ ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "hhCmH09KlPQv",
        "outputId": "4de4143a-905a-44a4-ee43-121827a57233"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Avis</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Polarite</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Avis\n",
              "Polarite       \n",
              "0         25000\n",
              "1         25000"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Le bon groupement\n",
        "df_movies.groupby(['Polarite']).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-g_qSa3lPQv"
      },
      "source": [
        "# 2- Vers l'analyse des \"avis\" (*sentiments*)\n",
        "## 2.1- Répartition des données pour l'analyse\n",
        "On les divise en 2 ensembles d'__apprentissage__ et de __test__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf-Ohmp7lPQw"
      },
      "source": [
        "__On découpe en train and test : par défaut 80% et 20%__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C5eFpyuhlPQw"
      },
      "outputs": [],
      "source": [
        "### Pour changer les proportions par défaut et avoir par exemple 80% 20%\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        train_test_split(df_movies[[\"Avis\"]], df_movies['Polarite'], train_size=0.80, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpop8Jx-lPQw"
      },
      "source": [
        "__Combien dans chaque paquet train / test ?__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSJuOuc0lPQw",
        "outputId": "2654607d-5a43-48ad-8d5c-808f17181878"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(40000, 1)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape  # les dimensions : (nb_lignes, nb_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyZvEXfMlPQw",
        "outputId": "45bfa7fd-8f44-4385-ecc9-e49370d49b38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO1lMfYllPQw"
      },
      "source": [
        "**Text Mining : l'approche classique (vector space)**      \n",
        "    \n",
        "<font color=\"red\"> TfIdf </font> est une méthode de convertion ses __mots__  en attributs __numériques__ (*features du vector space*).\n",
        "* `Tf(terme)` : *term fréquency* (pour un mot/terme) = la fréquence d'un terme dans le document\n",
        "* `Idf(terme)` : *Inverse document Frequency* (pour un mot/terme) = log(nombre total de documents __divisé__ par le nombre de documents où ce terme  apparaît;\n",
        "* `TfIdf(terme)` : $TF(terme) \\times Idf(terme)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8tcwsGUlPQ0"
      },
      "source": [
        "## 2.2- Calcul de la matrice numérique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyI8UhF9lPQ1"
      },
      "source": [
        "#### 2.2.2 : Stemming et Lemmatization  de notre corpus\n",
        "\n",
        "**on filtre les mots (lemmatization) pour ne garder que les mots significatifs, ensuite on calcule la matrice TfIdf**\n",
        " \n",
        "On importe d'abord quelques outils."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk_lfYNplPQ1"
      },
      "source": [
        "Importation du nécessaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDY5TNNhlPQ1",
        "outputId": "ca644d4d-b5c3-4764-8b94-3fae2468fd74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ],
      "source": [
        "if True : # SI déjà fait, ne pas refaire\n",
        "    !pip install textblob\n",
        "    import nltk\n",
        "    nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Y3AQgrlPQ1",
        "outputId": "e940dd4b-995e-4f51-e6f3-db5ddee80507"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('reuters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0MWTDCXlPQ1",
        "outputId": "6783438d-47ee-4754-a74f-f7d25461ea88"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "\n",
        "# Une seule fois :\n",
        "if True : # Si déjà fit\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('words')\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('brown')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS76BUGnlPQ2",
        "outputId": "52806c51-ae75-40b8-cee4-ca8fd3fc8d6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bat\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "import string\n",
        "\n",
        "# Initialisation du \"Wordnet Lemmatizer\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"bats\"))\n",
        "\n",
        "\n",
        "from nltk.corpus import brown  # Il y a davantage de mots ici\n",
        "words = set(brown.words())\n",
        "\n",
        "#words = set(nltk.corpus.words.words()) : pas beaucoup de mots !\n",
        "stop_words=set(stopwords.words('english')); # \";\" pour ne pas avoir les résultats !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3KQo9IQlPQ2"
      },
      "source": [
        "### 2.2.2.1 Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yxx5FKKlPQ2"
      },
      "source": [
        "__On définit notre lemmatizer qui pré-traitera  le corpus!__\n",
        "1) vérifier que nos \"mots\" sont parmi les mots acceptables (non \"usuels\") : dans \"words\"\n",
        "2) enlever les mots usuels (stopwords)\n",
        "3) enlever qq mots étranges\n",
        "4) enlever les ponctuations\n",
        "5) etc.\n",
        "6) Lemmatiser !    \n",
        "\n",
        "N.B. 'le' et 'u' sont mis \"à la main\" !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MT8pIV1TlPQ2"
      },
      "outputs": [],
      "source": [
        "def lemma(texte) :\n",
        "    #renvoie lemmatizer.lemmatize(texte)\n",
        "    return [lemmatizer.lemmatize(t) for t in word_tokenize(texte) if \\\n",
        "            t.lower() in words and \\\n",
        "            t.lower() not in stop_words \\\n",
        "            # cas des strs spécifiques non filtrés\n",
        "            and t not in [\"''\", '--', '1.2', '1/2', '18th', '2-3', '20th', '4.00', '4.2', '``', 'le', 'u', 'la']\\\n",
        "            #and t.lower() not in word_tokenize(stop_words).encode() \\ # génère un pb de 'byte' ?!\n",
        "            and t.lower() not in string.punctuation and not t.isdigit()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndTWMBRMlPQ2"
      },
      "source": [
        "\n",
        "<font color=\"red\"> Ici : si warning sur stopwords, lancer cette cellule une 2e fois </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPZ4Zp-VlPQ2"
      },
      "source": [
        "__Appliquer puis afficher quelques informations sont le corpus restant__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A66mCy7KlPQ2",
        "outputId": "596530f7-a75a-4c7e-caa2-9c80ff969783"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'u'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Il y a  22970  termes dans le vocabulaire\n",
            "\n",
            "Les 50 premiers termes:\n",
            "[\"'20s\" \"'30s\" \"'38\" \"'48\" \"'50\" \"'51\" \"'53\" \"'54\" \"'55\" \"'58\" \"'60\"\n",
            " \"'60s\" \"'61\" \"'76\" \"'90s\" \"'bout\" \"'em\" \"'fess\" \"'im\" \"'low\" \"'most\"\n",
            " \"'nother\" \"'nuff\" \"'pache\" \"'round\" \"'till\" \"'way\" '-ing' '-ism' '0.1'\n",
            " '0.10' '0.2' '0.5' '1,000' '1,000,000' '1,400' '1,500' '1,600' '1,700'\n",
            " '1-0' '1-1/2' '1-3' '1.0' '1.00' '1.07' '1.1' '1.25' '1.5' '1.8' '1/3']\n",
            "\n",
            "Tous les 50 termes:\n",
            "[\"'20s\" '1/4' '1950s' '26-year-old' '4.0' '6.3' 'aback' 'abrupt'\n",
            " 'accentuated' 'accounting' 'acreage' 'additionally' 'admitted'\n",
            " 'advertisement' 'afflicted' 'agin' 'airport' 'all-important' 'ally'\n",
            " 'ambiance' 'amusing' 'anguish' 'anterior' 'apathetic' 'applicable'\n",
            " 'approximation' 'aristocracy' 'arterial' 'aspen' 'associate' 'atomic'\n",
            " 'attribute' 'authorizes' 'avocation' 'baby-sitter' 'balanced'\n",
            " 'bankruptcy' 'bartender' 'bawling' 'bedlam' 'behaviorally' 'benefactor'\n",
            " 'better-than-average' 'billing' 'blackened' 'blessing' 'bloodshot' 'boar'\n",
            " 'bolting' 'bootlegging' 'bounty' 'brandishing' 'breathy' 'brisk'\n",
            " 'brownish' 'buggy' 'buoyed' 'butchered' 'cadet' 'camouflage' 'canvas'\n",
            " 'careening' 'carving' 'cathedral' 'cellist' 'chagrin' 'characterizes'\n",
            " 'checklist' 'chili' 'choreographic' 'circle' 'clamp' 'cleanly' 'clip'\n",
            " 'clucking' 'cobweb' 'coiled' 'colonist' 'command' 'commonly' 'compete'\n",
            " 'component' 'conceivable' 'condensation' 'confining' 'conjured'\n",
            " 'considered' 'constricting' 'contemplative' 'contradict' 'convert' 'cop'\n",
            " 'correctly' 'counseled' 'courtyard' 'cranky' 'creditor' 'critter'\n",
            " 'cruelest' 'cultivating' 'custodial' 'damaged' 'darned' 'death-wish'\n",
            " 'decisiveness' 'deep' 'definite' 'delineated' 'demoralized' 'depending'\n",
            " 'derivative' 'despairingly' 'determine' 'devotedly' 'dictionary'\n",
            " 'diligence' 'dirge' 'discernment' 'discriminating' 'disliked' 'displace'\n",
            " 'disservice' 'distrust' 'doan' 'dominated' 'doubt' 'dram' 'dresser'\n",
            " 'drugged' 'duly' 'dynasty' 'eat' 'educator' 'elaborate' 'elevated'\n",
            " 'embellished' 'emphasizing' 'encourage' 'enforce' 'enormity' 'entirety'\n",
            " 'epiphany' 'err' 'esteemed' 'eve' 'exacted' 'exciting' 'exhaustive'\n",
            " 'expedient' 'exploiting' 'extent' 'eye-to-eye' 'failing' 'fanatic'\n",
            " 'fastened' 'feasibility' 'fender' 'fielded' 'filth' 'first-aid'\n",
            " 'flammable' 'flesh' 'floundered' 'focussed' 'footnote' 'foresight'\n",
            " 'forsakes' 'fourteenth' 'freedom-loving' 'frivolous' 'full-grown'\n",
            " 'furthered' 'gallery' 'gasoline' 'generator' 'gherkin' 'glamour'\n",
            " 'glorify' 'goin' 'governs' 'granting' 'grayer' 'grin' 'grower' 'guitar'\n",
            " 'h' 'half-inch' 'handgun' 'hard-bitten' 'harvested' 'head-on' 'heated'\n",
            " 'helmsman' 'hesitated' 'highland' 'hit-and-miss' 'homeland' 'hooted'\n",
            " 'hot-blooded' 'hugged' 'hurdle' 'i.e.' 'ignorant' 'imbibe' 'impartial'\n",
            " 'implementing' 'improve' 'incapacitated' 'incomprehension'\n",
            " 'indecisiveness' 'individuality' 'inexact' 'inflated' 'inheritance'\n",
            " 'inquest' 'insoluble' 'instructs' 'intend' 'interlacing' 'intertwined'\n",
            " 'introverted' 'investor' 'irredeemably' 'jabbed' 'jewelry' 'joyfully'\n",
            " 'juvenile' 'kimono' 'knott' 'lagged' 'lark' 'law-breaking' 'learns'\n",
            " 'legitimized' 'levity' 'lifelong' 'limousine' 'listener' 'loathed'\n",
            " 'lonely' 'loosest' 'low-key' 'lumped' 'mackintosh' 'maintain' 'mana'\n",
            " 'mansion' 'marksmanship' 'masterful' 'mauling' 'median' 'memorable'\n",
            " 'merited' 'microscope' 'militant' 'minstrel' 'misplacing' 'mobilize'\n",
            " 'mold' 'monster' 'mosquito' 'movement' 'mumble' 'mute' 'napkin' 'naval'\n",
            " 'neglect' 'newer' 'ninetieth' 'non-conformists' 'northwest' 'nuance' 'ob'\n",
            " 'observer' 'octopus' 'oiled' 'one-room' 'opportune' 'orderly'\n",
            " 'ostentatious' 'outlived' 'overcame' 'overpriced' 'ox' 'painstakingly'\n",
            " 'panther' 'parceled' 'participation' 'patent' 'pavilion' 'pee-wee'\n",
            " 'peptide' 'perjury' 'personifies' 'pfennig' 'pick-up' 'pin-point'\n",
            " 'pitiable' 'plastered' 'pliable' 'poetically' 'politico' 'populous'\n",
            " 'postal' 'powerfully' 'precedes' 'prefer' 'present-day' 'prevailed'\n",
            " 'priority' 'proctor' 'program' 'prompted' 'prosaic' 'proved' 'psychiatry'\n",
            " 'pulpit' 'purged' 'pvt' 'questioned' 'racial' 'rainfall' 'rape'\n",
            " 'ravenous' 'readily' 'reasoning' 'recess' 'reconstruct' 'redeem'\n",
            " 'refilled' 'regaled' 'reimbursement' 'release' 'remarkably' 'renewal'\n",
            " 'replace' 'repudiation' 'residing' 'responding' 'retailer' 'reunion'\n",
            " 'revolt' 'ride' 'rioting' 'robe' 'roommate' 'routine' 'rummaged'\n",
            " 'sacrifice' 'saliva' 'sarcastic' 'sawmill' 'scenario' 'scoffing' 'screen'\n",
            " 'search' 'sedan' 'self-awareness' 'self-sufficient' 'sensitively'\n",
            " 'served' 'sexual' 'shared' 'shielded' 'shooing' 'showering' 'shutter'\n",
            " 'significance' 'simultaneously' 'six-foot' 'skipper' 'sleeping' 'slower'\n",
            " 'smithy' 'sneered' 'sobbed' 'softly' 'sombre' 'souffle' 'sparing'\n",
            " 'spectator' 'spinning' 'sporadic' 'spurt' 'stability' 'stance'\n",
            " 'statesmanship' 'steer' 'stilt' 'stooped' 'strange' 'stricture' 'stubbed'\n",
            " 'stylization' 'subsequently' 'successful' 'suitability' 'sunshine'\n",
            " 'supplier' 'surrendering' 'swanky' 'swindling' 'symptom' 'tactlessness'\n",
            " 'tank' 'taxiing' 'teething' 'tenant' 'terrified' 'theme' 'thinner'\n",
            " 'three-hour' 'thunderclap' 'timetable' 'tobacco' 'top-notch' 'touching'\n",
            " 'traffic' 'transferring' 'trapezoid' 'trespass' 'troika' 'truthfully'\n",
            " 'turntable' 'two-disc' 'ultra-modern' 'unawareness' 'unconnected'\n",
            " 'underline' 'undeveloped' 'unexplored' 'unhurried' 'unjustifiable'\n",
            " 'unobtainable' 'unrecognizable' 'unsolved' 'unwholesome' 'upstream'\n",
            " 'utilize' 'valuable' 'vehement' 'verse' 'view' 'virus' 'voiced' 'wacky'\n",
            " 'wan' 'warship' 'wax' 'week-old' 'well-organized' 'whiplash' 'wicket'\n",
            " 'wilt' 'wish' 'wondrously' 'worse' 'wretchedness' 'year-long' 'zenith']\n",
            "\n",
            "Le nbr d'occurrence des 50 premiers termes :\n",
            "[('reviewer', 17038), ('mentioned', 12575), ('watching', 22379), ('episode', 7051), ('hooked', 9844), ('right', 17120), ('exactly', 7252), ('happened', 9383), ('thing', 20496), ('struck', 19587), ('brutality', 2712), ('scene', 17603), ('violence', 22136), ('set', 18012), ('word', 22767), ('trust', 21094), ('faint', 7603), ('timid', 20654), ('pull', 15797), ('punch', 15815), ('regard', 16504), ('drug', 6399), ('sex', 18049), ('classic', 3576), ('use', 21881), ('called', 2926), ('nickname', 13376), ('given', 8789), ('maximum', 12460), ('security', 17800), ('state', 19292), ('focus', 8147), ('mainly', 12199), ('emerald', 6769), ('city', 3526), ('experimental', 7417), ('section', 17790), ('prison', 15453), ('cell', 3198), ('glass', 8811), ('face', 7571), ('privacy', 15456), ('high', 9677), ('agenda', 734), ('em', 6731), ('home', 9797), ('death', 5148), ('stare', 19269), ('dealing', 5141), ('shady', 18071)]\n",
            "\n",
            "Le nbr d'occurrence des 50 premiers termes :\n",
            "[('geometrically', 8735), ('hoydenish', 9936), ('typhoid', 21225), ('photochemical', 14727), ('basting', 1872), ('badgering', 1724), ('stationmaster', 19309), ('outfox', 13988), ('baseman', 1856), ('persiflage', 14636), ('mg.', 12647), ('unconquerable', 21354), ('memorandum', 12552), ('scalded', 17565), ('reprobate', 16792), ('winder', 22665), ('hesitatingly', 9654), ('mm.', 12845), ('boyars', 2520), ('probate', 15472), ('broil', 2673), ('cashew', 3106), ('blue-black', 2323), ('seafarer', 17739), ('electromagnetism', 6683), ('reelected', 16433), ('underarm', 21379), ('seaweed', 17766), ('eventfully', 7205), ('headsman', 9516), ('calibration', 2923), ('godhead', 8895), ('troopship', 21058), ('admiringly', 594), ('cutoff', 5012), ('squeamishness', 19186), ('compensatory', 3999), ('cobalt', 3747), ('discriminatory', 5902), ('limbic', 11792), ('telephoning', 20312), ('inversely', 11036), ('unawareness', 21303), (\"'fess\", 17), ('helpfulness', 9607), ('clear-headed', 3605), ('exalting', 7260), ('big-chested', 2133), ('censorial', 3209), ('clatter', 3587)]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "count_vec_lemmatise = CountVectorizer(tokenizer=lemma, stop_words=\"english\", analyzer='word',\n",
        "                            ngram_range=(1, 1), max_df=1.0, min_df=0.0, max_features=None)\n",
        "\n",
        "# Transformer les données en  bag of words\n",
        "count_train = count_vec_lemmatise.fit(df_movies[\"Avis\"])\n",
        "\n",
        "bag_of_words_of_corpus = count_vec_lemmatise.transform(df_movies[\"Avis\"])\n",
        "\n",
        "# On enlève qq termes inutiles qui nous ont échappés (qui ont été créés par lemmatize)\n",
        "# Il s'agit d'un Dict de Python.\n",
        "for terme_a_jeter in ['n', 'u', 'ft'] :\n",
        "    count_train.vocabulary_.pop(terme_a_jeter, terme_a_jeter+\" n'y est ps !\")\n",
        "\n",
        "\n",
        "# Quelques prints\n",
        "print(\"Il y a \", len(count_train.vocabulary_), \" termes dans le vocabulaire\\n\")\n",
        "\n",
        "# Print Les 50 premiers termes\n",
        "print(\"Les 50 premiers termes:\\n{}\".format(np.array(count_vec_lemmatise.get_feature_names_out()[:50])))\n",
        "print(\"\\nTous les 50 termes:\\n{}\".format(np.array(count_vec_lemmatise.get_feature_names_out()[::50]))) # Tous les 50 termes\n",
        "#print(type(count_train.vocabulary_))\n",
        "#print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))\n",
        "\n",
        "#les 50 premiers mots et leur nbr d'occurrence\n",
        "print(\"\\nLe nbr d'occurrence des 50 premiers termes :\")\n",
        "print([(k,v)  for k,v in count_train.vocabulary_.items()][:50])\n",
        "\n",
        "print(\"\\nLe nbr d'occurrence des 50 premiers termes :\")\n",
        "print([(k,v)  for k,v in count_train.vocabulary_.items()][-50:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFT8TjdZUDiV",
        "outputId": "79a8f2d9-b1fe-4833-a068-d51609ed7546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Collecting backports.csv (from pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting mysqlclient (from pattern)\n",
            "  Downloading mysqlclient-2.2.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.4)\n",
            "Collecting feedparser (from pattern)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pdfminer.six (from pattern)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Collecting python-docx (from pattern)\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting cherrypy (from pattern)\n",
            "  Downloading CherryPy-18.9.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
            "  Downloading cheroot-10.0.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting portend>=2.1.1 (from cherrypy->pattern)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern)\n",
            "  Downloading jaraco.collections-5.0.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting sgmllib3k (from feedparser->pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (42.0.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx->pattern) (4.10.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2024.2.2)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
            "  Downloading jaraco.functools-4.0.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
            "  Downloading tempora-5.5.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.text-3.12.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2023.4)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.context-4.3.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.6.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (2.16.3)\n",
            "Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Downloading CherryPy-18.9.0-py3-none-any.whl (348 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.8/348.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cheroot-10.0.0-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading jaraco.collections-5.0.0-py3-none-any.whl (10 kB)\n",
            "Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Downloading tempora-5.5.1-py3-none-any.whl (13 kB)\n",
            "Downloading jaraco.functools-4.0.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jaraco.text-3.12.0-py3-none-any.whl (11 kB)\n",
            "Downloading jaraco.context-4.3.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: pattern, mysqlclient, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=4b4603dbc3199b90b5f7a8a8f837e71aecd261a40d658b7beeefd58820f89ef8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.4-cp310-cp310-linux_x86_64.whl size=124731 sha256=980726d1cd29da03520fe3806b40c346dfa52bdd8b51e1f544cb47e0a16262f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/96/ac/2a4d8cb58a4d95de1dffc3f8b0ea42e0e5b63ab97640edbda3\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=82c68d04ee2b538aedd2fcc59d1e404f42b740fb8d1ad858b7bc934cbf578b1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built pattern mysqlclient sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, jaraco.context, feedparser, autocommand, tempora, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 cheroot-10.0.0 cherrypy-18.9.0 feedparser-6.0.11 jaraco.collections-5.0.0 jaraco.context-4.3.0 jaraco.functools-4.0.0 jaraco.text-3.12.0 mysqlclient-2.2.4 pattern-3.6 pdfminer.six-20231228 portend-3.2.0 python-docx-1.1.0 sgmllib3k-1.0.0 tempora-5.5.1 zc.lockfile-3.0.post1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deuxième méthode (lametization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C0moAvXQ8fY",
        "outputId": "7c092e20-6255-4c65-b9e4-eebe39a5cba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Les 50 premiers termes:\n",
            "['aa' 'aardvark' 'ab' 'aba' 'aback' 'abandon' 'abandonment' 'abash'\n",
            " 'abatement' 'abba' 'abbasi' 'abbey' 'abbot' 'abbreviate' 'abdomen'\n",
            " 'abdominal' 'abduct' 'abduction' 'abductor' 'aberrant' 'aberration'\n",
            " 'abeyance' 'abhor' 'abhorrence' 'abhorrent' 'abide' 'abigail' 'ability'\n",
            " 'abject' 'abjectly' 'ablaze' 'able' 'abler' 'ablest' 'ably' 'abnegation'\n",
            " 'abnormal' 'abnormality' 'abnormally' 'aboard' 'abolish' 'abolition'\n",
            " 'abolitionism' 'abolitionist' 'abominable' 'abominably' 'abomination'\n",
            " 'abominator' 'aboriginal' 'aborigine']\n",
            "\n",
            "Tous les 50 termes:\n",
            "['aa' 'abort' 'abysmal' 'accost' 'acquittal' 'adapter' 'admission'\n",
            " 'adversely' 'affinity' 'aggravatingly' 'aimlessnes' 'alcohol' 'allot'\n",
            " 'alum' 'amendment' 'amusedly' 'ancillary' 'anna' 'anthropic' 'anythe'\n",
            " 'appease' 'approximation' 'architecturally' 'armpit' 'artificially'\n",
            " 'aspirin' 'astor' 'atrium' 'audio' 'autocracy' 'avow' 'babu' 'bag'\n",
            " 'balloonist' 'bannister' 'barmaid' 'basi' 'bawl' 'becker' 'behalf'\n",
            " 'bemusedly' 'berth' 'bice' 'bimodal' 'bison' 'blameles' 'blindnes'\n",
            " 'blossom' 'boardman' 'bolero' 'booklet' 'borough' 'bourgeoise' 'brake'\n",
            " 'breath' 'brighten' 'bronchiti' 'bubbler' 'bulldoze' 'burbank'\n",
            " 'businessman' 'cabin' 'calculu' 'cancan' 'canvass' 'card' 'caroon'\n",
            " 'castanet' 'cathexi' 'ceil' 'cerebrate' 'champ' 'charger' 'cheek'\n",
            " 'chiasmu' 'chiropractor' 'chorus' 'cinematographical' 'clairvoyance'\n",
            " 'cleanse' 'clinic' 'cluck' 'coco' 'coinage' 'collin' 'combust'\n",
            " 'commercially' 'compare' 'complicatedly' 'conceitedly' 'condensation'\n",
            " 'conflate' 'connection' 'consign' 'consult' 'continental' 'controller'\n",
            " 'cookery' 'cordially' 'correlative' 'cotter' 'course' 'cracky' 'creak'\n",
            " 'cretaceou' 'crochet' 'crudenes' 'cucumber' 'curie' 'cuti' 'daftnes'\n",
            " 'dang' 'davenport' 'debase' 'decipher' 'deerskin' 'deflower' 'delicacy'\n",
            " 'demobilization' 'dent' 'derail' 'design' 'detect' 'device' 'dialectical'\n",
            " 'dietetic' 'dimension' 'directnes' 'discard' 'discuss' 'disjunct'\n",
            " 'displace' 'dissocial' 'disturbingly' 'dockside' 'doli' 'doorpost'\n",
            " 'downer' 'dramaturgical' 'drinkable' 'dryly' 'dun' 'dyer' 'ease' 'eddy'\n",
            " 'effusively' 'elder' 'ellipse' 'embraceable' 'employ' 'endanger'\n",
            " 'enlarge' 'enthusiast' 'enzyme' 'equity' 'escapist' 'ethnical' 'eventles'\n",
            " 'exasperate' 'execrably' 'exorcism' 'explainable' 'expressionles'\n",
            " 'extrapolate' 'facetiousnes' 'fallback' 'far' 'fatherland' 'fecal'\n",
            " 'fennec' 'fiance' 'figurehead' 'finisher' 'fistula' 'flank' 'flesh'\n",
            " 'floorwalker' 'fluorescent' 'fomentation' 'forearm' 'forgery' 'fortunate'\n",
            " 'frame' 'freewill' 'frivolity' 'fulcrum' 'furnish' 'gain' 'gangly'\n",
            " 'gassy' 'gena' 'gentlemanlike' 'ghost' 'girdler' 'gleefulnes' 'gnarl'\n",
            " 'goldfish' 'gosh' 'gram' 'grasp' 'greave' 'grinder' 'grovel' 'guileful'\n",
            " 'gush' 'haggle' 'hamburger' 'hansel' 'harmlessly' 'hatles' 'headlong'\n",
            " 'heave' 'hellhole' 'hermaphroditic' 'hideaway' 'hippo' 'hoggish'\n",
            " 'homeward' 'hooligan' 'horrify' 'houseful' 'hulu' 'hungrily' 'hygiene'\n",
            " 'iceberg' 'idiotically' 'illusionist' 'immersion' 'impecuniou' 'implode'\n",
            " 'improbably' 'inartistic' 'incisive' 'inconspicuou' 'indecisivenes'\n",
            " 'indiscoverable' 'ineffectual' 'infancy' 'influence' 'inheritor'\n",
            " 'inoculate' 'insincere' 'institutionalization' 'intelligently'\n",
            " 'interferingly' 'interrupt' 'introspectively' 'invincible' 'irredeemable'\n",
            " 'isolationism' 'jami' 'jerk' 'jog' 'judiciou' 'kabuki' 'ken' 'kilometer'\n",
            " 'kitchenette' 'koko' 'laconic' 'lamplight' 'larva' 'launcher' 'leader'\n",
            " 'leftover' 'lenient' 'lewd' 'lieve' 'lime' 'lion' 'liturgy' 'locksmith'\n",
            " 'looker' 'loutish' 'luger' 'lustful' 'macroscopic' 'magnum' 'malevolent'\n",
            " 'maneuverability' 'mansion' 'marinate' 'mascara' 'matchbox' 'mawkishnes'\n",
            " 'meddle' 'mellow' 'mentionable' 'messines' 'michigan' 'mildew' 'mindles'\n",
            " 'misadventure' 'misjudge' 'mistrustful' 'modify' 'monetary' 'monsignor'\n",
            " 'morality' 'moth' 'mouthy' 'multidirectional' 'muser' 'myrtle' 'nap'\n",
            " 'naturalistic' 'neck' 'nephew' 'newsreader' 'ninny' 'non' 'northbound'\n",
            " 'novelization' 'nut' 'objectively' 'obsolete' 'octogenarian' 'oldish'\n",
            " 'opacity' 'optimistic' 'organize' 'otherworld' 'outmode' 'overall'\n",
            " 'overlong' 'overtop' 'packer' 'paleolithic' 'pansexual' 'param'\n",
            " 'parochial' 'passive' 'patine' 'peace' 'pedicab' 'peni' 'perdition'\n",
            " 'permanent' 'perspective' 'petroleum' 'phobic' 'picket' 'pillage' 'pipe'\n",
            " 'placement' 'platinum' 'plesiosaur' 'poacher' 'polemical' 'pom' 'popper'\n",
            " 'portray' 'potency' 'pragmatic' 'precociously' 'prejudicial' 'preset'\n",
            " 'prick' 'prissy' 'produce' 'prohibition' 'propagate' 'prostitution'\n",
            " 'provocatively' 'psychogenic' 'pug' 'puppetmaster' 'putrescent'\n",
            " 'quantitative' 'quilly' 'racialism' 'railbird' 'random' 'ration'\n",
            " 'reactor' 'reawaken' 'recitative' 'recruit' 'redraw' 'refrain' 'regular'\n",
            " 'relapse' 'reluctantly' 'renegade' 'replacement' 'repulsive' 'resilient'\n",
            " 'restful' 'retool' 'reversal' 'rheumy' 'right' 'risible' 'rockslide'\n",
            " 'roomy' 'rouse' 'ruffle' 'ruthlessly' 'safely' 'salon' 'sandwich'\n",
            " 'satiate' 'sawyer' 'scary' 'schoolmate' 'scramble' 'scrupulou' 'seasick'\n",
            " 'seducer' 'sellable' 'sensory' 'serf' 'severance' 'sham' 'shear'\n",
            " 'shingle' 'shoppe' 'shrine' 'sidesplitter' 'silo' 'singlehand' 'skeet'\n",
            " 'skullcap' 'sleek' 'slobbery' 'smarm' 'smut' 'snivele' 'soberingly'\n",
            " 'solarization' 'somnambulant' 'sorry' 'spaceship' 'speciality' 'spew'\n",
            " 'spiritualistic' 'spool' 'spunky' 'st' 'stampede' 'stasi' 'steam'\n",
            " 'sternly' 'stint' 'story' 'strayer' 'strom' 'stupendou' 'subjectivity'\n",
            " 'substantially' 'sucker' 'sulphuric' 'superabundant' 'supper' 'surplu'\n",
            " 'swank' 'swindler' 'symmetry' 'syrupy' 'taj' 'tangentially' 'tartan'\n",
            " 'tch' 'tee' 'templar' 'tentatively' 'testament' 'thema' 'thievery'\n",
            " 'threepenny' 'thunderou' 'tiller' 'tire' 'toilet' 'tooth' 'torture'\n",
            " 'tower' 'trainable' 'transient' 'traumatize' 'trendle' 'trio'\n",
            " 'troublemaker' 'try' 'turbine' 'tweaker' 'typist' 'unabate' 'unappeal'\n",
            " 'unbelievability' 'uncharacteristically' 'unconquer' 'undaunt'\n",
            " 'undermine' 'undiscern' 'unendurable' 'unfail' 'unfortunately' 'unhelp'\n",
            " 'uninitiat' 'unkind' 'unmate' 'unoppose' 'unprofessionalism' 'unrehearse'\n",
            " 'unsatisfactorily' 'unsolvable' 'unsurprise' 'untrust' 'unwinnable'\n",
            " 'uptown' 'uta' 'valid' 'vasectomy' 'venison' 'vertebrae' 'victimize'\n",
            " 'vindictively' 'visible' 'vogue' 'vulturine' 'wallflower' 'warn'\n",
            " 'waterfall' 'weather' 'welterweight' 'whet' 'whizz' 'wildfire' 'winkle'\n",
            " 'witles' 'wood' 'workout' 'wrench' 'ya' 'yew' 'zat']\n",
            "\n",
            "Le nbr d'occurrence des 50 premiers termes :\n",
            "[('watch', 26138), ('episode', 7668), ('hook', 10894), ('right', 19550), ('exactly', 7888), ('thing', 23405), ('strike', 22282), ('brutality', 2838), ('unflinch', 24831), ('violence', 25863), ('set', 20582), ('word', 26583), ('trust', 24138), ('faint', 8229), ('heart', 10526), ('timid', 23565), ('sex', 20607), ('classic', 3927), ('use', 25530), ('nickname', 15417), ('maximum', 14206), ('security', 20387), ('state', 22001), ('mainly', 13866), ('emerald', 7406), ('city', 3889), ('experimental', 8038), ('section', 20378), ('prison', 17897), ('glass', 9687), ('face', 8194), ('inward', 12369), ('privacy', 17902), ('high', 10710), ('agenda', 445), ('em', 7372), ('home', 10832), ('death', 5642), ('dodgy', 6721), ('shady', 20628), ('far', 8300), ('say', 20104), ('main', 13865), ('appeal', 996), ('fact', 8208), ('dare', 5565), ('forget', 9001), ('pretty', 17832), ('paint', 16330), ('charm', 3608)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "if True : # Mettre 3false\" pour ne pas afficher des messages d'erreur\n",
        "    # Il y a des problèmes avec la ligne suivante sur certaine machines\n",
        "    from pattern.en import lemma,lexeme\n",
        "    import nltk\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "    class LemmaTokenizer(object):\n",
        "        def __init__(self):\n",
        "            self.wnl = WordNetLemmatizer()\n",
        "        def __call__(self, articles):\n",
        "            #return [lemma(t) for t in word_tokenize(articles) if t.lower() in words]\n",
        "\n",
        "            return [lemma(t) for t in word_tokenize(articles) if t.lower() in words and \\\n",
        "                    t.lower() not in stop_words \\\n",
        "                    #and t.lower() not in word_tokenize(stop_words).encode() \\ # génère un pb de 'byte' ?!\n",
        "                    # cas des strs spécifiques non filtrés\n",
        "                    and t not in [\"''\", '--', '1.2', '1/2', '18th', '2-3', '20th', '4.00', '4.2', '``']\\\n",
        "                    and t.lower() not in string.punctuation and not t.isdigit()]\n",
        "\n",
        "\n",
        "\n",
        "    words = set(nltk.corpus.words.words())\n",
        "\n",
        "    count_vec_lemmatise = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=\"english\", analyzer='word',\n",
        "                                ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
        "\n",
        "    # Transforms the data into a bag of words\n",
        "    count_train = count_vec_lemmatise.fit(df_movies[\"Avis\"])\n",
        "    bag_of_words = count_vec_lemmatise.transform(df_movies[\"Avis\"])\n",
        "\n",
        "    # Print the first 10 features of the count_vec\n",
        "    print(\"Les 50 premiers termes:\\n{}\".format(count_vec_lemmatise.get_feature_names_out()[:50]))\n",
        "    print(\"\\nTous les 50 termes:\\n{}\".format(count_vec_lemmatise.get_feature_names_out()[::50])) # Tous les 50 termes\n",
        "    #print(type(count_train.vocabulary_))\n",
        "    #print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))\n",
        "\n",
        "    #les 50 premiers mots et leur nbr d'occurrence\n",
        "    print(\"\\nLe nbr d'occurrence des 50 premiers termes :\")\n",
        "    print([(k,v)  for k,v in count_train.vocabulary_.items()][:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlRdVp48lPQ3"
      },
      "source": [
        "#### 2.2.3.2 **Calcul TfIdf de notre corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4OXwZiVlPQ3"
      },
      "source": [
        "<font color=\"red\" size=\"3\"> Si vous avez installé \"Pattern\", vous pouvez changer la 1ere ligne de 1a cellule\n",
        "suivante par :      \n",
        "    \n",
        "TfIdf_lemmatise = TfidfVectorizer(tokenizer=lemma, stop_words=\"english\",\\\n",
        "     smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RYHQUohDmgjK"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr5HwKbylPQ4",
        "outputId": "58fe8ef7-81af-4ad1-c94e-88bb33ecebc2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1) Les 10 dernières lignes de la  matrice TfIdf (en ligne : les indices, en colonne les termes/mots) :\n",
            "         \b   \\t    \u0010           !          \"    #    $    %         &  ...  \\\n",
            "39990  0.0  0.0  0.0  171.0  0.0   0.000000  0.0  0.0  0.0  0.000000  ...   \n",
            "39991  0.0  0.0  0.0  123.0  0.0  17.624115  0.0  0.0  0.0  0.000000  ...   \n",
            "39992  0.0  0.0  0.0  276.0  0.0   0.000000  0.0  0.0  0.0  0.000000  ...   \n",
            "39993  0.0  0.0  0.0  154.0  0.0   0.000000  0.0  0.0  0.0  0.000000  ...   \n",
            "39994  0.0  0.0  0.0  259.0  0.0   0.000000  0.0  0.0  0.0  0.000000  ...   \n",
            "39995  0.0  0.0  0.0  381.0  0.0   0.000000  0.0  0.0  0.0  0.000000  ...   \n",
            "39996  0.0  0.0  0.0  586.0  0.0  49.347522  0.0  0.0  0.0  0.000000  ...   \n",
            "39997  0.0  0.0  0.0  289.0  0.0   0.000000  0.0  0.0  0.0  0.000000  ...   \n",
            "39998  0.0  0.0  0.0   63.0  0.0   3.524823  0.0  0.0  0.0  0.000000  ...   \n",
            "39999  0.0  0.0  0.0  262.0  0.0  24.673761  0.0  0.0  0.0  3.888853  ...   \n",
            "\n",
            "         “    ”    …    ″    ₤    ▼    ★    、        ，  \n",
            "39990  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "39991  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "39992  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "39993  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "39994  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "39995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "39996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "39997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "39998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "39999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "\n",
            "[10 rows x 160 columns]\n",
            "--------------------------------------------------\n",
            "2) Quelques indice Idf pour certains mots :\n",
            "    idf_weights\n",
            "\b     11.819778\n",
            "\\t     8.523941\n",
            "\u0010     11.819778\n",
            "       1.000000\n",
            "!      2.058488\n",
            "..          ...\n",
            "▼     11.819778\n",
            "★     11.819778\n",
            "、     11.819778\n",
            "     11.819778\n",
            "，     11.819778\n",
            "\n",
            "[160 rows x 1 columns]\n",
            "--------------------------------------------------\n",
            "2bis) Le 100 premiers termes : colonnes de la matrice TfIdf\n",
            "['\\x08' '\\t' '\\x10' ' ' '!' '\"' '#' '$' '%' '&' \"'\" '(' ')' '*' '+' ','\n",
            " '-' '.' '/' '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' ':' ';' '<' '=' '>'\n",
            " '?' '@' '[' '\\\\' ']' '^' '_' '`' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'j' 'k' 'l'\n",
            " 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' '{' '|' '}' '~'\n",
            " '\\x80' '\\x84' '\\x85' '\\x8d' '\\x8e' '\\x91' '\\x95' '\\x96' '\\x97' '\\x9a'\n",
            " '\\x9e' '\\xa0' '¡' '¢' '£' '¤' '¦' '§' '¨' '©' 'ª' '«' '\\xad' '®' '°' '³'\n",
            " '´' '·' 'º' '»']\n",
            "--------------------------------------------------\n",
            "3) Le 100 derniers termes de la matrice = colonnes de la matrice\n",
            "['u' 'v' 'w' 'x' 'y' 'z' '{' '|' '}' '~' '\\x80' '\\x84' '\\x85' '\\x8d'\n",
            " '\\x8e' '\\x91' '\\x95' '\\x96' '\\x97' '\\x9a' '\\x9e' '\\xa0' '¡' '¢' '£' '¤'\n",
            " '¦' '§' '¨' '©' 'ª' '«' '\\xad' '®' '°' '³' '´' '·' 'º' '»' '½' '¾' '¿'\n",
            " 'ß' 'à' 'á' 'â' 'ã' 'ä' 'å' 'æ' 'ç' 'è' 'é' 'ê' 'ë' 'ì' 'í' 'î' 'ï' 'ð'\n",
            " 'ñ' 'ò' 'ó' 'ô' 'õ' 'ö' 'ø' 'ù' 'ú' 'û' 'ü' 'ý' 'þ' 'ğ' 'ı' 'ō' 'ż' 'א'\n",
            " 'ג' 'ו' 'י' 'כ' 'ל' 'מ' 'ן' 'ר' '–' '‘' '’' '“' '”' '…' '″' '₤' '▼' '★'\n",
            " '、' '\\uf0b7' '，']\n",
            "--------------------------------------------------\n",
            "Il y a 2400 documents, 3204 termes \n",
            "4) La matrice sur X_train : \n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "--------------------------------------------------\n",
            "5) Les 50 premiers termes du vocabulaire  et leur indices :\n",
            "[('o', 54), ('n', 53), ('e', 45), (' ', 3), ('f', 46), ('t', 59), ('h', 48), ('r', 57), ('v', 61), ('w', 62), ('s', 58), ('m', 52), ('d', 44), ('c', 43), ('g', 47), ('j', 49), ('u', 60), ('1', 20), ('z', 65), ('p', 55), ('y', 64), (\"'\", 10), ('l', 51), ('b', 42), ('k', 50), ('.', 17), (',', 15), ('x', 63), ('<', 31), ('/', 18), ('>', 33), ('(', 11), (')', 12), ('q', 56), ('-', 16), ('\"', 5), ('!', 4), (':', 29), ('2', 21), ('?', 34), ('&', 9), ('3', 22), ('0', 19), ('5', 24), ('7', 26), ('8', 27), ('9', 28), ('+', 14), ('*', 13), ('6', 25)]\n",
            "Voici les vecteurs de quelques termes :\n",
            "Certains mots / temes n'existent pas\n",
            "Certains mots / temes n'existent pas\n",
            "Certains mots / temes n'existent pas\n",
            "Certains mots / temes n'existent pas\n"
          ]
        }
      ],
      "source": [
        "TfIdf_lemmatise = TfidfVectorizer(tokenizer=lemma, stop_words=\"english\",\n",
        "     smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
        "# max_df = min_df : par default=1.0\n",
        "\n",
        "corpus_fitted = TfIdf_lemmatise.fit(df_movies[\"Avis\"])\n",
        "train_lemmatise_transformed = corpus_fitted.transform(X_train[\"Avis\"])\n",
        "test_lemmatise_transformed = corpus_fitted.transform(X_test[\"Avis\"])\n",
        "\n",
        "# Une partie de la matrice TDIDF\n",
        "temp_df=pd.DataFrame(train_lemmatise_transformed.toarray(), columns=TfIdf_lemmatise.get_feature_names_out())\n",
        "\n",
        "#print(temp_df.columns.values)\n",
        "print(\"1) Les 10 dernières lignes de la  matrice TfIdf (en ligne : les indices, en colonne les termes/mots) :\")\n",
        "print(temp_df.tail(10))\n",
        "print( \"-\"*50)\n",
        "\n",
        "df_idf = pd.DataFrame(TfIdf_lemmatise.idf_, index=TfIdf_lemmatise.get_feature_names_out(),columns=[\"idf_weights\"])\n",
        "print(\"2) Quelques indice Idf pour certains mots :\")\n",
        "print(df_idf)\n",
        "print(\"-\"*50)\n",
        "\n",
        "print(\"2bis) Le 100 premiers termes : colonnes de la matrice TfIdf\")\n",
        "liste_termes=TfIdf_lemmatise.get_feature_names_out()\n",
        "print(liste_termes[:100])\n",
        "print(\"-\"*50)\n",
        "\n",
        "print(\"3) Le 100 derniers termes de la matrice = colonnes de la matrice\")\n",
        "print(liste_termes[-100:])\n",
        "print(\"-\"*50)\n",
        "\n",
        "print(\"Il y a 2400 documents, 3204 termes \")\n",
        "print (\"4) La matrice sur X_train : \\n\", train_lemmatise_transformed.toarray())\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"5) Les 50 premiers termes du vocabulaire  et leur indices :\")\n",
        "print([(k,v)  for k,v in TfIdf_lemmatise.vocabulary_.items()][:50])\n",
        "\n",
        "print(\"Voici les vecteurs de quelques termes :\")\n",
        "try : # Suivant le mot donné, il y a erreur si le mot n'existe pas\n",
        "    print(\"\\nvaleur de Zero\", TfIdf_lemmatise.vocabulary_[\"zero\"])\n",
        "    TfIdf_lemmatise.vocabulary_['zero']\n",
        "except :\n",
        "     print(\"Certains mots / temes n'existent pas\")\n",
        "\n",
        "try : # Suivant le mot donné, il y a erreur si le mot n'existe pas\n",
        "    print(pd.DataFrame([temp_df[\"yucky\"]], index=[\"yucky\"]).T)\n",
        "except :\n",
        "     print(\"Certains mots / temes n'existent pas\")\n",
        "\n",
        "try : # Suivant le mot donné, il y a erreur si le mot n'existe pas\n",
        "    print(pd.DataFrame([temp_df[\"young\"]], index=[\" young\"]).T)\n",
        "except :\n",
        "     print(\"Certains mots / temes n'existent pas\")\n",
        "\n",
        "try : # Suivant le mot donné, il y a erreur si le mot n'existe pas\n",
        "    print(pd.DataFrame([temp_df[\"yellow\"]],index=[\"yellow\"]).T)\n",
        "except :\n",
        "     print(\"Certains mots / temes n'existent pas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRpXB_19lPQ4"
      },
      "source": [
        "<font size=\"4\"> On a maintenant la matrice TdIdf.</font>\n",
        "On peut déjà faire des calculs de similarité avec.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s8f8wuDBo1i"
      },
      "source": [
        "### Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODulHJVYlPQ4",
        "outputId": "0f139ade-6b2d-48b0-a9bd-4a7717d4840a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Un exemple d'avis :  Craig Brewer is now officially a writer/director for whom I will see any film by, no matter how bad it may look. His debut, Hustle and Flow, was one of my favorites from that year, with its emotionally charged storyline and realistic, fallible characters. I wasn't quite sure what I would end up thinking after seeing this sophomore effort. The cast seemed great, the trailer used music effectively, however, it seemed like there was a good chance it would cross into absurdity, and fast. Fortunately, Black Snake Moan hits all its marks dead-on. The acting is astonishing, the writing superb, and the editing style, as well as juxtaposed music, riveting the whole way. Brewer seems to be a master at getting his characters to have the right mix of both compassion and malice as they set forward on their paths toward redemption.<br /><br />The first moment I knew I was in for a treat was during the abbreviated credit sequence at the beginning. Like he did with Hustle and Flow, Brewer lays the music over the widescreen shots perfectly with simply titled fonts coming up statically. The 70's aesthetic was welcome and helped show that this would be another great character piece in the vain of those from that decade of some of cinema's best. From here we continued on with the short snippets into the lives of both Lazarus and Rae, each vignette mirroring the other while they journey to the fateful moment their paths finally cross. The editing between them was fluid and relevant rather than abruptly cutting before the scene felt finished with its purpose. Rae's boyfriend leaves for duty in the service and Laz's wife leaves him for his brother. Each feels the loneliness and reverts to what they know in that situationRae to sex and Laz to the bottle. Only when Rae is left for dead at the side of the road and her savior comes from his farm to take her in does the reasoning for their actions finally start to become clear.<br /><br />Samuel L. Jackson is fantastic as the older bluesman farmer trying to reconcile his life with God and that of the flesh and the pain it has brought him. There are the moments of stoic sternness as well as those of kindheartedness with his captive/patient. You never really look at the setup as comical or unrealistic because he sells what he is doing so well. Also, the character of Rae is not chained up for very long, despite what the trailers would have you believe. The situation starts a bit awkward until we see that the chaining was for her own good and is actually used for only a day or two. As for that chained girl, Christina Ricci really shines. I never really saw her as anything special, but this role is a true breakthrough for her. This girl is so troubled that her past sexual abuse has scarred her very deep down. Any time she is away from her love she starts seeing flashes of the man who took her childhood innocence away and itches to be touched by any man available to let the image go away. Her nymphomania is not for pleasure, but rather for survival from the haunting nightmares always hiding behind her eyelids. Ricci fully inhabits the role and shows all the emotional trauma to great effect and realism. Mention must also be made of Justin Timberlake, again showing some real acting talent. Where this guy came from I have no clue, but hopefully he will continue taking more films and steer away from the mostly crap music he churns out.<br /><br />While not as solid and consistent as Hustle and Flow, Moan still ranks equally to it, in my mind, because when it is on, it is spectacular. Towards the end we have a truly enthralling sequence with \"This Little Light of Mine\" singing out, and earlier, the interaction between captive and captor, when the chain is first introduced, shows some top-notch work. The truly magical moment, though, is when Jackson sings (yes that is him throughout, like it was Terrence Howard in Hustle) the titular song while a thunderstorm roars and the lights flicker. If I don't see a more beautifully shot sequence all year, I won't be surprised. What these two people do for each other is wonderful and shows what humanity is capable of. One thing I think I really enjoy with Brewer's work is the fact that he doesn't show sinners becoming redeemed heroes. Instead he shows us that no matter how bad you have been, or how bad life has been, everyone can strive for redemption and to be better people. We don't have saints here, but fallible people looking to right their ship. If the course stays true or if it falls back into darkness, no one really knows, but at least they can say that they tried as hard as they could.\n",
            "---------------\n",
            "\n",
            "TFIDF du query :   (0, 22902)\t5.373502031073541\n",
            "  (0, 22813)\t3.4338333796039997\n",
            "  (0, 22456)\t7.437751649736401\n",
            "  (0, 22379)\t2.895121982223208\n",
            "  (0, 22374)\t2.5260163042950277\n",
            "  (0, 22312)\t8.548419994255585\n",
            "  (0, 20497)\t4.977274424576793\n",
            "  (0, 20175)\t4.329248882349571\n",
            "  (0, 20014)\t6.435283221621194\n",
            "  (0, 19924)\t3.3814118735400167\n",
            "  (0, 19820)\t10.721165995742174\n",
            "  (0, 19620)\t4.202510470781936\n",
            "  (0, 19253)\t4.245733279038083\n",
            "  (0, 18897)\t4.53701710480469\n",
            "  (0, 18508)\t20.06963870846163\n",
            "  (0, 18478)\t6.660722985195754\n",
            "  (0, 18214)\t7.520371911045477\n",
            "  (0, 17120)\t3.191580534951134\n",
            "  (0, 16227)\t2.2035062465142934\n",
            "  (0, 16188)\t6.472670753692815\n",
            "  (0, 15477)\t3.6115589009134488\n",
            "  (0, 15146)\t4.678533162059792\n",
            "  (0, 15008)\t6.242861767904196\n",
            "  (0, 14865)\t3.3883608700154495\n",
            "  (0, 14823)\t9.334871634622283\n",
            "  :\t:\n",
            "  (0, 5531)\t7.023987738813542\n",
            "  (0, 5130)\t3.9191416713922784\n",
            "  (0, 4790)\t6.637994734118198\n",
            "  (0, 4479)\t6.181423615076538\n",
            "  (0, 4417)\t6.68987956948721\n",
            "  (0, 4312)\t6.177871213472169\n",
            "  (0, 4165)\t7.3771270279199666\n",
            "  (0, 3978)\t5.290359446148058\n",
            "  (0, 3885)\t13.341259670467858\n",
            "  (0, 3881)\t2.7314925584414063\n",
            "  (0, 3603)\t4.536330055653652\n",
            "  (0, 3270)\t4.171515253508363\n",
            "  (0, 3260)\t6.802498447595359\n",
            "  (0, 3222)\t10.569074026793249\n",
            "  (0, 3135)\t4.941451816118958\n",
            "  (0, 2945)\t3.8028005376480207\n",
            "  (0, 2821)\t6.360192770266124\n",
            "  (0, 2547)\t6.343314732478772\n",
            "  (0, 2181)\t3.223958772538856\n",
            "  (0, 2099)\t5.414763113528454\n",
            "  (0, 2052)\t9.111728083308073\n",
            "  (0, 2016)\t5.03532122177264\n",
            "  (0, 1637)\t4.7172789286356345\n",
            "  (0, 1141)\t10.913500361739636\n",
            "  (0, 111)\t10.210340371976184\n",
            "\n",
            "cosine_similarities entre les 2 phrases : [0.         0.0212509  0.01953226 ... 0.02893528 0.01841887 0.04262611]\n",
            "\n",
            "elements non. nuls  du consine :  [0.0212509  0.01953226 0.0152189  ... 0.02893528 0.01841887 0.04262611]\n",
            "\n",
            "Les indices des documents proches du query avec similarité par cosinus p/r à train set:\n",
            "[19639 36986  7817 24241 36374 36996 19358 13643 11063 31409]\n",
            "\n",
            "Le query =  I'm sorry but this guy is not funny. I swear I've heard heard 4 year olds come up with better jokes then some of his. \"Dee dee dee\" for instance is possibly the worst catch phrase I've ever heard. It lacks any creativity at all, and to be making fun of mentally challenged people when you've reached level of having your own show is incredibly dim-witted on Mencia's part.<br /><br />Though every one compares this fool to Chappelle, their is no contest. First off they had very very different shows. I think all in all Menica's show on average had only about 2 short 5 minute skits in between his 10 minute rants about god knows what. Chappelles show came off more as sketch comedy, with 2-4 skits that occupied all the show. All chappelle did was a short summarization of each skit before and after each one. This is where Mencia fails even more. What would make Mencia think having a show which consists of the same standup comedy that he talks about on his standup specials would be a beneficial idea? Does anybody really want to listen to a bit George Lopez pioneered years before Mencia, but just dragged beyond belief to the point where its dead? Snowflake's chance in Hell.<br /><br />My point is even though most people hate this guy for his rascism, I just cant stand him for his imcompetence. Comedy Central was looking for a minority they could brand as \"controversial\" and then leave him to follow Chappelles path. The problem, is this guy made it very clear he doesn't want to be Chappelle. So instead he conducts his crappy show like a burning trainwreck right into the ground. Does anybody want to watch a weekly standup about the same stuff every thursday, I know sure as hell I don't.<br /><br />I cant express my gratitude to Comedy Central though. This idiot's show is done. Personally after watching his standup, I don't know how he got his own show in the first place. There are so many more deserving comics like Jim Gafigan, Zach Galifinakis, etc... In fact anyone is better than this fool.\n",
            "\n",
            "Les documents proches du Query : I'm sorry but this guy is not funny. I swear I've heard heard 4 year olds come up with better jokes then some of his. \"Dee dee dee\" for instance is possibly the worst catch phrase I've ever heard. It lacks any creativity at all, and to be making fun of mentally challenged people when you've reached level of having your own show is incredibly dim-witted on Mencia's part.<br /><br />Though every one compares this fool to Chappelle, their is no contest. First off they had very very different shows. I think all in all Menica's show on average had only about 2 short 5 minute skits in between his 10 minute rants about god knows what. Chappelles show came off more as sketch comedy, with 2-4 skits that occupied all the show. All chappelle did was a short summarization of each skit before and after each one. This is where Mencia fails even more. What would make Mencia think having a show which consists of the same standup comedy that he talks about on his standup specials would be a beneficial idea? Does anybody really want to listen to a bit George Lopez pioneered years before Mencia, but just dragged beyond belief to the point where its dead? Snowflake's chance in Hell.<br /><br />My point is even though most people hate this guy for his rascism, I just cant stand him for his imcompetence. Comedy Central was looking for a minority they could brand as \"controversial\" and then leave him to follow Chappelles path. The problem, is this guy made it very clear he doesn't want to be Chappelle. So instead he conducts his crappy show like a burning trainwreck right into the ground. Does anybody want to watch a weekly standup about the same stuff every thursday, I know sure as hell I don't.<br /><br />I cant express my gratitude to Comedy Central though. This idiot's show is done. Personally after watching his standup, I don't know how he got his own show in the first place. There are so many more deserving comics like Jim Gafigan, Zach Galifinakis, etc... In fact anyone is better than this fool.\n",
            "un docmument proche : The Underground Comedy movie is perhaps one of the worst comedies I've ever seen. I should have known it was going to be bad when the box had the phrase \"guaranteed to offend\" written on it... meaning that the filmmakers were going to focus more on grossing you out than making you laugh.<br /><br />This movie is an amateurish jumble of childish skits, bad characters, and worse jokes... from the pathetic Bat-Man sketch to the painfully unfunny Arnold Shvollenpecker skit, they just aren't funny. The few skits that are a little funny are few and far between - watching Micheal Clark Duncan play a gay virgin, for example - but even they go on too long and get ruined from Vince Offer's ineptness at comedy.<br /><br />Keep The Underground Comedy Movie underground... bury it!\n",
            "un docmument proche : This show reminds me of an episode of \"The Simpsons,\" where Smithers had just been fired by Mr. Burns and was forced to live a miserable life being unemployed. All he did to while the way the hours was to drink and watch Comedy Central. The implication was that Comedy Central was a pathetic TV channel for miserable people with nothing else to occupy their time. \"Mind of Mencia\" is slightly better than most of Comedy Central's programming, but it still serves as an example of why this channel is low-quality filler for people with absolutely no lives. <br /><br />Truth be told, Mencia is a fairly competent comedian who throws out trite, soft-ball ethnic jokes and cashes in on bland stereotypes of people based on their race, gender, or economic status, or a combination of two out of three of these characteristics. If you've heard one of these jokes by one of your friends at a bar or at work, believe me, you've heard what passes as comedy on \"Mind of Mencia.\" Carlos also tries to make fun of current events, but lacks creativity or originality. This is not to say that Carlos is a washout all the time. I found some of his skits amusing, such as his rap video portrayal as an oil sheik. If you find bland, run-of-the-mill comedy up your alley, you'll probably enjoy \"Mind of Mencia.\" For those people who are tired of repetitive, pedestrian comedians trotting out the same old time-worn jokes, stay away.\n",
            "un docmument proche : And I repeat, please do not see this movie! This is more than a review. This is a warning. This sets the record for the worst, most effortless comedy ever made. At least with most of the recent comedies nowadays, the gags are crude and flat, but the writers and directors put in at least some sort of effort into making them funny. I never get tired of repeating one of my favorite mottos: Everyone thinks they can do comedy, and only 10 percent of them are right. Comedy is hard! This is not some genre any fool can play around with. I think it's atrocious that the filmmakers are comparing this piece of garbage to \"Kentucky Fried Movie.\" Basically, these bozos are comparing their so-called comic talents to those of the brilliant Jim Abrahams and the Zucker Brothers. Come on, I've seen Pauly Shore movies that are 10 times funnier than \"The Underground Comedy Movie.\" Here's a sample of the comedy for those curious about seeing this movie: One sketch involves a superhero dressed like a penis named D**kman. The whole joke is that he defeats his enemies by squirting them with semen. That's it. That's the whole joke. Wow. This is enough to make Carrot Top roll his eyes. Another sketch involves a man having sex with a dead person in a porn movie. And in another sketch, there's a bag lady beauty contest, in which we're exposed to the horrible sights of bikini-clad middle-aged women with beer guts and stretch marks. Plus, making fun of the homeless is more sad than funny. It's a step away from mocking the mentally handicapped. The whole movie is supposed to be a satire. I think the filmmakers forgot that a key element of satire...is TRUTH!!! For anybody who actually enjoyed this crap, explain to me what is truthful about ANY of these gags! Some of the sketches might've sounded funny on paper, but anybody who's taken any screen writing classes knows that if a sight gag sounds too funny on paper, it probably won't be funny on screen. If I tell someone about a big, black, muscular gay virgin, who's saving himself for the right man, he or she would probably laugh. But watching the premise played out on screen for about 10 minutes is a complete drag. I hate how whenever people criticize a low-brow comedy like this for not being funny, they're regarded as stuck-up squares. I just saw \"White Chicks\" recently. That's another low-brow, politically incorrect comedy, but I laughed my head off. The most offensive thing about \"The Underground Comedy Movie\" is it's not funny! What the writers and directors don't understand is that merely being filthy and tasteless doesn't work. There has to be more! Just think of the famous scene from \"There's Something About Mary\" (ironically, enough the bozo filmmakers put the Farrellys on their special thanks list). The joke about the semen wasn't just funny because it involved bodily fluids. There was a buildup. Ben Stiller was masturbating in the bathroom to make sure he didn't go out on a date with a \"loaded gun.\" Then he looked around to see where all the semen went after it was released. A knock is on the door, and he has to answer it. His date, Mary, is at the door and that's when it's revealed that the semen is hanging off Ben's ear. In this movie, there are multiple gags involving characters squirting loads of semen at people, with no buildup whatsoever. As Jay Leno always says, \"This comedy thing's not so easy, is it?\" Keep that in mind, Vince Offer, 'cause you weren't cut out for this genre!! The only reason people might laugh at these gags is because they want to feel hip. Let's face it, nowadays it's hip to laugh at anything politically incorrect. I know comedy is subjective...but this movie shouldn't be funny to anybody, except maybe the filmmakers themselves. As a side note, the movie had to have been made before Michael Clarke Duncan's fame in movies like \"Armageddon\" and \"The Green Mile.\" There can't be any other reason why an actor of his caliber would volunteer to be part of this amateurish freak show. All the others in the cast are either non-actors, has-been actors or B-movie stars. Karen Black made a good impression in \"Five Easy Pieces,\" but I don't think she's done anything of value ever since. Slash was probably drugged into being in this film. Gina Lee Nolin is nothing without \"Baywatch.\" Angelyne is the film's biggest star (keeping in mind Duncan wasn't famous at the time), and there are still probably a ton of people who haven't heard of her--for good reason. Usually, I'm in support of extremely low-budget flicks, but this one deserves to drift into obscurity. I hope to Lord this doesn't become a cult classic! Shouldn't there be a law against distributing crap like this?\n",
            "un docmument proche : To put it simply, Mind of Mencia is the worst, unfunniest show on Comedy Central, and quite possibly all of television... ever. I love Comedy Central and watch many shows and movies there all the time, but every time the intro for this show even starts after some good comedy, I can't find the remote fast enough.<br /><br />Mencia tells used, worn out racial jokes in a bad attempt at being controversial, then will point out the ethnicity he just talked about in the audience as if to say 'they liked it, so all of your race likes it, and by extension me too'. Give me a break Mencia, I don't know if those people are plants or not, but just because you go to a black guy and high five him after one of your ridiculous jokes, doesn't mean he thinks you're funny, it just means you put him on the spot and what else is he going to do, smack you on TV? He gives the impression that his jokes are deep, meaningful, and thought provoking, which is apparent in his commercials, but when you actually watch the show almost 100% of what he says is common sense. He also claims that only smart people get some of his jokes, but you don't have to be any smarter than a monkey to understand what he just said when he says that, maybe you need to go finish 3rd grade Carlos. The guy goes on to say that he says what others are only thinking, but I think you'll find that if anyone was thinking what he says, they figured out all on their own that it was ridiculous, and that's why they never said it in the first place.<br /><br />All in all, watch 1 episode and be done with it if you must, because after you've seen 1 episode, you've seen them all. Why Comedy Central keeps renewing this horrible show is well beyond me. If you're looking for good comedy, look elsewhere, because you will find very little of it here.\n",
            "un docmument proche : I may differ from many people on this board but I enjoy watching Mind Of Mencia. The reason I like Mind Of Mencia is the host is not afraid to speak his mind or exploit stereotypes. Carlos Mencia does what we all do with our friends but are unwilling to admit and then some.<br /><br />Mencia has no problem doing jokes about any race, religion, sex, or orientation. While he gets a lot of flack for this it is a breath of fresh air in these politically correct times. Mencia does not care if he offends anyone but he is not a racist and even does jokes about his own race.<br /><br />The typical format for the Mind of Mencia goes like this: there will be an open skit making fun of a person or recent event. Mencia then comes out and does a 5 or 6 minute stand up where he talks about various issues. The show then has 2 separate skits divided by commercials. In these skits Mencia does a variety of things such as making fun of people, giving his personal opinions with a funny twist, or simply doing parodies of people, events, or movies. At the end of the show Mencia comes out for a minute with either one final skit or something else to say.<br /><br />People criticize Mencia for exploiting stereotypes and say his statements are overly offensive. Carlos only does what everyone else thinks of certain races but are afraid to say. As for Mencia being offensive he is only speaking his mind. I find nothing wrong with that.<br /><br />The show is not perfect. The skits can be not put together the best and sometimes Mencia does go over the line in his jokes. For the most part however it is a show where the comedian says what is on his mind no matter what the consequences and presents in in a humorous matter. So if you aren't afraid to laugh at stereotypes and see someone speak his mind and often say what you have wanted to say then watch Mind Of Mencia. However if you are easily offended you should not watch this show because you will be offended or worse yet you might even laugh.\n",
            "un docmument proche : I missed the beginning but I did see most of it. A friend got it on DVD in the cheap room at FYE.<br /><br />The skits are all very short, and yet most of them are still too long. The majority of them, they seem to have forgotten to have something funny! Quite a lot of racist/sexist/\"homophobic\" humor in it, skits based on stereotypes, or skits which use racist terms for people.<br /><br />I'm trying to remember anything I thought was funny in it, and I'm having trouble.... The logo for the Tunnel Vision network is a lipsticked mouth with an eyeball in it. The mouth opens and closes over the eye like eyelids. Kind of creepy.<br /><br />What a disappointment. Most of the actors went on to better things, and it's lucky this bomb didn't hold them back.\n",
            "un docmument proche : I read all the reviews here AFTER watching this piece of cinematic garbage and it took me at least 2 pages to find out that somebody else didn't think that this appallingly unfunny montage WASN'T the acme of humour in the 70s or indeed in any other era! If this isn't the least funny set of sketch *comedy* I've ever seen it'll do till it comes along. Half of the skits had already been done (and infinitely better) by acts such as Monty Python and Woody Allen... If I was to say that a nice piece of animation that lasts about 90 seconds is the highlight of this film it would still not get close to summing up just how mindless and drivel-ridden this waste of 75 minutes is. Seminal comedy? Only in the world where seminal really DOES mean semen. Scatological humour? Only in a world where scat IS actually feces. Precursor jokes? Only if by that we mean that this is a handbook of how NOT to do comedy. Tits and bums and the odd beaver. Nice...if you are a pubescent boy with at least one hand free and haven't found out that Playboy exists. Give it a break because it was the early 70s? No way. There had been sketch comedy going back at least ten years prior. The only way I could even forgive this film even being made is if it was at gunpoint. Retro? Hardly. Sketches about clowns subtly perverting children may be cutting edge in some circles (and it could actually have been funny) but it just comes off as really quite sad. What kept me going throughout the entire 75 minutes? Sheer belief that they may have saved a genuinely funny skit for the end. I gave the film a 1 because there was no lower score...and I can only recommend it to insomniacs or coma patients...or perhaps people suffering from lockjaw...their jaws would finally drop open in disbelief.\n",
            "un docmument proche : Hearing about how hilarious this movie is, I finally rent it at<br /><br />the video store and for 75 minutes maybe I laughed 3 times. This<br /><br />movie, a collection of skits that make fun of television is an<br /><br />incoherrent mess. The jokes fall flat, the humor deals with<br /><br />issues from the 1970's that just aren't relevant anymore, and<br /><br />the jokes go on way too long (almost like the new SNL skits the<br /><br />past few years). Yeah, Chevy Chase is in this but maybe of all<br /><br />about 1 or 2 minutes. I liked the fact that this was very<br /><br />raunchy and had nudity galore but couldn't it be funny? Do<br /><br />yourself a favor and rent Kentucky Fried Movie which is a far<br /><br />superior film made in the\n",
            "un docmument proche : When I first saw Stella on comedy central I thought \"god, this is horrible!\" But I loved Wet Hot American Summer. So I knew something didn't make sense. There was nothing left for me to do but search out for the Stella shorts which I had heard contained more dildos than any other comedy troupe out there. When I found them not only were my dildo expectations met, but I found probably the greatest comedy scenes I've seen in years. The set of twenty odd short films put me on the edge of my seat with water running down my legs. Not only did I laugh my ass off the first time I watched them but continued to laugh harder when I watched them over and over again. It's really a shame Comedy Central had the restrictions of cable television for the Stella series. It would have faired much better on HBO or Showtime. Their brand of humor can't be restricted by censors or popular culture. Given even if it was on HBO I'm sure the \"masses\" would have voted it canceled in a season or less, but still it would have been fantastic. Needless to say I'm now in love with Stella and all it's individual members. Any Stella fan should check out David Wain's site with all his short films. They are equally as enjoyable as Stella itself. So my hat goes off to Michael Showalter, Michael Ian Black, and David Wain for the unbelievable comedy they have made with Stella. I hope to see much more of the troupe in the future.\n",
            "un docmument proche : This show has all the typical characters in a comedy: the good guy, the idiot, the pervert, the rich girl... but it's set on the 70's. That's the only difference that it has with other TV comedies. I don't know how you can like this show. Its humor is pathetic! I mean, the jokes are so direct... A typical dialog is this: \"Fez: Oh, Jackie I want to have sex with you. (audience laughs) Jackie: Fez you're a pervert. (audience laughs) Fez: Oh yes I am. (audience cheers and applauds)\" This isn't funny. I think that if it didn't have those laughs (I don't know how you call that in English, sorry) you wouldn't laugh at all. This isn't intelligent comedy, this is an insult to the public. I like most of the American comedies, but this isn't good at all. I would give it 4 out of 10. (Sorry for my poor English again.)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Un exemple de vecteur\n",
        "print(\"Un exemple d'avis : \", np.array(X_test[\"Avis\"])[0])\n",
        "\n",
        "indice_query=1\n",
        "\n",
        "# QQ affichages : on le refait ensuite (voir query_TFIDF)\n",
        "#vecteur = TfIdf_lemmatise.transform([np.array(X_test[\"Avis\"])[indice_query]]) # ZZ : [] important\n",
        "#print(\"\\nLe vecteur TfIdf : \" , vecteur.toarray())\n",
        "#print(\"\\nLe TfIdf du vecteur : \", vecteur)\n",
        "# ZZ : ci-dessous, les affichages donnent les indices aussi. Seul les réels = TfIdf\n",
        "#print(\"\\nET les indices non nuls de ce vecteur : \", end='')\n",
        "#print(np.nonzero(vecteur.toarray())[1])# Les éléments non nul\n",
        "#print(\"**\", np.nonzero(vecteur)[1])# Les éléments non nul\n",
        "print('---------------')\n",
        "\n",
        "# Pour le cas de l'avis  d'indice indice_query\n",
        "query_TFIDF = TfIdf_lemmatise.transform([np.array(X_test[\"Avis\"])[indice_query]])\n",
        "print(\"\\nTFIDF du query :\", query_TFIDF)\n",
        "\n",
        "cosine_similarities = cosine_similarity(query_TFIDF, train_lemmatise_transformed).flatten()\n",
        "related_product_indices = cosine_similarities.argsort()[:-11:-1]\n",
        "print(\"\\ncosine_similarities entre les 2 phrases :\", cosine_similarities)\n",
        "\n",
        "print(\"\\nelements non. nuls  du consine : \", cosine_similarities[np.nonzero(cosine_similarities)])\n",
        "# print(cosine_similarities[np.count_nonzero(cosine_similarities, axis=0)])\n",
        "print(\"\\nLes indices des documents proches du query avec similarité par cosinus p/r à train set:\")\n",
        "print(related_product_indices)\n",
        "\n",
        "words= np.array(X_test[\"Avis\"])[indice_query]\n",
        "print(\"\\nLe query = \", words)\n",
        "\n",
        "# Quelques un des documents proches :\n",
        "print(\"\\nLes documents proches du Query :\", np.array(X_test[\"Avis\"])[indice_query])\n",
        "for ind in related_product_indices :\n",
        "    print(\"un docmument proche :\", X_train.iloc[ind][0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq3WotzIlPQ4"
      },
      "source": [
        "**Rappel des dimensions de notre corpus (train & test)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ0ToNmvlPQ5",
        "outputId": "c52ecb60-f261-47bc-fcce-0eab36cbf335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((40000, 160), (10000, 160))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fit_train=train_lemmatise_transformed\n",
        "fit_test = test_lemmatise_transformed\n",
        "fit_train.shape, fit_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbDCiLK2lPQ5"
      },
      "source": [
        "### 2.3 : Supplément : calcul TfIdF (vactor space) sur le corpus par un Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRf93dyXlPQ5"
      },
      "source": [
        "**Pour calculer l'indice TfIdf dans le corpus, il faut d'abord compter le nombre d'occurrence du mot puis calculer son Tf puis son Idf et enfin son TfIdf.**\n",
        "\n",
        "Les deux étapes (comptage et TfIdf) peuvent être mises en séquence à l'aide d'un *pipeline*.\n",
        "\n",
        "Pour __\"pipeline\"__, voir https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html\n",
        "\n",
        "__CountVectorizer__ produit un sac de mots (bag of words) : un vecteur avec le nbr d'occurrenecs des mots\n",
        "\n",
        "__TfidfTransformer__ prend ce \"sac\" et le transforme en une matrice avec la fréquence tf-idf (term frequency * inverted document frequency).\n",
        "\n",
        "*make_pipeline* regroupe les deux actions (qui peuvent également être faites séparément)\n",
        "\n",
        "On peut créer des pipelines à multiples étages qui enchainent les actions d'un étage au suivant :\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LogisticRegression())\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjiFLwADlPQ5"
      },
      "source": [
        "<font color=\"red\">**Remarque**</font>\n",
        "\n",
        "<font color=\"red\">* On a fait le même travail plus haut en Lemmatisant et on a déjà créer fit_train et fit_test</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-UXrFx3lPQ5"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "if True :\n",
        "    # On a fait le travaille plus haut et déjà créer fit_train et fit_test\n",
        "    pipe = make_pipeline(CountVectorizer(), TfidfTransformer()) # Construire un Construct a Pipeline from the given estimators.\n",
        "    pipe.fit(X_train['Avis'])\n",
        "\n",
        "    fit_train = pipe.transform(X_train['Avis'])\n",
        "    fit_train.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2vB58pklPQ5",
        "outputId": "8819ea0d-56ed-45cb-aa0e-f25d73cd1ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n",
            "0.0 0.9906546770580443\n"
          ]
        }
      ],
      "source": [
        "print(fit_train.min(), fit_train.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAsaA2rBlPQ5",
        "outputId": "d7b5370e-d22b-4059-d5b5-0e3e0fc73942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 93030)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "fit_test = pipe.transform(X_test['Avis'])\n",
        "print(fit_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWYWBFSSlPQ5"
      },
      "source": [
        "##  Partie Analyse\n",
        "### 2.4- Modèle arbre de décision\n",
        "**Un seul arbre de décision produira des résultats assez médiocres.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q--LWlRplPQ5",
        "outputId": "285a5075-0dd5-47a4-cad9-36fb4ab1a6e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7291"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(fit_train, y_train)\n",
        "dt.score(fit_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWqe12EulPQ5"
      },
      "source": [
        "**Le score est assez __médicore__; ce sera une base de comparaison.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6AwUbfUlPQ5"
      },
      "source": [
        "### 2.5- Modèle Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "8XzdP96FlPQ5",
        "outputId": "4dfb9a1d-baca-4d50-e371-aafe57a22573"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=50)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(n_estimators=50)\n",
        "clf.fit(fit_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stgPlAZwlPQ5",
        "outputId": "27c51ced-845b-4601-b8e0-5878b3ded724"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8357"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.score(fit_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scMhYsRHlPQ6"
      },
      "source": [
        "<font color=\"red\"> **Le score s'est un peu amélioré.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3clF8t2BlPQ6"
      },
      "source": [
        "#### 2.5.1- D'autres métriques d'évaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ArPvnnelPQ6",
        "outputId": "1da1b4c7-db10-4454-e4a7-3b6b57bd5a16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score_t=array([[0.26, 0.74],\n",
            "       [0.72, 0.28],\n",
            "       [0.18, 0.82],\n",
            "       ...,\n",
            "       [0.52, 0.48],\n",
            "       [0.1 , 0.9 ],\n",
            "       [0.56, 0.44]])\n",
            "score_a=array([[0.12, 0.88],\n",
            "       [0.24, 0.76],\n",
            "       [0.18, 0.82],\n",
            "       ...,\n",
            "       [0.22, 0.78],\n",
            "       [0.2 , 0.8 ],\n",
            "       [0.98, 0.02]])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "score_t = clf.predict_proba(fit_test)\n",
        "score_a = clf.predict_proba(fit_train)\n",
        "print(f\"{score_t=}\")\n",
        "print(f\"{score_a=}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4tdLth6lPQ6"
      },
      "source": [
        "#### 2.5.2 Courbe ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcfO6jJZlPQ6"
      },
      "outputs": [],
      "source": [
        "fpr_t, tpr_t, seuil_t = roc_curve(y_test, score_t[:, 1])\n",
        "fpr_a, tpr_a, seuil_a = roc_curve(y_train, score_a[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAoo1hzBlPQ6"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibyfdtn_lPQ6"
      },
      "source": [
        "#### 2.5.2.1 Courbe ROC (tpr vs. fpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "VN8QMHzHlPQ6",
        "outputId": "7c8bd4d2-d4cd-4e35-cf60-d9a98081a773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remarquer l'AUC\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAF0CAYAAADIGPXJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAABd0UlEQVR4nO3deVwU9f8H8Ney7C6H3Pcloqmg5gVJQCooiqiY5UGRitc3zVt+WSLemaalWZr3VeZBnlnhgQqkiYWKaYLmgYoCcikg5x6f3x+4m+sCsiuw1/v5eOwDGGZm37Ow89rPZz4zw2GMMRBCCCHPMVB3AYQQQjQPhQMhhBAFFA6EEEIUUDgQQghRQOFACCFEAYUDIYQQBRQOhBBCFFA4EEIIUUDhQAjRWleuXIGJiQnWrFmj7lJ0ziuFw5UrVzBmzBh4eHjAyMgIzZo1Q9euXbFixQoUFhY2VI1K2bFjBzgcDi5cuNCoz3P37l1wOBzZw8DAAFZWVujduzdOnDhR63LHjh3DgAEDYGdnB4FAADc3N0RGRiItLa3WZc6cOYPhw4fDxcUFfD4fFhYW8Pf3x/r161FaWtoYm1cr6Xbv2LGjSZ/3eaNHj0aLFi3kphUWFuK9996Dvb09OBwOBg8eDADgcDhYuHBhk9eoioULF4LD4TTY+qTvBenD0NAQrq6uGDNmDB4+fKj0+gIDAxEYGNhg9Um1aNECo0ePlv2clZWFhQsX4vLly3UuV1JSgqFDh2Lq1KmYOnVqg9f1POlreffu3QZZX1lZGRYuXIjExMQGWV9tzp07h4ULF+LJkyfKL8xUtGnTJmZoaMjat2/PvvvuO5aQkMBOnDjBli5dyjw8PNjgwYNVXfUr2b59OwPAUlJSGvV5MjIyGAA2depUlpyczM6ePcu2bNnC3NzcGJfLZUlJSQrLzJo1iwFg/fr1Yz/99BNLSkpimzdvZl5eXkwgELADBw4oLDN//nwGgPn7+7OtW7eyxMREFhcXx+bOncvs7e3ZjBkzGnU7XyTd7u3btzfp8z7v1q1b7NKlS3LTZsyYwfh8Pvvxxx9ZcnIyu3HjBmOMseTkZJaZmamOMpW2YMEC9gpvSQXS98L27dtZcnIyO336NFu4cCETCATMw8ODPX36VKn19ezZk/Xs2bPB6pO6dOkSu3XrluznlJSUev2PDRs2jH3wwQdMIpE0eE0vkr6WGRkZDbK+vLw8BoAtWLCgQdZXmy+//FLlulX6Tzx37hzjcrmsX79+rKKiQuH3lZWV7Oeff1Zl1SqrqqpiQqGwycPhyy+/lJuelJTEALBRo0bJTd+9ezcDwD766COFdT19+pR5e3szExMTdvv2bdn0n376iQFg48aNq/ENUFxczI4fP95AW1Q/mhAONQkODmZeXl7qLkOmtLRU6WUaKxxefC/MmzePAWA//vijUutr6HAoKyurcXp9w6Ep6WM4qNSttHTpUnA4HGzatAkCgUDh93w+H4MGDZL9LJFIsGLFCnh6ekIgEMDe3h6jRo3CgwcP5JZ7sXkp9WJzNjExERwOBzt37sT//d//wcXFBQKBALdu3ZLN8/jxY4wZMwbW1tYwNTVFWFgY7ty5o7DukydPonfv3jA3N4eJiQkCAgJw6tQpFV6Vaj4+PgCAR48eyU3//PPPYWVlha+++kphGVNTU6xZswZlZWX4+uuvZdMXL14MKysrfPvttzV2N5iZmaFv374q11qThw8f4sMPP4Sbmxv4fD6cnZ0xdOhQhe153q1btzBmzBi0bt0aJiYmcHFxQVhYGK5evSo3n0QiwZIlS9C2bVsYGxvD0tISHTt2xDfffCObJy8vT/b8AoEAdnZ2CAgIwMmTJ2XzPN+tJO3mOnnyJNLT02VdKNLmek3dSjk5OZgwYQJcXV3B5/Ph4eGBRYsWQSQSyeaR/o+92OyvqVtt9OjRaNasGa5evYq+ffvCzMwMvXv3rvN1/u2339C5c2cIBAJ4eHjU+H8BAIwxrFu3Dp07d4axsTGsrKwwdOjQGv+X6+vNN98EANy7dw8AUFFRgejoaHh4eIDP58PFxQWTJ0+uV1fEokWL4OvrC2tra5ibm6Nr167YunUr2AvX82zRogUGDhyIgwcPokuXLjAyMsKiRYtkv5O+7xMTE/HGG28AAMaMGSP7ez7/N7xw4QIGDRoEa2trGBkZoUuXLvjpp5/knq+srAwff/yxrMvb2toaPj4+2LNnz0u36fz58wgICICRkRGcnZ0RHR0NoVCoMF9tXZa17cek7t69Czs7OwDVr590G59f5ubNm4iIiIC9vT0EAgG8vLzw3Xffya3nZe+nhQsXYtasWQAADw8PhffGyxjWa67niMVinD59Gt7e3nBzc6vXMh999BE2bdqEKVOmYODAgbh79y7mzZuHxMREXLp0Cba2tsqWAQCIjo6Gn58fNmzYAAMDA9jb28t+N27cOPTp0we7d+9GZmYm5s6di8DAQFy5cgWWlpYAgB9//BGjRo3C22+/je+//x48Hg8bN25ESEgIjh8//tI3eE0yMjIAAG3atJFNy87OxrVr1xAeHg4TE5Mal/Pz84O9vT3i4+Nly/zzzz91LtPQHj58iDfeeANCoRBz5sxBx44dUVBQgOPHj+Px48dwcHCocbmsrCzY2Njgiy++gJ2dHQoLC/H999/D19cXqampaNu2LQBgxYoVWLhwIebOnYsePXpAKBTi+vXrcjuhkSNH4tKlS/j888/Rpk0bPHnyBJcuXUJBQUGNz+3k5ITk5GRMmjQJRUVF2LVrFwCgXbt2Nc6fk5ODbt26wcDAAPPnz0erVq2QnJyMJUuW4O7du9i+fbtKr11VVRUGDRqECRMmYPbs2XJB86JTp07h7bffhp+fH/bu3QuxWIwVK1bUGMATJkzAjh07MG3aNCxfvhyFhYVYvHgx/P398ffff9f6N6mL9EOUnZ0dGGMYPHgwTp06hejoaHTv3h1XrlzBggULkJycjOTk5Bo/AErdvXsXEyZMQPPmzQFU71inTp2Khw8fYv78+XLzXrp0Cenp6Zg7dy48PDxgamqqsL6uXbti+/btGDNmDObOnYsBAwYAAFxdXQEACQkJ6NevH3x9fbFhwwZYWFhg7969CA8PR1lZmWwHGxUVhZ07d2LJkiXo0qULSktL8c8//9T6fySVlpaG3r17o0WLFtixYwdMTEywbt067N69u34vbj04OTnh2LFj6NevH8aNG4fx48cDgCww0tLS4O/vj+bNm2PlypVwdHTE8ePHMW3aNOTn52PBggUAXv5+Gj9+PAoLC7FmzRocPHgQTk5OAGp/byhQtqmRk5PDALD33nuvXvOnp6czAGzSpEly0//8808GgM2ZM0c2zd3dnUVGRiqs48XmbEJCAgPAevTooTCvtPn3zjvvyE3/448/GAC2ZMkSxlh1s9/a2pqFhYXJzScWi1mnTp1Yt27d6twuaffK8uXLmVAoZBUVFezy5cvMz8+POTk5yTXjzp8/zwCw2bNn17lOX19fZmxsrNQyDWns2LGMx+OxtLS0WuepT7eSSCRiVVVVrHXr1mzmzJmy6QMHDmSdO3eus4ZmzZq99DhKZGQkc3d3l5vWs2dP1r59e4V58ULTfcKECaxZs2bs3r17cvN99dVXDAC7du0aY+y//7GEhAS5+Wra/sjISAaAbdu2rc66pXx9fZmzszMrLy+XTSsuLmbW1tZy3UrJyckMAFu5cqXc8pmZmczY2Jh98skndT6P9L1w/vx5JhQKWUlJCfv111+ZnZ0dMzMzYzk5OezYsWMMAFuxYoXcsrGxsQwA27Rpk2zay7qVxGIxEwqFbPHixczGxkauK9Td3Z1xuVzZsaDnvfi+r6tbydPTk3Xp0oUJhUK56QMHDmROTk5MLBYzxhjr0KGDSsc9w8PDmbGxMcvJyZFNE4lEzNPTU6F75sX/rdq2pyZ1dSuFhIQwV1dXVlRUJDd9ypQpzMjIiBUWFjLG6vd+avJuJWUkJCQAgEIzq1u3bvDy8nqlLpwhQ4bU+rsPPvhA7md/f3+4u7vL6jl37hwKCwsRGRkJkUgke0gkEvTr1w8pKSn1Ggn06aefgsfjwcjICJ07d8Y///yDX375RWE0TX0wxhp0tApjTG7b6vo0CwBHjx5FUFAQvLy8lHoekUiEpUuXol27duDz+TA0NASfz8fNmzeRnp4um69bt274+++/MWnSJBw/fhzFxcUK6+rWrRt27NiBJUuW4Pz58zU251/Fr7/+iqCgIDg7O8u9LqGhoQCApKQklddd1/+jVGlpKVJSUvDuu+/CyMhINt3MzAxhYWEKtXI4HIwYMUKuVkdHR3Tq1Kne3QNvvvkmeDwezMzMMHDgQDg6OuLo0aNwcHDA6dOnASi+P4cNGwZTU9OXvj9Pnz6N4OBgWFhYgMvlgsfjYf78+SgoKEBubq7cvB07dpRrUSvr1q1buH79uuy9/fxr0r9/f2RnZ+PGjRsAqv+Pjh49itmzZyMxMRHl5eX1eo6EhAT07t1brkXG5XIRHh6uct3KqKiowKlTp/DOO+/AxMREYRsrKipw/vx5APV7P70KpcPB1tYWJiYmsu6Tl5E246RNmuc5Ozu/tJlXl5rWKeXo6FjjNOnzSZvwQ4cOBY/Hk3ssX74cjLF6DcedPn06UlJScPbsWXz11VcQCoV4++235bZL2uR+2Wt27949WVddfZepi7Sr7PlHXfLy8mTNd2VERUVh3rx5GDx4MH755Rf8+eefSElJQadOneTelNHR0fjqq69w/vx5hIaGwsbGBr1795YbdhwbG4vIyEhs2bIFfn5+sLa2xqhRo5CTk6N0XTV59OgRfvnlF4XXpX379gCA/Px8ldZrYmICc3Pzl873+PFjSCSSWv8/X6yVMQYHBweFes+fP1/vWn/44QekpKQgNTUVWVlZuHLlCgICAgBUvz8NDQ1lXRpSHA5H7v1Sk7/++kt2zGvz5s34448/kJKSgpiYGABQ2CHX9X6tD+l79uOPP1Z4PSZNmgTgv7/ft99+i08//RSHDx9GUFAQrK2tMXjwYNy8ebPO5ygoKKjX36axFBQUQCQSYc2aNQrb2L9/fwD/bWN93k+vQuljDlwuF71798bRo0fx4MGDl+5MbGxsAFT3ob84b1ZWltzxBiMjI1RWViqsIz8/v8bjEnV9yq5pZ5KTk4PXXnsNAGTrW7NmjewA3Yvq05/r6uoqOwgdEBAAR0dHjBgxAgsWLMDatWsBVL8p2rdvjxMnTqCsrKzGYwjJycl49OgRhg0bJlvm9ddfr3OZlwkLC0NKSkq957ezs1MYJFAf0mM3S5culZuen58vO74DAIaGhoiKikJUVBSePHmCkydPYs6cOQgJCUFmZiZMTExga2uL1atXY/Xq1bh//z6OHDmC2bNnIzc3F8eOHVO6thfZ2tqiY8eO+Pzzz2v8vbOzMwDIPtW/+P9Y2w65vi0+KysrcDicWv8/X6yVw+HgzJkzNfb713Us4HleXl6y/9EX2djYQCQSIS8vTy4gGGPIycmRHRyuyd69e8Hj8fDrr7/KtYIOHz5c4/yv2iqWvmejo6Px7rvv1jiP9PiWqakpFi1ahEWLFuHRo0eyVkRYWBiuX79e63PY2NjU628DVL/+Ne2vXuUDr5WVFbhcLkaOHInJkyfXOI+HhweA+r2fXoVK3UrR0dFgjOF///sfqqqqFH4vFArxyy+/AAB69eoFoHoH8ryUlBSkp6fLHfRt0aIFrly5Ijffv//+K2sqKkN6YFLq3LlzuHfvnmzUU0BAACwtLZGWlgYfH58aH3w+X+nn/eCDDxAYGIjNmzfLRoMAQExMDB4/foyPP/5YYZnS0lJMmzYNJiYmmDlzpmz6vHnz8PjxY0ybNk1h9AcAPH36tM4T7mxsbBS2qS6hoaFISEhQ+vXmcDgKO6rffvutzhOtLC0tMXToUEyePBmFhYU1nlzUvHlzTJkyBX369MGlS5eUqqk2AwcOxD///INWrVrV+DeXhoO0W/DF/8cjR4680vObmpqiW7duOHjwICoqKmTTS0pKZO+Z52tljOHhw4c11vr666+/Ui0AZO+/F9+fBw4cQGlpaZ2DMqQn1nG5XNm08vJy7Ny585Vqkv4vvdjyaNu2LVq3bo2///671vesmZmZwvocHBwwevRovP/++7hx4wbKyspqfe6goCCcOnVKbnCAWCxGbGyswrw17a9Onz6Np0+fqryNJiYmCAoKQmpqKjp27FjjNko/cD+vtvdTbc9TH0q3HIDqkTXr16/HpEmT4O3tjY8++gjt27eHUChEamoqNm3ahA4dOiAsLAxt27bFhx9+iDVr1sDAwAChoaGy0Upubm5yO8ORI0dixIgRmDRpEoYMGYJ79+5hxYoVCk3e+rhw4QLGjx+PYcOGITMzEzExMXBxcZE1P5s1a4Y1a9YgMjIShYWFGDp0KOzt7ZGXl4e///4beXl5WL9+vSovD5YvXw5fX1989tln2LJlCwDg/fffx6VLl/DVV1/h7t27GDt2LBwcHHDjxg18/fXXuH37Nnbv3o2WLVvK1jNs2DDMmzcPn332Ga5fv45x48ahVatWKCsrw59//omNGzciPDy8wYazLl68GEePHkWPHj0wZ84cvP7663jy5AmOHTuGqKgoeHp61rjcwIEDsWPHDnh6eqJjx464ePEivvzyS4WWYlhYGDp06AAfHx/Y2dnh3r17WL16Ndzd3dG6dWsUFRUhKCgIERER8PT0hJmZGVJSUnDs2LFaPymqso3x8fHw9/fHtGnT0LZtW1RUVODu3buIi4vDhg0b4OrqCkdHRwQHB2PZsmWwsrKCu7s7Tp06hYMHD75yDZ999hn69euHPn364P/+7/8gFouxfPlymJqaynVlBgQE4MMPP8SYMWNw4cIF9OjRA6ampsjOzsbZs2fx+uuv46OPPnqlWvr06YOQkBB8+umnKC4uRkBAgGy0UpcuXTBy5Mhalx0wYABWrVqFiIgIfPjhhygoKMBXX31V7xZNbVq1agVjY2Ps2rULXl5eaNasGZydneHs7IyNGzciNDQUISEhGD16NFxcXFBYWIj09HRcunQJ+/btAwD4+vpi4MCB6NixI6ysrJCeno6dO3fCz8+vzk/Uc+fOxZEjR9CrVy/Mnz8fJiYm+O6772o8/jhy5EjMmzcP8+fPR8+ePZGWloa1a9fCwsLipdtoZmYGd3d3/Pzzz+jduzesra1ha2uLFi1a4JtvvsFbb72F7t2746OPPkKLFi1QUlKCW7du4ZdffpEdJ3rZ+wmA7APEN998g8jISPB4PLRt27bGEFWg9CHs51y+fJlFRkay5s2bMz6fz0xNTVmXLl3Y/PnzWW5urmw+sVjMli9fztq0acN4PB6ztbVlI0aMUDhzVSKRsBUrVrCWLVsyIyMj5uPjw06fPl3raKV9+/Yp1CQdoXHixAk2cuRIZmlpyYyNjVn//v3ZzZs3FeZPSkpiAwYMYNbW1ozH4zEXFxc2YMCAGtf9vNpOgpMaNmwYMzQ0lDvzkzHG4uLiWP/+/ZmNjY3s+UaOHCkbJVOTpKQkNnToUObk5MR4PB4zNzdnfn5+7Msvv2TFxcV11qmszMxMNnbsWObo6Mh4PB5zdnZmw4cPZ48ePWKM1Txa5/Hjx2zcuHHM3t6emZiYsLfeeoudOXNG4e+2cuVK5u/vz2xtbRmfz2fNmzdn48aNY3fv3mWMMVZRUcEmTpzIOnbsyMzNzZmxsTFr27YtW7BggdxJZa8yWomx6pEi06ZNYx4eHozH4zFra2vm7e3NYmJi5M4azs7OZkOHDmXW1tbMwsKCjRgxgl24cKHG0UqmpqZKvc5HjhxhHTt2lL0OX3zxRa0nwW3bto35+voyU1NTZmxszFq1asVGjRrFLly4UOdz1PeE0PLycvbpp58yd3d3xuPxmJOTE/voo4/Y48eP5earabTStm3bWNu2bZlAIGAtW7Zky5YtY1u3blUYIePu7s4GDBhQ4/PXNLpnz549zNPTk/F4PIW/4d9//82GDx/O7O3tGY/HY46OjqxXr15sw4YNsnlmz57NfHx8mJWVlay2mTNnsvz8/DpfC8aqRza++eabTCAQMEdHRzZr1iy2adMmhW2qrKxkn3zyCXNzc2PGxsasZ8+e7PLly/UarcQYYydPnmRdunRhAoGAAZBbJiMjg40dO5a5uLgwHo/H7OzsmL+/v2y0JWMvfz9JRUdHM2dnZ2ZgYFDjCLzacBirob+CEEKIXqOrshJCCFFA4UAIIUQBhQMhhBAFFA6EEEIUUDgQQghRQOFACCFEgUonwTU1iUSCrKwsmJmZNeiF6QghRF0YYygpKYGzszMMDDTvc7pWhENWVla97x1BCCHaJDMzU6ULXjY2rQgH6anemZmZ9bryJSGEaLri4mK4ubnV71IWaqAV4SDtSjI3N6dwIIToFE3tKte8ji5CCCFqR+FACCFEAYUDIYQQBRQOhBBCFFA4EEIIUUDhQAghRAGFAyGEEAVKh8Pvv/+OsLAwODs7g8Ph4PDhwy9dJikpCd7e3jAyMkLLli2xYcMGVWolhBDSRJQOh9LSUnTq1Alr166t1/wZGRno378/unfvjtTUVMyZMwfTpk3DgQMHlC6WEEJI01D6DOnQ0FCEhobWe/4NGzagefPmWL16NQDAy8sLFy5cwFdffYUhQ4Yo+/SEEEKaQKNfPiM5ORl9+/aVmxYSEoKtW7dCKBSCx+MpLFNZWYnKykrZz8XFxY1dJiGkDhIJQ6VIgnKhGBVCMcqFYgjFElSJJBCKJagUSSAUM9nPVSIJqp77vfSrSMIgYdXrkzAGMWNgDBA/+1ki/T2r/pk9+776K8BQ/T1jDAzPpinMV70OgMl+X+t2Pfdc4udqU1ZLO1OsGNpJ5ddXEzV6OOTk5MDBwUFumoODA0QiEfLz8+Hk5KSwzLJly7Bo0aLGLo0QnSESS1AmFKO8SoyyKjHKqkTPfS9GuVBU/fX5aVWi535f/Tvpzr/68V8YVIok6t5EjaaLr0+TXHjvxQtLSZO8tgtORUdHIyoqSvaz9OqFhOgSxhhKKkUoLheiqFyI4nLRs69CFFdUTysqF+Jp5X87+vIqMcpe2NGXV4lRJW66nROfawABzwACQy74XA74hgbgcQ3kvvJlP3PAN+RWf+UagGvAAdeAAwOO9AFwDTjgcDjgGgAGnGffP/udgUH1PqJ6OmDAATio/p7D4YADgCNdB6qnyc37bJ66SGt6/nlfdi28kpKnMDU1ldVnbqTYA6LtGj0cHB0dkZOTIzctNzcXhoaGsLGxqXEZgUAAgUDQ2KUR0mAqRWI8KROisLQKj0urUFj27GupEI/LqvC4rAqFpVV4UvYsCCqqQ0CFHow6GXAAE74hjPlcmPC5MOZVf31+WvV0Q5gKuNXTeNVfjfmGMDI0gDGfCyNe9bJGPAMY8Z7/mQuugWZeRbSpPHjwAEHv9EdwcDDWrVunsVdVfVWNHg5+fn745Zdf5KadOHECPj4+NR5vIEQTFVcIkZZVjLSsYlzPKcaj4kq5Hf7TSpHK6+YbGsDCmAdzI8Pqr8Y8WDx7mBvx0MzI8Lkd/bPvn9/R8w1lO3iBoYHO7qw0QWZmJoKCgnD79m1Z17idnZ26y2oUSofD06dPcevWLdnPGRkZuHz5MqytrdG8eXNER0fj4cOH+OGHHwAAEydOxNq1axEVFYX//e9/SE5OxtatW7Fnz56G2wpCGghjDNlFFUjLKsa1rGKkZRchLbsYmYXlL12Wa8CBlQkPViZ8WJnyYS39avpsmgkflib/7filQWDE4zbBlpFXdf/+fQQFBeHOnTvw8PBAYmKizgYDoEI4XLhwAUFBQbKfpccGIiMjsWPHDmRnZ+P+/fuy33t4eCAuLg4zZ87Ed999B2dnZ3z77bc0jJWonVAswZ280uoAkIVBMZ6UCWuc38XSGO2czdHOyRyuVsawNpUPATOBoawPmuiWe/fuISgoCBkZGWjZsiUSEhLQvHlzdZfVqDisrnFeGqK4uBgWFhYoKiqiO8ERlTwpq0JadjHSs0uQnl2M9Oxi3Hz0tMYDuYYGHLxm30wWBO2dLdDOyRwWJtQNqo/u3buHwMBA3L17F61atUJCQkKDDJDR9P2aVtwmlJD6EksYMvJLkZ5dfWxAGgbZRRU1zt9MYAgvJzNZALRzNsdr9s2oq4fIXLlyBZmZmWjVqhUSExPh6uqq7pKaBIUD0VoSCcOd/FJcffgEVx4U4eqDIlzLKka5UFzj/G7WxvByNIeXU/VD2j1EXUGkLmFhYTh06BC6dOmiN8EAUDgQLcEYw72CMlx5WISrD6rD4FpWcY2jhIx4BmjraI52TmayIPB0NIOZDo5FJ40jIyMDXC5XdlwhLCxMzRU1PQoHopGelFUh5e5jXLz3WNYyKKmoOQjaO1vgdRcLdHSt/trSrpnej8Unqrtz5w4CAwPB4/GQmJiotyfgUjgQjfCouAJ/ZRTKHjcelSjMwzc0QDsnc1kIdHS1RCs7Uxhy6bYkpGHcvn0bgYGBePDgATw9PWFoqL+7SP3dcqI2jDHcLyzDnxmFSMkoxF93C3GvoExhvlZ2pnijhTU6u1nidVcLtHEwA4+CgDSSW7duITAwEA8fPoSnpycSEhLg6Oio7rLUhsKBNImsJ+VIvJGHc7fzkXK3EI+KK+V+b8ABvJzM0c3DGr4e1vBpYQ3bZnQJFdI0bt68iaCgIDx8+BBeXl5ISEhQuGCovqFwII2iSiTBhbuFSPw3D4k3cvHvo6dyv+dxOejkaok3PKzRzcMa3u5WOnnxMqL5bt68icDAQGRlZaFdu3Y4ffq03gcDQOFAGtDDJ+VIvJFb3UK4lY/Sqv+GlBpwgM5ulujRxg6+Hjbo0tySziUgGsHMzAzm5uawsrLC6dOnYW9vr+6SNAKFA1EZYwyXM58g7mo2Em/k4WaufOvAthkfPdrYIbCtPXq0toWlCV9NlRJSO0dHRyQkJMDAwICC4TkUDkRphaVVOHjpAX66kCnXXWTAAbo0t0Lgs0Bo72xOJ5gRjZSeno7U1FREREQAgF4feK4NhQOpF7GE4eytfPyUkokTaTkQiqsvySUwNEC/Do4I9nJAd2odEC2QlpaGXr16ITc3F8bGxnjnnXfUXZJGonAgdXrwuAz7LjzA/osP8PDJf5et7uBijvA3mmNQJ2dYGNOBZKId0tLSEBQUhNzcXHTu3Bk9evRQd0kai8KBKKgUiRGf9gixKZk4eysf0uv2mhsZ4p0uLhj+hhvaO1uot0hClPTPP/+gV69eyMvLQ5cuXRAfH1/r3SgJhQN5wYlrOZj/8zXkFP93FVP/VjYIf8MNIe0daYQR0UovBsPJkydhbW2t7rI0GoUDAQDklVRi4ZFr+O1qNgDAwVyAYd5uGO7jhuY2JmqujhDVZWdnIygoCPn5+fD29saJEycoGOqBwkHPMcaw/+IDLPktHUXlQnANOPiwR0tM792aWglEJzg6OmL06NFITEzEiRMnYGVlpe6StALdCU6PZRaWYc6hqzhzMx8A0N7ZHMuHdEQHFzqeQHQLYwzl5eUwMdGcVrCm79foKmZ6SCxh2HLmDvp+/TvO3MyHwNAAn/bzxM+TAygYiE64fPkyIiIiUFFRfeyMw+FoVDBoA+pW0jPXc4rx6YGr+DvzCQDA18MaXwzpCA9bU/UWRkgDuXTpEoKDg/H48WO4uLjgyy+/VHdJWonCQU9UiSRYm3AL6xJuQSRhMBMYIrq/F957w43OYiY64/lg8PX1xdy5c9VdktaicNADGfmlmL43FVceFAEA+rRzwGdvd4CjhZGaKyOk4Vy8eBHBwcF48uQJ/Pz8cOzYMY3sy9cWFA46jDGGA5ceYv7P/6CsSgwLYx4+f6cDBrzuBA6HWgtEd6SkpKBv37548uQJ/P39cfToUQqGV0ThoKOKyoWIOXQVv16pPm/B18MaX4d3hrOlsZorI6RhVVVVYdiwYXjy5AkCAgJw9OhRmJmZqbssrUejlXRQyt1C9P/mDH69kg2uAQezQtpi9//epGAgOonP5yM2NhYDBw6kYGhAdJ6DDhGJJVhz+hbWnL4JCQOaW5vgm/c6o0tzOumH6J6qqirw+dp7FWBN369Ry0FHZBaWIXzTeXxzqjoY3u3igt+mvUXBQHRScnIy2rRpg4sXL6q7FJ1F4aADfr2Shf7fnsHFe4/RTGCI1eGdsSq8M8zonsxEB507dw4hISG4d+8elixZou5ydBYdkNZiVSIJlsalY8e5uwCALs0t8e17XeBmTWeCEt30xx9/oF+/fnj69CmCgoLw448/qrsknUXhoKVyiiowefclXLz3GAAwKbAVovq0gSGXGoNEN509exahoaF4+vQpevXqhV9++YUuidGIKBy00Lnb+Zi2JxX5T6tgZmSIVcM7o087B3WXRUijOXPmDEJDQ1FaWorevXvjyJEjFAyNjMJBizDGsPH3O1hx7DokDPB0NMOGEd5oQddFIjpu1apVKC0tRXBwMI4cOQJjYxqW3dgoHLREcYUQH//0N06kPQIAvNvVBZ8Pfh3GfLrnAtF9u3btwtKlSxETE0PB0EToPActcD2nGB/9eAkZ+aXgcw2wYFA7RHRrTpfAIDrtzp078PDw0Nn/c03fr9HRSw135O8svPPdOWTkl8LF0hj7JvrhA193nX3DEAIAp0+fRocOHTBv3jxowedXnUTdShrs6NVsTN+bCsaA7q1t8c17XWBtqr1nhBJSH6dOncLAgQNRUVGB1NRUiMViGBrSrqqp0SuuoS7cLcT02MtgDHi/W3MsGdwBXLrvAtFxJ0+eRFhYGCoqKjBgwAAcOHCAgkFNqFtJA93KfYrxP1xAlUiCYC8HCgaiF+Lj42XBMHDgQBw4cAACgUDdZektCgcNk1tSgdHb/8KTMiE6u1lizftdKBiIzjtx4oQsGMLCwrB//34KBjWjcNAgpZUijN2RggePy9HCxgRbI31oqCrRC5mZmaisrMTbb79NwaAhqDNPQ4jEEkzefQn/PCyGtSkfO8Z0g00zeoMQ/TBu3Di4uLigV69eWn0Zbl1CLQcNwBjD3MP/IPFGHox4Btga6UNnPROdd/r0aeTl5cl+7tevHwWDBqFw0ADrk25jb0omDDjAmve70j0YiM779ddfERoail69eqGwsFDd5ZAaUDio2YlrOfjy+A0AwMJB7ekCekTn/fLLL3j33XdRVVUFLy8vuq2nhqJwUKP07GLMeHYuw8g33THKr4W6SyKkUf38888YMmQIhEIhhg8fjl27doHHo5tSaSKVwmHdunXw8PCAkZERvL29cebMmTrn37VrFzp16gQTExM4OTlhzJgxKCgoUKlgXZH/tBLjv7+AsioxAl6zwfywduouiZBGdfjwYQwbNgxCoRDvvfceBYOGUzocYmNjMWPGDMTExCA1NRXdu3dHaGgo7t+/X+P8Z8+exahRozBu3Dhcu3YN+/btQ0pKCsaPH//KxWurSpEYH/14EQ+fVA9Z/S6iK3h0kx6iw3777TdZMLz//vvYuXMnnfms4ZTeI61atQrjxo3D+PHj4eXlhdWrV8PNzQ3r16+vcf7z58+jRYsWmDZtGjw8PPDWW29hwoQJuHDhwisXr40YY5h76B+k3H0MMyNDbIl8A5YmNEKD6Lb27dvD2dkZERER+OGHHygYtIBS4VBVVYWLFy+ib9++ctP79u2Lc+fO1biMv78/Hjx4gLi4ODDG8OjRI+zfvx8DBgyo9XkqKytRXFws99AVW89mYN/FB89GJnXBa/bN1F0SIY2uRYsWOH/+PAWDFlEqHPLz8yEWi+HgID+ixsHBATk5OTUu4+/vj127diE8PBx8Ph+Ojo6wtLTEmjVran2eZcuWwcLCQvZwc3NTpkyNlXAjF0vj0gEAMQPaIbCtvZorIqTx7Nu3D4cPH5b97OTkBC6XzvjXFip1dL94LwHGWK33F0hLS8O0adMwf/58XLx4EceOHUNGRgYmTpxY6/qjo6NRVFQke2RmZqpSpka5lVuCabtTIWFAuI8bxga0UHdJhDSa2NhYvP/++xg2bJjediFrO6Xad7a2tuByuQqthNzcXIXWhNSyZcsQEBCAWbNmAQA6duwIU1NTdO/eHUuWLIGTk5PCMgKBQKeurVIlkmDK7lSUVIrQrYU1PhvcgW7WQ3TW3r17MWLECIjFYkRGRqJLly7qLomoQKmWA5/Ph7e3N+Lj4+Wmx8fHw9/fv8ZlysrKYGAg/zTSpqW+3OFpbcItXM8pgbUpH+tGdAXfkEYmEd20e/dufPDBBxCLxRgzZgy2bt1KXUlaSum9VFRUFLZs2YJt27YhPT0dM2fOxP3792XdRNHR0Rg1apRs/rCwMBw8eBDr16/HnTt38Mcff2DatGno1q0bnJ2dG25LNNQ/D4uwLuEWAOCztzvAli6mR3TUrl27MHLkSEgkEowdOxZbtmyhYNBiSg8bCA8PR0FBARYvXozs7Gx06NABcXFxcHd3BwBkZ2fLnfMwevRolJSUYO3atfi///s/WFpaolevXli+fHnDbYWGqhJJMGv/FYgkDP1fd8SAjopdaIToguTkZIwaNQoSiQTjx4/Hxo0bFXoMiHbhMC3o2ykuLoaFhQWKiopgbm6u7nLqbfXJf7H65E1YmfAQH9WTWg1EZ0kkEvzvf/8Dl8vFhg0bKBjqQdP3azTguJGkZRVj7enq7qRF1J1EdJR0pKKBgQE2b94MABQMOoL+io1AKJZg1v6/IZIwhLR3QBh1JxEdtGPHDnzwwQcQiUQAqkOBgkF3UMuhEWxIvI1rWcWwNOHRsFWik7Zv345x48aBMYY+ffpgzJgx6i6JNDCK+QZ2PacY356+CQBYNKg97M2M1FwRIQ1r69atsmCYPHkyRo8ere6SSCOgcGhgn/+WDqGYIdjLAYM66f5QXaJfNm/ejPHjx4MxhqlTp2LNmjXUMtZRFA4N6EZOCc7czIcBB1gQ1o7eNESnbNq0CR9++CEAYNq0afjmm2/of1yHUTg0oK1n7wAAQjs4wc3aRM3VENJwsrOzMWPGDADA9OnTsXr1agoGHUcHpBtIXkklDqdmAQDGdfdQczWENCwnJyf8/PPPOHXqFJYtW0bBoAcoHBrIzvP3UCWWoGtzS3RtbqXucghpEEVFRbCwsAAA9OnTB3369FFzRaSpULdSA6gQivHj+XsAgPHdW6q5GkIaxtq1a+Hp6Ynr16+ruxSiBhQODeBQ6kMUllbB1coYfdvVfOlyQrTJmjVrMHXqVOTk5ODAgQPqLoeoAYXDK5JIGLaezQAAjAnwgCGXXlKi3b755htMmzYNADB79mzMmTNHzRURdaA92StKupmHW7lPYSYwxHAfV3WXQ8gr+frrr2WjkqKjo7F06VI6+KynKBxe0dYz1a2G97q5wcyIp+ZqCFHdqlWrEBUVBQCIiYnB559/TsGgxygcXkF6djHO3soH14CDSP8W6i6HEJVVVVUhNjYWADBv3jx89tlnFAx6joayvoLvz90FAPRr7whXKzrpjWgvPp+P48eP46effsL//vc/CgZCLQdVPSmrwuHLDwGAWg1Ea6WkpMi+t7S0xIcffkjBQABQOKgsNiUTFUIJvJzM8UYLOumNaJ9ly5ahW7du+Prrr9VdCtFAFA4qEEsYdj476W20vzt90iJa5/PPP5cNUS0tLVVzNUQTUTio4FT6Izx4XA5LEx7e7uyi7nIIUcqSJUswd+5che8JeR6Fgwq+T74LAAh/ww1GPK56iyFECYsXL8a8efMAAEuXLkVMTIyaKyKaikYrKenmoxL8casABhxg5Jvu6i6HkHpbvHgxFixYAAD44osv8Omnn6q5IqLJKByUJG01BHs50PBVolWMjKpvWbtixQrMmjVLzdUQTUfhoITiCiEOXqoevjqahq8SLfPJJ58gMDAQ3bp1U3cpRAvQMQcl7LvwAGVVYrRxaAa/VjbqLoeQOjHGsGnTJhQXF8umUTCQ+qJwqCfGGHY9G746yq8FDV8lGo0xhpiYGEyYMAGhoaEQCoXqLoloGepWqqd/Hz3FnfxS8A0NMLgLDV8lmosxhjlz5uCLL74AAAwbNgw8Hl0UkiiHwqGe4tNyAABvvWaLZgJ62YhmYoxh9uzZWLFiBQD5ezMQogzay9VTfHouAKAP3emNaCjGGD755BN89dVXAKrv5jZlyhQ1V0W0FYVDPTwqrsDfmU8AAL097dVbDCG1+Oyzz2TBsHbtWkyePFnNFRFtRgek6+Fk+iMAQGc3S9ibG6m5GkJqNmTIENjb2+O7776jYCCvjFoO9XAyrTocqEuJaLL27dvjxo0bsLS0VHcpRAdQy+ElSitF+ON2AQAKB6JZGGP49NNPkZCQIJtGwUAaCoXDS5y5mYcqkQTuNiZobd9M3eUQAqA6GKZPn44VK1YgLCwMjx49UndJRMdQt9JLnJB2KXk50IlvRCMwxjBt2jSsXbsWHA4H33zzDRwcqFVLGhaFQx1EYglOX68ewhpMXUpEAzDGMGXKFKxbtw4cDgdbtmzB2LFj1V0W0UEUDnW4eO8xnpQJYWnCg4873QqUqJdEIsGUKVOwfv16cDgcbN26FWPGjFF3WURHUTjUIf5Zl1IvT3sYcunwDFGvbdu2yYJh+/btiIyMVHdJRIdRONRB1qXkRV1KRP0iIyNx4sQJDBw4EKNGjVJ3OUTHUTjU4l5BKe7kl8LQgIPurW3VXQ7RUxKJBABgYGAAHo+H2NhYGhhBmgT1ldQi8UYeAMCnhRXMjOiKlqTpSSQSfPjhh5g0aZIsJCgYSFOhlkMtEm5UdykFtaVrKZGmJ5FIMH78eGzfvh0GBgYYO3Ys3aiHNClqOdSgQihG8rOzogMpHEgTE4vFGDdunCwYdu3aRcFAmhy1HGqQfKcAlSIJnC2M0MaBzoomTUcsFmPs2LH44YcfwOVysWvXLoSHh6u7LKKHKBxqkPTseEPPtvbUx0uajFgsxpgxY7Bz505wuVzs2bMHw4YNU3dZRE+p1K20bt06eHh4wMjICN7e3jhz5kyd81dWViImJgbu7u4QCARo1aoVtm3bplLBTeG/4w12aq6E6JMLFy5g9+7d4HK52Lt3LwUDUSulWw6xsbGYMWMG1q1bh4CAAGzcuBGhoaFIS0tD8+bNa1xm+PDhePToEbZu3YrXXnsNubm5EIlEr1x8Y8jIL8W9gjLwuBwEvEZDWEnT8fX1xd69e8HhcDBkyBB1l0P0HIcxxpRZwNfXF127dsX69etl07y8vDB48GAsW7ZMYf5jx47hvffew507d2Btba1SkcXFxbCwsEBRURHMzc1VWkd9bTubgcW/piHgNRvsGv9moz4XISKRCPn5+XB0dFR3KaSJNeV+TRVKdStVVVXh4sWL6Nu3r9z0vn374ty5czUuc+TIEfj4+GDFihVwcXFBmzZt8PHHH6O8vLzW56msrERxcbHco6nQEFbSVEQiEUaMGAF/f3/cv39f3eUQIkepbqX8/HyIxWKFywM7ODggJyenxmXu3LmDs2fPwsjICIcOHUJ+fj4mTZqEwsLCWo87LFu2DIsWLVKmtAZRIRTjz4xCAEAgHW8gjUgkEuGDDz7ATz/9BB6Ph2vXrtXaLUuIOqh0QPrFETyMsVpH9UgkEnA4HNlY7f79+2PVqlXYsWNHra2H6OhoFBUVyR6ZmZmqlKm0a1lFqBJJYGcmQCs7GsJKGodQKERERIQsGA4cOIDQ0FB1l0WIHKVaDra2tuByuQqthNzc3FpvNuLk5AQXFxdYWFjIpnl5eYExhgcPHqB169YKywgEAggEAmVKaxBXHxQBADq6WNAQVtIohEIh3n//fRw4cAB8Ph8HDhzAwIED1V0WIQqUajnw+Xx4e3sjPj5ebnp8fDz8/f1rXCYgIABZWVl4+vSpbNq///4LAwMDuLq6qlBy47n6sPrYRgcXi5fMSYjyhEIh3nvvPVkwHDx4kIKBaCylu5WioqKwZcsWbNu2Denp6Zg5cybu37+PiRMnAqjuEnr+csIRERGwsbHBmDFjkJaWht9//x2zZs3C2LFjYWxs3HBb0gD+eVjdcqBwII2huLgY169fh0AgwOHDhzFgwAB1l0RIrZQ+zyE8PBwFBQVYvHgxsrOz0aFDB8TFxcHd3R0AkJ2dLTfyolmzZoiPj8fUqVPh4+MDGxsbDB8+HEuWLGm4rWgA5VVi3MwtAQC8TuFAGoGNjQ1Onz6Na9euoVevXuouh5A6KX2egzo0xXjgi/ceY8j6c7BtJkBKTG865kAaRFVVFZKSktCnTx91l0I0jE6d56DLpF1Kr7uYUzCQBlFZWYmhQ4ciJCQEP/zwg7rLIUQpdOG9Z/4LB+pSIq+usrISQ4YMwW+//QYjIyM4OzuruyRClEIth2eu0sFo0kAqKirw7rvv4rfffoOxsTF+/fVXBAcHq7ssQpRCLQdUnxl9M7d6qO3rrhQORHUVFRV45513cOzYMVkw0MFnoo0oHACkZxdDLGGwbcaHo7mRusshWkooFGLw4ME4fvw4jI2N8dtvvyEoKEjdZRGiEupWgvz5DXQwmqjK0NAQHTt2hImJCeLi4igYiFajcMB/xxvoYDR5FRwOB8uXL8fff/+NwMBAdZdDyCuhcABdNoOorqysDPPmzUNFRQWA6oB47bXX1FwVIa9O7485SCQMd/KqD0a3dTBTczVEm5SVlSEsLAynT5/G9evXsW/fPnWXREiD0fuWQ25JJSpFEnANOHCx0qxrPRHNVVpaioEDB+L06dNo1qwZZsyYoe6SCGlQet9yuFdQCgBwsTQGj6v3WUnqQRoMiYmJMDMzw7Fjx2q9KjEh2orCoaAMAOBuY6LmSog2KC0txYABA5CUlAQzMzMcP34cfn5+6i6LkAan9x+V7xVWtxwoHEh9REREICkpCebm5jhx4gQFA9FZFA7SloO1qZorIdpgzpw5aN68OU6cOIE333xT3eUQ0mioW4m6lYgSfH19cfPmTfD5fHWXQkij0uuWA2MMdwuk3UrUciCKiouLMXDgQFy4cEE2jYKB6AO9bjk8KROipEIEAGhuTS0HIq+4uBj9+vVDcnIy0tLScOPGDfB4PHWXRUiT0OuWw73C6i4lB3MBjPlcNVdDNElRURFCQkKQnJwMKysr7N+/n4KB6BW9bjlIz3Ggg9HkedJg+PPPP2FtbY2TJ0+iS5cu6i6LkCal5+FQ3XJoTgejyTNPnjxBSEgI/vrrL1hbW+PUqVPo3LmzussipMnpd7fSs3BoQeFAnlm8eDH++usv2NjY4PTp0xQMRG/pdcvh/rMT4JrTSCXyzJIlS5CVlYXo6Gh06tRJ3eUQojZ6HQ53ZSfAUctBn5WVlcHY2BgcDgcmJibYu3evuksiRO30tlupQihGXkklAMCNwkFvFRYW4q233kJMTAwYY+ouhxCNobfhkFtcHQx8QwNYmdAQRX1UUFCA3r17IzU1FVu2bEFubq66SyJEY+htOOQUV9+5y8nCiO4brYfy8/PRu3dvXL58Gfb29khISICDg4O6yyJEY+jtMYfsonIAgIO5kZorIU1NGgxXrlyBg4MDTp8+jXbt2qm7LEI0it6Gw6PnWg5Ef+Tl5aF37964evUqHBwckJCQAC8vL3WXRYjG0d9upaLqYw6O1HLQK0lJSbh69SocHR2RmJhIwUBILfS25ZBTXN2t5EgtB70ydOhQfP/99+jWrRs8PT3VXQ4hGkt/w6GouluJWg6679GjRzAwMICdnR0AYNSoUWquiBDNp8fdStXh4EAtB52Wk5ODoKAg9OrVC3l5eeouhxCtoZfhIJEw5D47AY4OSOsuaTCkp6fj8ePHKC4uVndJhGgNvQyH/NJKiCQMBhzArplA3eWQRpCdnY2goCBcv34drq6uSEpKQqtWrdRdFiFaQy/DQdqlZGcmgCFXL18CnfZ8MLi5uSExMZGCgRAl6eWekQ5G666srCwEBgbixo0baN68OQUDISrSz3B4dgIcDWPVPUKhEJWVlXB3d0diYiJatmyp7pII0Up6OZSVWg66SxoKANCiRQu11kKINtPLlkPWk+oT4JwtjdVcCWkImZmZiIuLk/3cokULCgZCXpGehkN1y4HCQfvdv38fgYGBePvtt+UCghDyavQyHB5Sy0En3Lt3D4GBgbhz5w6aN2+O119/Xd0lEaIz9C4cxBImOyDtQuGgte7evYvAwEBkZGSgVatWSExMhJubm7rLIkRn6N0B6bySSoglDIYGHNiZ0Qlw2kgaDPfu3cNrr72GhIQEuLq6qrssQnSK3rUcpF1KDuZG4BrQHeC0zaNHj2TB0Lp1ayQmJlIwENII9C4cpCOVqEtJO9nZ2SE4OBht2rRBQkICXFxc1F0SITpJpXBYt24dPDw8YGRkBG9vb5w5c6Zey/3xxx8wNDRE586dVXnaBvHfMFY6x0EbGRgYYNOmTTh37hwFAyGNSOlwiI2NxYwZMxATE4PU1FR0794doaGhuH//fp3LFRUVYdSoUejdu7fKxTYEOsdB+9y+fRvTpk2DSCQCUB0QNjY2aq6KEN2mdDisWrUK48aNw/jx4+Hl5YXVq1fDzc0N69evr3O5CRMmICIiAn5+fioX2xAe0jkOWuXWrVvo2bMn1qxZg5iYGHWXQ4jeUCocqqqqcPHiRfTt21duet++fXHu3Llal9u+fTtu376NBQsW1Ot5KisrUVxcLPdoKHTMQXvcvHkTPXv2xMOHD+Hl5YWZM2equyRC9IZS4ZCfnw+xWAwHBwe56Q4ODsjJyalxmZs3b2L27NnYtWsXDA3rN3J22bJlsLCwkD0acvx6VhF1K2mDf//9F4GBgcjKykK7du2QkJAAR0dHdZdFiN5Q6YA0hyM/BJQxpjANAMRiMSIiIrBo0SK0adOm3uuPjo5GUVGR7JGZmalKmQrKqkR4UiYEADjRAWmNdePGDVkwtG/fHgkJCQofSAghjUupk+BsbW3B5XIVWgm5ubk1vnlLSkpw4cIFpKamYsqUKQAAiUQCxhgMDQ1x4sQJ9OrVS2E5gUAAgaDhT1DLLa6+Nagxjwszgd6d/6cVhEIh+vfvj+zsbLz++us4deoU7Ozs1F0WIXpHqZYDn8+Ht7c34uPj5abHx8fD399fYX5zc3NcvXoVly9flj0mTpyItm3b4vLly/D19X216pUkvW+0vbmgxpYOUT8ej4eNGzfC19eXgoEQNVL643NUVBRGjhwJHx8f+Pn5YdOmTbh//z4mTpwIoLpL6OHDh/jhhx9gYGCADh06yC1vb28PIyMjhelNIU8aDnTZDI3zfNdkcHAwevfuTQFOiBopHQ7h4eEoKCjA4sWLkZ2djQ4dOiAuLg7u7u4Aqu/f+7JzHtQlt6R6GKu9GR1v0CRpaWmIiIjA3r174enpCUDxuBYhpGlxGGNM3UW8THFxMSwsLFBUVARzc3OV17P82HWsT7yN0f4tsHBQ+waskKjq2rVr6NWrF3JzcxEaGkr3ZCB6o6H2a41Fr66tJD0gTVdj1Qz//PMPgoKCkJubiy5dumDnzp3qLokQ8ox+hYOsW4nCQd2uXr2KoKAg5OXloWvXrjh58iRdEoMQDaJX4SA7IG1OxxzU6e+//0ZQUBDy8/Ph7e2NkydPwtraWt1lEUKeo5/hQC0HtZozZw4KCgrg4+OD+Ph4WFlZqbskQsgL9CYchGIJCkqrANAxB3XbtWsXPvzwQwoGQjSY3oRD/tPqVoOhAQfWJnw1V6N/8vLyZN9bWlpi48aNsLS0VF9BhJA66U04SEcq2TYTwIBuD9qkLl26hLZt22LVqlXqLoUQUk/6Ew7PXTqDNJ2LFy+id+/eePz4Mfbv3w+hUKjukggh9aA34VBYWh0ONqbUpdRULly4gODgYDx58gT+/v44duwYeDyeussihNSD3oRDUXn1J1ZLOt7QJFJSUmTBEBAQgGPHjmnkWaCEkJrpTThI7+NgYUyfXBvbX3/9heDgYBQVFeGtt97C0aNHYWZmpu6yCCFK0JtwkLYcKBwaX3JyMoqLi9G9e3cKBkK0lN7c8YbCoelMnz4ddnZ2GDRoEJo1a6bucgghKqCWA2kQFy9eRHFxsezniIgICgZCtJjehYOlCYVDQ/vjjz8QGBiIkJAQuYAghGgvvQsHajk0rLNnzyIkJARPnz6FqakpDA31pqeSEJ2mN+FAo5Ua3pkzZ9CvXz+UlpYiODgYR44cgYmJibrLIoQ0AL0IB4mEobjiWThQt1KD+P333xEaGorS0lL06dOHgoEQHaMX4VBSIYL0ZqjUcnh1zwdDSEgIfv75ZxgbG6u7LEJIA9KLDmLp8QZjHhcCQ66aq9F+tra2MDU1RY8ePXDo0CEYGdHNkwjRNXoVDtRqaBjt2rXDuXPn4OrqSsFAiI7Si3AoeHbRPSu66J7KTp06BQ6Hg169egEAXnvtNTVXRAhpTPoRDk+r7wBn24zCQRXx8fEYNGgQOBwOzp49i65du6q7JEJII9OLA9KFz24Pak0tB6WdOHECgwYNQkVFBXr37o327duruyRCSBPQi3AooHBQyfHjx2XBEBYWhv3790MgoJslEaIP9CMcnv53i1BSP8eOHcPbb7+NyspKvP322xQMhOgZvQgH6lZSTkpKiiwYBg8ejJ9++gl8Pr12hOgT/TggTeGglM6dO2PgwIFgjCE2NpZu7UmIHtKTcJB2K1E41AePx8PevXtl3xNC9I9+dCs9lbYcqM+8Nr/88gumTp0KiUQCoDoUKBgI0V8633KoEIpRWiUGANhQy6FGR44cwdChQyEUCtG5c2eMGzdO3SURQtRM51sO0uMNPC4HZgKdz0KlHT58WBYM4eHhiIyMVHdJhBANoPPh8F+XEh8cDkfN1WiWQ4cOYdiwYRAKhXjvvffw448/0s16CCEA9CEcyqrDwcqEupSed+DAAQwfPhwikQgRERHYuXMnBQMhREbnw6FKVH2A1YhHl+qWysnJwYgRIyASifDBBx/g+++/p2AghMjR+T2CSPxs9A2XupSkHB0d8eOPP+K3337D5s2bweVScBJC5Ol8OAgl1beAMzTQ+UbSS1VVVcnOdB4yZAiGDBmi5ooIIZpK5/eY0paDoZ63HGJjY9GhQwfcv39f3aUQQrSAHoSDtOWgv+GwZ88eRERE4ObNm9iwYYO6yyGEaAGdDwehRNpy0PlNrdHu3bsxYsQISCQSjB07FkuWLFF3SYQQLaDze0zxs2MO+nhA+scff8TIkSMhkUgwfvx4bN68GQZ07IUQUg86v6cQivXzgPTOnTsRGRkJiUSC//3vf9i4cSMFAyGk3nR+b6GPB6SFQiG+/PJLSCQSTJgwARs2bKBgIIQoReeHsoqk3Up6tHPk8XiIj4/Hli1bEB0dTcFACFGazu81hHrUcrh165bsewcHB8TExFAwEEJUotKeY926dfDw8ICRkRG8vb1x5syZWuc9ePAg+vTpAzs7O5ibm8PPzw/Hjx9XuWBlSYey8nR8tNLWrVvh6emJHTt2qLsUQogOUHqPGRsbixkzZiAmJgapqano3r07QkNDaz256vfff0efPn0QFxeHixcvIigoCGFhYUhNTX3l4uvjaaUIAGAq0N1LRGzZsgXjx4+HWCxusteVEKLjmJK6devGJk6cKDfN09OTzZ49u97raNeuHVu0aFG95y8qKmIAWFFRUb2XkYqKvczcP/2VrUu4pfSy2mDjxo0MAAPApk+fziQSibpLIoTUw6vs15qCUi2HqqoqXLx4EX379pWb3rdvX5w7d65e65BIJCgpKYG1tXWt81RWVqK4uFjuoaqSCiEAwMxI9469b9y4ERMmTAAAzJgxA19//TXds4IQ0iCUCof8/HyIxWI4ODjITXdwcEBOTk691rFy5UqUlpZi+PDhtc6zbNkyWFhYyB5ubm7KlCmnpKK6W0nXwmH9+vWYOHEiAGDmzJlYtWoVBQMhpMGodJT2xZ0QY6xeO6Y9e/Zg4cKFiI2Nhb29fa3zRUdHo6ioSPbIzMxUpUwA/x1zMDfiqbwOTSQ9xvN///d/WLlyJQUDIaRBKfVx2tbWFlwuV6GVkJubq9CaeFFsbCzGjRuHffv2ITg4uM55BQIBBAKBMqXVStqt1EzHWg5Lly6VDQagYCCENDSlWg58Ph/e3t6Ij4+Xmx4fHw9/f/9al9uzZw9Gjx6N3bt3Y8CAAapVqiJd6lY6dOgQysvLAVS33vr370/BQAhpFEp3K0VFRWHLli3Ytm0b0tPTMXPmTNy/f1/W/x0dHY1Ro0bJ5t+zZw9GjRqFlStX4s0330ROTg5ycnJQVFTUcFtRh5JKaThod7fS6tWr8e677+Kdd96BUChUdzmEEB2n9Mfp8PBwFBQUYPHixcjOzkaHDh0QFxcHd3d3AEB2drbcOQ8bN26ESCTC5MmTMXnyZNn0yMjIRj9hq1Iklt1DWptbDl9//TWioqIAAN7e3nS/Z0JIo+Mwxpi6i3iZ4uJiWFhYoKioCObm5vVeruBpJbyXnAQA3F7aH1wtvOHPypUr8fHHHwMAYmJi8Nlnn1FXEiE6QNX9WlPR6WtKlFWJAQBGPAOtDIavvvpKFgzz5s2jYCCENBmdDodyYXU4mPC1rxtm9erVmDVrFgBgwYIFWLx4MQUDIaTJaN9eUwnSloMxT/uuq+Tn5wdzc3NERUVhwYIF6i6HEKJndDwcqkcqmfC1Lxx8fX2RlpYGFxcXdZdCCNFDut2tJG05aEk4rFy5EhcuXJD9TMFACFEXHW85aE+30pIlSzBv3jxYWloiPT0djo6O6i6JEKLHdLvlIDsgrdnh8Nlnn2HevHkAgE8++YSCgRCidrodDlWaP1pp0aJFmD9/PgDgiy++QHR0tJorIoQQPelWMtLQbqWFCxdi0aJFAIDly5fjk08+UXNFhBBSTafDQXrpDCOe5jWQfvjhB1kwfPnll7KT3QghRBPodDgIxdXhwONqXjgMHToUO3fuRGhoqOy6SYQQoin0JBw048xi6WWsOBwOTExMcOzYMXC5mtnlRQjRb5r3kboBVWlQy4Exhjlz5iAmJkYWEhQMhBBNpSctB/WGA2MM0dHRWL58OQAgLCwMfn5+aq2JEELqotvhIKr+hM43VF84MMbw6aef4ssvvwQArFmzhoKBEKLxdDsc1HzMgTGGWbNmYeXKlQCAtWvXyt3wiBBCNJVOh4P0mIOhQdO3HBhj+Pjjj7Fq1SoAwLp16/DRRx81eR2EEKIKnQ4Hkbi6W4mnhm6lv/76SxYM69evl91jmxBCtIFOh4OsW0kNd4Hz9fXFli1bIBaL8eGHHzb58xNCyKvQ6XAQSapbDoZNNFqJMSa7LywAjBs3rkmelxBCGpr6TwBoROzZ16ZoNzDGMHXqVAQEBCAvL68JnpEQQhqPbofDs5PNGvt4NGMMU6ZMwXfffYe0tDQkJSU17hMSQkgj0+lwkEjDgdN4bQeJRILJkydj3bp14HA42Lp1K4YOHdpoz0cIIU1Bp485SKqPR4PTSOEgkUgwadIkbNy4ERwOB9u3b0dkZGSjPBchhDQlnQ4HBmnLoeHXLZFIMHHiRGzevBkcDgc7duzAqFGjGv6JCCFEDXQ6HJ4NVmqUbqX8/HzEx8fDwMAA33//PUaMGNHgz0EIIeqi0+Egu0R2I6zb3t4eCQkJuHDhAh1jIIToHJ0Oh8pnd4ITNNCd4CQSCS5dugQfHx8AQIsWLdCiRYsGWTchhGgSnR6tVFopAgCY8F89A8ViMcaNGwc/Pz8cPnz4lddHCCGaTMfDQQwAaCZ4tXCQBsOOHTvAGENlZWVDlEcIIRpLp7uVSqukLQfV77gmFosxZswY7Ny5E1wuF3v27MGwYcMaqkSiJSQSCaqqqtRdBtEiPB5Pq+/2qLPhwBiTdSuZqthyEIvFiIyMxK5du8DlcrF37146+KyHqqqqkJGRAYn0xBlC6snS0hKOjo6Ndq5VY9LZcKgUSWRDWVUJB7FYjFGjRmH37t0wNDTE3r17MWTIkAaukmg6xhiys7PB5XLh5uYGAzXcG4RoH8YYysrKkJubCwBwcnJSc0XK09lwePqs1QAAxjzlm3YcDgcmJiYwNDREbGws3n333YYsj2gJkUiEsrIyODs7w8TERN3lEC1ibGwMAMjNzYW9vb3WdTHp7Mcg6Y1+DA044KpwirSBgQE2btyI8+fPUzDoMbG4elADn89XcyVEG0k/UAiFQjVXojydDQdVCIVCfPvtt7I/pIGBAby9vdVcFdEE2thnTNRPm/9vKByeEQqFiIiIwPTp0zF27Fh1l0MIIWqls8cclCEUCvH+++/jwIED4PP5CA8PV3dJhBCiVnrfcqiqqkJ4eLgsGA4ePIiBAwequyxCdE5lZSWmTp0KW1tbmJqaYtCgQXjw4EGdy5SUlGDGjBlwd3eHsbEx/P39kZKSIjfPwYMHERISAltbW3A4HFy+fFnu93fv3gWHw6nxsW/fPtk848aNg4eHB4yNjdGqVSssWLBA7tyWHTt21Loe6agkXaLX4SANhkOHDkEgEODw4cMYMGCAussiRCfNmDEDhw4dwt69e3H27Fk8ffoUAwcOlB30r8n48eMRHx+PnTt34urVq+jbty+Cg4Px8OFD2TylpaUICAjAF198UeM63NzckJ2dLfdYtGgRTE1NERoaCgC4fv06JBIJNm7ciGvXruHrr7/Ghg0bMGfOHNl6wsPDFdYTEhKCnj17wt7evoFeJQ3CtEBRUREDwIqKiuq9TPaTcub+6a+sVfRvtc7z/vvvMwBMIBCwo0ePNkSpRMeUl5eztLQ0Vl5eru5SlHb06FEWEBDALCwsmLW1NRswYAC7desWY4yxhIQEBoA9fvxYNn9qaioDwDIyMmTTzp49y3r06MGMjY2ZpaUl69u3LyssLFS6lidPnjAej8f27t0rm/bw4UNmYGDAjh07VuMyZWVljMvlsl9//VVueqdOnVhMTIzC/BkZGQwAS01NfWk9nTt3ZmPHjq1znhUrVjAPD49af5+bm8t4PB774Ycfap2nrv8fVfZrTUmvWw5jx46FlZUVfv75Z/Tr10/d5RAtwBhDWZVILQ/27BL09VVaWoqoqCikpKTg1KlTMDAwwDvvvFPvM70vX76M3r17o3379khOTsbZs2cRFhYm+6S/dOlSNGvWrM7HmTNnAAAXL16EUChE3759Zet3dnZGhw4dcO7cuRqfXyQSQSwWw8jISG66sbExzp49q9Rr8byLFy/i8uXLGDduXJ3zFRUVwdrautbf//DDDzAxMdHZqybo9QHp4OBgZGRkwMLCQt2lEC1RLhSj3fzjannutMUhSl1h+MUz+rdu3Qp7e3ukpaXVa/kVK1bAx8cH69atk01r37697PuJEydi+PDhda7DxcUFAJCTkwM+nw8rKyu53zs4OCAnJ6fGZc3MzODn54fPPvsMXl5ecHBwwJ49e/Dnn3+idevW9dqGmmzduhVeXl7w9/evdZ7bt29jzZo1WLlyZa3zbNu2DREREbKT3XSNXrUcKisrMWbMGKSnp8umUTAQXXX79m1ERESgZcuWMDc3h4eHBwDg/v379Vpe2nKojbW1NV577bU6Hy/bcTLG6jwXYOfOnWCMwcXFBQKBAN9++y0iIiJUPtu4vLwcu3fvrrPVkJWVhX79+mHYsGEYP358jfMkJycjLS3tpa0PbaY3LYeKigoMGTIEcXFx+P3333H9+nXweDx1l0W0jDGPi7TFIWp7bmWEhYXBzc0NmzdvhrOzMyQSCTp06ICqqio0a9YMAOS6ql48i/dlO/alS5di6dKldc5z9OhRdO/eHY6OjqiqqsLjx4/lWg+5ubl1foJv1aoVkpKSUFpaiuLiYjg5OSE8PFwWdMrav38/ysrKar3fe1ZWFoKCguDn54dNmzbVup4tW7agc+fOOn2SrEoth3Xr1sHDwwNGRkbw9vaW9SvWJikpCd7e3jAyMkLLli2xYcMGlYpVVUVFBd555x3ExcXB2NgYmzdvpmAgKuFwODDhG6rloczZtgUFBUhPT8fcuXPRu3dveHl54fHjx7Lf29nZAQCys7Nl014cAtqxY0ecOnWq1ueYOHEiLl++XOdDetdEb29v8Hg8xMfHy5bPzs7GP//8U2c4SJmamsLJyQmPHz/G8ePH8fbbb9frdXjR1q1bMWjQINn2P+/hw4cIDAxE165dsX379lovsvj06VP89NNPOt1qAKD8aKW9e/cyHo/HNm/ezNLS0tj06dOZqakpu3fvXo3z37lzh5mYmLDp06eztLQ0tnnzZsbj8dj+/fvr/ZyvOlopJCSEAWAmJiYsISGh3usgRFtHK4nFYmZjY8NGjBjBbt68yU6dOsXeeOMNBoAdOnSIVVVVMTc3NzZs2DB248YN9uuvv7K2bdvKjVa6ceMG4/P57KOPPmJ///03S09PZ+vWrWN5eXkq1TRx4kTm6urKTp48yS5dusR69erFOnXqxEQikWyeXr16sTVr1sh+PnbsGDt69Ci7c+cOO3HiBOvUqRPr1q0bq6qqks1TUFDAUlNT2W+//cYAsL1797LU1FSWnZ0t9/w3b95kHA6nxpGJDx8+ZK+99hrr1asXe/DgAcvOzpY9XrRlyxZmZGRUr1Fb2jxaSelw6NatG5s4caLcNE9PTzZ79uwa5//kk0+Yp6en3LQJEyawN998s97P+Srh4P7Jz7JgSExMrPfyhDCmveHAGGPx8fHMy8uLCQQC1rFjR5aYmCgLB8aqh6m+/vrrzMjIiHXv3p3t27dPYShrYmIi8/f3ZwKBgFlaWrKQkBC54a/KKC8vZ1OmTGHW1tbM2NiYDRw4kN2/f19uHnd3d7ZgwQLZz7Gxsaxly5aMz+czR0dHNnnyZPbkyRO5ZbZv384AKDyeXw9jjEVHRzNXV1cmFosVaqttHTV9fvbz82MRERH13mZtDQcOY/UfH1dVVQUTExPs27cP77zzjmz69OnTcfnyZSQlJSks06NHD3Tp0gXffPONbNqhQ4cwfPhwlJWV1di9U1lZKXcrzuLiYri5uaGoqAjm5ub1qjWnqAJvLjsFJhahYP0HiIuLQ48ePeq7qYQAqO6SzMjIkHWjEqKMuv5/iouLYWFhodR+rSkpdcwhPz8fYrEYDg4OctPrGo6Wk5NT4/wikQj5+fk1LrNs2TJYWFjIHm5ubsqUKYfD4eDo0aMUDIQQogSVRiu9eGCMvWQ4Wk3z1zRdKjo6GlFRUbKfpS0HZVia8LBhRFcAQPcO2ncXJkIIUSelwsHW1hZcLlehlZCbm6vQOpBydHSscX5DQ0PY2NjUuIxAIIBAIFCmNAVGPC76USgQQohKlOpW4vP58Pb2lhuOBgDx8fG1Dkfz8/NTmP/EiRPw8fGh4aSEEKKhlD7PISoqClu2bMG2bduQnp6OmTNn4v79+5g4cSKA6i6h508wmThxIu7du4eoqCikp6dj27Zt2Lp1Kz7++OOG2wpCCCENSuljDuHh4SgoKMDixYuRnZ2NDh06IC4uDu7u7gCqT2x5/vR8Dw8PxMXFYebMmfjuu+/g7OyMb7/9VuG6L4RoMiUG9REiU9+LHGoipYayqoumD/kiukssFuPmzZswMTGBnZ2dVt8TmDQdxhiqqqqQl5cHsViM1q1bK5xxren7Nb25thIhquByuXB1dcWDBw9w9+5ddZdDtIyJiQmaN29e66U4NBmFAyEv0axZM7Ru3VrhwnSE1IXL5cLQULlrYmkSCgdC6oHL5ap8mWhCtJH2tXUIIYQ0OgoHQgghCigcCCGEKNCKYw7S0bbFxcVqroQQQhqGdH+mqWcTaEU4lJSUAMArXZ2VEEI0UUlJiUbey14rToKTSCTIysqCmZmZUsPCpFdzzczM1MiTTF6Vrm8foPvbSNun/VTdRsYYSkpK4OzsrJHnQWhFy8HAwACurq4qL29ubq6z/5iA7m8foPvbSNun/VTZRk1sMUhpXlwRQghROwoHQgghCnQ6HAQCARYsWPDKNw7SVLq+fYDubyNtn/bT1W3UigPShBBCmpZOtxwIIYSohsKBEEKIAgoHQgghCigcCCGEKND6cFi3bh08PDxgZGQEb29vnDlzps75k5KS4O3tDSMjI7Rs2RIbNmxookpVo8z2HTx4EH369IGdnR3Mzc3h5+eH48ePN2G1ylP27yf1xx9/wNDQEJ07d27cAhuAsttYWVmJmJgYuLu7QyAQoFWrVti2bVsTVas8Zbdv165d6NSpE0xMTODk5IQxY8agoKCgiapVzu+//46wsDA4OzuDw+Hg8OHDL11G2/YxtWJabO/evYzH47HNmzeztLQ0Nn36dGZqasru3btX4/x37txhJiYmbPr06SwtLY1t3ryZ8Xg8tn///iauvH6U3b7p06ez5cuXs7/++ov9+++/LDo6mvF4PHbp0qUmrrx+lN0+qSdPnrCWLVuyvn37sk6dOjVNsSpSZRsHDRrEfH19WXx8PMvIyGB//vkn++OPP5qw6vpTdvvOnDnDDAwM2DfffMPu3LnDzpw5w9q3b88GDx7cxJXXT1xcHIuJiWEHDhxgANihQ4fqnF/b9jF10epw6NatG5s4caLcNE9PTzZ79uwa5//kk0+Yp6en3LQJEyawN998s9FqfBXKbl9N2rVrxxYtWtTQpTUIVbcvPDyczZ07ly1YsEDjw0HZbTx69CizsLBgBQUFTVHeK1N2+7788kvWsmVLuWnffvstc3V1bbQaG0p9wkHb9jF10dpupaqqKly8eBF9+/aVm963b1+cO3euxmWSk5MV5g8JCcGFCxc07v7AqmzfiyQSCUpKSmBtbd0YJb4SVbdv+/btuH37NhYsWNDYJb4yVbbxyJEj8PHxwYoVK+Di4oI2bdrg448/Rnl5eVOUrBRVts/f3x8PHjxAXFwcGGN49OgR9u/fjwEDBjRFyY1Om/YxL6MVF96rSX5+PsRiMRwcHOSmOzg4ICcnp8ZlcnJyapxfJBIhPz8fTk5OjVavslTZvhetXLkSpaWlGD58eGOU+EpU2b6bN29i9uzZOHPmDAwNNf9fV5VtvHPnDs6ePQsjIyMcOnQI+fn5mDRpEgoLCzXuuIMq2+fv749du3YhPDwcFRUVEIlEGDRoENasWdMUJTc6bdrHvIzWthykXryEN2Oszst61zR/TdM1hbLbJ7Vnzx4sXLgQsbGxsLe3b6zyXll9t08sFiMiIgKLFi1CmzZtmqq8BqHM31AikYDD4WDXrl3o1q0b+vfvj1WrVmHHjh0a2XoAlNu+tLQ0TJs2DfPnz8fFixdx7NgxZGRkYOLEiU1RapPQtn1MbTT/41ctbG1tweVyFT6h5ObmKiS3lKOjY43zGxoawsbGptFqVYUq2ycVGxuLcePGYd++fQgODm7MMlWm7PaVlJTgwoULSE1NxZQpUwBU70gZYzA0NMSJEyfQq1evJqm9vlT5Gzo5OcHFxUXuUs5eXl5gjOHBgwdo3bp1o9asDFW2b9myZQgICMCsWbMAAB07doSpqSm6d++OJUuWaNUn65po0z7mZbS25cDn8+Ht7Y34+Hi56fHx8fD3969xGT8/P4X5T5w4AR8fH/B4vEarVRWqbB9Q3WIYPXo0du/erdH9uMpun7m5Oa5evYrLly/LHhMnTkTbtm1x+fJl+Pr6NlXp9abK3zAgIABZWVl4+vSpbNq///77yvc0aQyqbF9ZWZnCjW24XC4Azb1dpjK0aR/zUmo6EN4gpMPotm7dytLS0tiMGTOYqakpu3v3LmOMsdmzZ7ORI0fK5pcOM5s5cyZLS0tjW7du1ehhZspu3+7du5mhoSH77rvvWHZ2tuzx5MkTdW1CnZTdvhdpw2glZbexpKSEubq6sqFDh7Jr166xpKQk1rp1azZ+/Hh1bUKdlN2+7du3M0NDQ7Zu3Tp2+/ZtdvbsWebj48O6deumrk2oU0lJCUtNTWWpqakMAFu1ahVLTU2VDdXV9n1MXbQ6HBhj7LvvvmPu7u6Mz+ezrl27sqSkJNnvIiMjWc+ePeXmT0xMZF26dGF8Pp+1aNGCrV+/vokrVo4y29ezZ08GQOERGRnZ9IXXk7J/v+dpQzgwpvw2pqens+DgYGZsbMxcXV1ZVFQUKysra+Kq60/Z7fv2229Zu3btmLGxMXNycmIffPABe/DgQRNXXT8JCQl1vqd0YR9TG7pkNyGEEAVae8yBEEJI46FwIIQQooDCgRBCiAIKB0IIIQooHAghhCigcCCEEKKAwoEQQogCCgdCCCEKKBwIIYQooHAghBCigMKBEEKIAgoHQgghCv4fjuXtrIN1wIYAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "if True :\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
        "    ax.plot([0, 1], [0, 1], 'k--')\n",
        "    aucf_t = auc(fpr_t, tpr_t)\n",
        "    ax.plot(fpr_t, tpr_t, label='auc=%1.5f' % aucf_t)\n",
        "    ax.set_title('Courbe ROC - classifieur de Polarités du test')\n",
        "    ax.legend();\n",
        "\n",
        "print(\"Remarquer l'AUC\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAOk75-glPQ6"
      },
      "source": [
        "#### 2.5.3 Comparaison des ROC du train et du test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "qRv0cNPtlPQ6",
        "outputId": "76a0eb9f-5ff9-4727-8fc5-a0b3048a2246"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Les AUCs train/test:', 1.0, 0.9172693652393754)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "aucf_t = auc(fpr_t, tpr_t)\n",
        "aucf_a = auc(fpr_a, tpr_a)\n",
        "fig, axs = plt.subplots(1, 2, figsize=(8,4))\n",
        "\n",
        "axs[0].plot(fpr_a, tpr_a, label='auc=%1.5f' % aucf_t)\n",
        "axs[0].set_title('Courbe ROC - Apprentissage')\n",
        "axs[1].plot(fpr_t, tpr_t, label='auc=%1.5f' % aucf_a)\n",
        "axs[1].set_title('Courbe ROC - TEST')\n",
        "\n",
        "fig.suptitle('RendomForest :  Courbe ROC du classifieur de Polarités des données')\n",
        "(\"Les AUCs train/test:\", aucf_a, aucf_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZS6AT0QlPQ6"
      },
      "source": [
        "# 3. A propose des classifieurs en Text Mining (espace vectoriel) appliqués sur cette DB.\n",
        "-  La transformation du texte en feature génère beaucoup de variables;\n",
        "- __RF__ : Random Forest est une méta méthode aggrégative. Il peut y avoir des problèmes avec RF.\n",
        "    - Intuitivement, il y a un grand risque d'overfitting dans une espace à plusieurs centaines de dimensions, et donc quasiment vide d'échantillons.\n",
        "    - Un des aspects à prendre en compte dans RF est la profondeur de l'arbre. Par défaut, elle est de 10, soit  210=1024  décisions de seuils, soit au mieux  210  variables ce qui est loin du nombre de variables total.\n",
        "\n",
        "- __AD__ :  en présence de beaucoup de variables et un arbre de décision n'exploite quasiment que le fait qu'elles soient non nulles.\n",
        "    - Un arbre de décision consiste à prendre des décisions sur des seuils puis retourne une constante tirée d'une feuille de l'arbre.\n",
        "- __Modèles Linéaires__ : Un modèle linéaire ferait tout aussi bien l'affaire avec en plus la possibilité de tenir compte de la valeur de la variable.\n",
        "    - __MNB__ : le Multinomial Naive Bayes (utilisé pour la détection de spam), qui marche  bien sur BOW (les sacs de mots)\n",
        "    - __Logit__ :  La régression logistique fonctionne assez bien.\n",
        "    - __SVM Linéaire__ : LinearSVM est assez efficace aussi sur ces modèles.\n",
        "\n",
        "- Ces classificateurs améliorent sensiblement les performance des modèles\n",
        "\n",
        "- Les __n-grammes__ n'améliorent pas significativement les performances,\n",
        "- __SVD__ détériore parfois les performances\n",
        "- __word2vect__ améliore légèrement les performances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fewlgXYlPQ7",
        "outputId": "ac1b51c8-3c63-4602-b244-09c95389f5b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 22973)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fit_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMxsBMrllPQ7"
      },
      "source": [
        "## 3.1-__modèle linéaire__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0v3GaYylPQ7"
      },
      "source": [
        "Essayons d'abord avec un __modèle linéaire__ (__logit__)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdjYiVUNlPQ7",
        "outputId": "141d4a33-4808-4f25-a54d-e77a6ffa434e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8379"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(solver='lbfgs')\n",
        "lr.fit(fit_train, y_train)\n",
        "lr.score(fit_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRhljMFzlPQ7"
      },
      "source": [
        "**On constae une amélioration du score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdcff82llPQ7"
      },
      "source": [
        "## 3.2- Multinomial Bayesian (MNB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNJ0jUhwlPQ7",
        "outputId": "6e233c14-f8f3-48bd-fc15-c6b5017b4f91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8338"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(fit_train, y_train)\n",
        "mnb.score(fit_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FFFNx19lPQ7"
      },
      "source": [
        "**On obtient encore un bon score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOkrvr_YlPQ7"
      },
      "source": [
        "## 3.3- Retour au RandomForest\n",
        "* Si on augmente la profondeur de l'arbre, la forêt aléatoire peut __parfois__ être plus performante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQzOx5oklPQ7",
        "outputId": "7aebe7d4-31df-449e-dd8c-ba116e69a838"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8396"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf20 = RandomForestClassifier(n_estimators=120, max_depth=20)\n",
        "clf20.fit(fit_train, y_train)\n",
        "clf20.score(fit_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgUaqa6ilPQ7"
      },
      "source": [
        "**On obtient un score pas tellement mieux.**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUaEF8NKlPQ7"
      },
      "source": [
        "### 3.3.1 Itération su Random Forest   \n",
        "Essayons d'autre combinaisons des paramètres dans une itération"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "csxNuinalPQ7",
        "outputId": "4ea63f33-c26e-45c8-f960-a1429fad9e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clf1,10  : score =  0.5937\n",
            "clf1,30  : score =  0.6486\n",
            "clf1,50  : score =  0.6191\n",
            "clf1,70  : score =  0.641\n",
            "clf1,90  : score =  0.625\n",
            "clf51,10  : score =  0.8\n",
            "clf51,30  : score =  0.8267\n",
            "clf51,50  : score =  0.8317\n",
            "clf51,70  : score =  0.8338\n",
            "clf51,90  : score =  0.8228\n",
            "clf101,10  : score =  0.8237\n",
            "clf101,30  : score =  0.8407\n",
            "clf101,50  : score =  0.8425\n",
            "clf101,70  : score =  0.8436\n",
            "clf101,90  : score =  0.8484\n",
            "clf151,10  : score =  0.8254\n",
            "clf151,30  : score =  0.8413\n",
            "clf151,50  : score =  0.8489\n",
            "clf151,70  : score =  0.8503\n",
            "clf151,90  : score =  0.85\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clfs=[(\"clf\"+str(N)+','+str(depth), RandomForestClassifier(n_estimators=N, max_depth=depth))  for N in range(1,200,50) for depth in range(10,100,20)]\n",
        "for nom, _clf_ in clfs :\n",
        "    _clf_.fit(fit_train, y_train)\n",
        "    print(nom, ' : score = ', _clf_.score(fit_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cf90xoHlPQ8"
      },
      "source": [
        "**La dernière combinaison semble meilleure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzcSm2lylPQ8"
      },
      "source": [
        "### 3.3.2 Une des meilleures combinaisons (?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wU8JyhyWlPQ8",
        "outputId": "2e69bb74-d97f-4329-d3b7-66547d08f39b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8461"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf50_ = RandomForestClassifier(n_estimators=150, max_depth=90)\n",
        "clf50_.fit(fit_train, y_train)\n",
        "clf50_.score(fit_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CGU5b8Y0lPQ8",
        "outputId": "51feb952-6f16-4bdb-fe29-6294f5be07a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8471"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf50_ = RandomForestClassifier(n_estimators=100, max_depth=70)\n",
        "clf50_.fit(fit_train, y_train)\n",
        "clf50_.score(fit_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtWeQBtblPQ8"
      },
      "source": [
        "#### 3.3.3.  Et une autre (200 arbres, profondeur max = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3uyhdT8lPQ8",
        "outputId": "8a2dc7c3-c9c3-47cb-c81c-3b971dfdeb40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6429"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf50 = RandomForestClassifier(n_estimators=200, max_depth=100)\n",
        "clf50.fit(fit_train, y_train)\n",
        "clf50.score(fit_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2S-TtCVlPQ8"
      },
      "source": [
        "## 3.4- Gradient Boost\n",
        "* Un méta-modèle de __gradient boosting__ devrait dépasser les RFs puisque les arbres ne sont plus appris indépendemment les uns des autres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKsJpxQQlPQ8",
        "outputId": "93a04755-09e8-48b1-bcb4-1cf35d2c6383"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5688"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gbc40 = GradientBoostingClassifier(n_estimators=200, max_depth=40)\n",
        "gbc40.fit(fit_train, y_train)\n",
        "gbc40.score(fit_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAXpiqcTlPQ8"
      },
      "source": [
        "## 3.5- SVM linéaire avec la méthode OneVsRestClassifier\n",
        "\n",
        "Connu également sous le nom de one-vs-all  \n",
        "dans cette méta-classifieur, on crée un classifieur per classe.\n",
        "\n",
        "**Cette méta-méthode a besoin** (en paramètre) d'une méthode de classification à utiliser.\n",
        "\n",
        "Extrait de la Doc :\n",
        "\n",
        "For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, __it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZZLSnYQlPQ8",
        "outputId": "a7922236-dd6c-42e8-ab7d-5b3aa631535b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8968"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "modele_one_vs_linear_SVC= OneVsRestClassifier(LinearSVC())\n",
        "modele_one_vs_linear_SVC.fit(fit_train, y_train)\n",
        "modele_one_vs_linear_SVC.score(fit_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYcztDwIlPQ8"
      },
      "source": [
        "__On constate que SVM se comporte (comme souvent) bien !__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjhwV1UflPQ8"
      },
      "source": [
        "## 3.6- Classifieur SVM linéaire avec la méthode OneVsRestClassifier\n",
        "\n",
        "Même chose mais on utilise la méthode SVM (pour la classification : SVC)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_A6w8dplPQ8",
        "outputId": "23ff2281-1ff7-4fcc-c4a7-815b845f0bad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6219"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "modele_one_vs_SVC = OneVsRestClassifier(SVC())\n",
        "modele_one_vs_SVC.fit(fit_train, y_train)\n",
        "modele_one_vs_SVC.score(fit_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV4w63s2lPQ9"
      },
      "source": [
        "<font color=\"red\"> **Et en fonction des score, on choisit une des méthodes.**</font>\n",
        "\n",
        "Pour simplifier, on choisit ici la régression logistique (**logit**) pour la suite.\n",
        "\n",
        "**MNB peut aussi faire l'affaire (voir ci-dessous).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYRw2xhzlPQ9"
      },
      "source": [
        "# 4- Les n-grammes\n",
        "L'approche Bag of Word (matrices Tf, TfIdf)  ci-dessus ne tient pas compte ni du contexte, ni de l'ordre des mots. Chaque phrase est convertie en un sac de mots (ou bag of words).\n",
        "\n",
        "On va tenir compte de séquence plus ou moins longue à l'ide des __n-grams__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpfVTHJdlPQ9"
      },
      "outputs": [],
      "source": [
        "# s'il faut télécharger des données\n",
        "#\n",
        "nltk_fait = True # A mettre dès le premier download (voir plus haut)\n",
        "if not nltk_fait :\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    nltk_fait == True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPFnM3ZDlPQ9"
      },
      "source": [
        "\n",
        "## 4.1 : Un exemple de 3-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyWus3yFlPQ9",
        "outputId": "6667ff76-7a91-45bb-8769-17b13d884274"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(None, None, 'Edward'),\n",
              " (None, 'Edward', 'Dmytryk'),\n",
              " ('Edward', 'Dmytryk', 'directed'),\n",
              " ('Dmytryk', 'directed', 'this'),\n",
              " ('directed', 'this', 'shadowy'),\n",
              " ('this', 'shadowy', 'movie'),\n",
              " ('shadowy', 'movie', 'about')]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "generated_ngrams = ngrams(word_tokenize(X_train.iloc[0,0]), 3, pad_left=True, pad_right=True)\n",
        "list(generated_ngrams)[:7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlcIUoH1lPQ9"
      },
      "source": [
        "## 4.2- Utilisation du bi-gram     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGd6Q60glPQ9"
      },
      "source": [
        "**On applique le principe de bi-gramme avec scikit-learn.**\n",
        "\n",
        "**On lémmatise,puis TfIdf sur les bi-grammes**\n",
        "\n",
        "Appliqué aux données de base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpIzl2z3lPQ9",
        "outputId": "9f52e698-32da-442a-d0ad-1133e613e694"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(40000, 2108120)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "pipe2 = make_pipeline(CountVectorizer(ngram_range=(1, 2)), TfidfTransformer())\n",
        "pipe2.fit(X_train['Avis'])\n",
        "fit_train2 = pipe2.transform(X_train['Avis'])\n",
        "fit_train2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTv8zAlGlPQ9"
      },
      "source": [
        "Il y a plus de colonnes (normal !)\n",
        "\n",
        "**On vérifie** que les features ressemblent à des couples de mots (non traités / pas de lemmatisation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlQkE3J6lPQ9",
        "outputId": "4d419154-cf85-4591-b981-8fbd3b2e41ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['000 audience', '000 bakhtiari', '000 bc', '000 bet', '000 big',\n",
              "       '000 bombs', '000 bone', '000 books', '000 br', '000 brazilians'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cl = pipe2.steps[0]\n",
        "cl[1].get_feature_names_out()[100:110]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9otwAMIlPQ9"
      },
      "source": [
        "* Gagné (vérifié) !  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqC1JCMtlPQ9",
        "outputId": "f51f92ce-f164-43c0-8fa3-a7d4ada7f627"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 2108120)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fit_test2 = pipe2.transform(X_test['Avis'])\n",
        "fit_test2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVachQbFlPQ-"
      },
      "source": [
        "### 4.2.1- Même aplication mais avec les données lemmatisées\n",
        "\n",
        "Appliqué aux données lemmatisées. (si Warning, lancez une 2e fois)     \n",
        "Si Warning, relancer !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byASs8BDlPQ-",
        "outputId": "12454cb1-9fa8-4b33-b624-5d313d0c3fe6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['acros', 'afterward', 'alway', 'anythe', 'everythe', 'hundr', 'indee', 'les', 'make', 'nevertheles', 'nothe', 'perhap', 'seriou', 'somethe', 'thu', 'u', 'wherea'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40000, 1471725)\n"
          ]
        }
      ],
      "source": [
        "#count_vec_lemmatise = CountVectorizer(tokenizer=lemma, stop_words=\"english\", analyzer='word',\n",
        "#                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
        "if True :\n",
        "    count_vec_lemmatise = CountVectorizer(tokenizer=lemma, stop_words=\"english\", analyzer='word',\n",
        "                                ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None)\n",
        "    pipe2_bis = make_pipeline(CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=\"english\", analyzer='word',\n",
        "                                ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None), TfidfTransformer())\n",
        "    pipe2_bis.fit(X_train['Avis'])\n",
        "    fit_train2_bis = pipe2_bis.transform(X_train['Avis'])\n",
        "    print(fit_train2_bis.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKUUHKwZlPQ-"
      },
      "outputs": [],
      "source": [
        "if True :\n",
        "    cl_bis = pipe2_bis.steps[0]\n",
        "    cl_bis[1].get_feature_names_out()[100:110]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvCkftVlPQ-",
        "outputId": "ff089ad3-0486-4b9e-8fe2-eb1e6f29f1b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 1471725)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fit_test2_bis = pipe2_bis.transform(X_test['Avis'])\n",
        "fit_test2_bis.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjHC1AYmlPQ-"
      },
      "source": [
        "## 4.3 - Application de la méthode Logit aux bi-grammes (crées sur les données de base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_stIzjslPQ-"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf2 = LogisticRegression(solver='lbfgs')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZh-VcNDlPQ-"
      },
      "source": [
        "**Et on 'fit'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5ucn0TllPQ-",
        "outputId": "876ab00c-24f5-4de8-93ed-82490cb83546"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf2.fit(fit_train2, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOkL78TQlPQ-"
      },
      "source": [
        "Et on calcule le score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irJWWzL9lPQ-",
        "outputId": "3df731f4-191f-415e-c2c3-7a968c7d3ac1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8987"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf2.score(fit_test2, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdXZ32tolPQ-"
      },
      "source": [
        "<font color=\"red\"> Ce qui améliore   les résultats de façon significative.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEmhkFyelPQ-"
      },
      "source": [
        "### 4.3.1- Application de la méthode Logit aux bi-grammes (crées sur les données lemmatisées)\n",
        "**La même chose avec les données lemmatisées :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfzQiKK6lPQ-"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf2_bis = LogisticRegression(solver='lbfgs')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-62nXAhUlPQ-"
      },
      "source": [
        "**Et on 'fit'**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkyaqQDdlPQ_",
        "outputId": "52ffcb26-07fd-4115-ba1b-9840ba554b82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression()\n"
          ]
        }
      ],
      "source": [
        "print(clf2_bis.fit(fit_train2_bis, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbcXu8YTlPQ_"
      },
      "source": [
        "ET on calcule le score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub7QP2-UlPQ_",
        "outputId": "fbc3b26d-8011-4a92-f73f-c0df9093c9b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8766\n"
          ]
        }
      ],
      "source": [
        "print(clf2_bis.score(fit_test2_bis, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoHQ4M0TlPQ_"
      },
      "source": [
        "**Pas TOP !!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3430FOcplPQ_"
      },
      "source": [
        "**pas tellement Encourageant ?? !**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGCodJ8ElPQ_"
      },
      "source": [
        "## 4.4- Cross-validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asbEZILTlPQ_"
      },
      "source": [
        "\n",
        "On ne l'applique qu'aux données de bse (non lémmatisées)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ov0I6wNYlPQ_",
        "outputId": "19ed5a0c-df01-42a3-a56d-1a52bb4cb0dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegressionCV(cv=5)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "clf2_ = LogisticRegressionCV(cv=5)\n",
        "clf2_.fit(fit_train2, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SlCzrWLlPQ_",
        "outputId": "ce967a53-2b47-49c4-f7db-a49c4ac61a06"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9162"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf2_.score(fit_test2, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhVLQImGlPQ_"
      },
      "source": [
        "<font color=\"red\">  Pas mal !</font>\n",
        "    \n",
        "__Rappel__ : Logit était un des meilleurs avec MNB.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ROD8y38lPRA"
      },
      "source": [
        "\n",
        "## 4.5-  Application de  MNB (sur les données de base)\n",
        "**D'abord MNB sur les données de de base !**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxt6fOxslPRA",
        "outputId": "e74dcc33-467c-4625-845b-3e6be6cbbfbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8862"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(fit_train2, y_train)\n",
        "mnb.score(fit_test2, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrz4on25lPRA"
      },
      "source": [
        "## 4.6- MNB avec Validation Croisée (XV)\n",
        "\n",
        "Pour rendre les résultats plus fiables, on doit faire XV en MNB.\n",
        "\n",
        "Pour cela, on applique \"k_folds MNB\" en partant du début du DB, transformé en 2-grams ...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z3oLUHXlPRA"
      },
      "source": [
        "**K-folds** pratiqué sur l'ensemble des données\n",
        "\n",
        "On a besoin de créer les folds soi-même pour MNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxD3CKmTlPRA",
        "outputId": "a17c0fc3-da5d-493a-cbd3-d409399bd404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 2468006)\n",
            "  (0, 2460156)\t0.06312244732574757\n",
            "  (0, 2459694)\t0.015657331418500838\n",
            "  (0, 2456364)\t0.030354118152065134\n",
            "  (0, 2456295)\t0.023043462602789406\n",
            "  (0, 2455233)\t0.019293482408135502\n",
            "  (0, 2454885)\t0.027260068249352832\n",
            "  (0, 2440019)\t0.056015415411665805\n",
            "  (0, 2439952)\t0.024089437504291555\n",
            "  (0, 2439590)\t0.03349160839673966\n",
            "  (0, 2438485)\t0.012073491396120916\n",
            "  (0, 2430385)\t0.04926680283261607\n",
            "  (0, 2430236)\t0.05035067559958957\n",
            "  (0, 2430151)\t0.04977922365885089\n",
            "  (0, 2419675)\t0.03343077043155168\n",
            "  (0, 2419439)\t0.033415662337708554\n",
            "  (0, 2416595)\t0.0477594459534412\n",
            "  (0, 2414727)\t0.032511056274860825\n",
            "  (0, 2413603)\t0.026210337211178682\n",
            "  (0, 2408477)\t0.03866046329787878\n",
            "  (0, 2390058)\t0.09345024020604871\n",
            "  (0, 2388183)\t0.020428108714313054\n",
            "  (0, 2382620)\t0.05345126498583529\n",
            "  (0, 2380277)\t0.012537581509352143\n",
            "  (0, 2378871)\t0.05250355281606642\n",
            "  (0, 2377459)\t0.0394845976562129\n",
            "  :\t:\n",
            "  (0, 164111)\t0.036666356824415394\n",
            "  (0, 160383)\t0.018079746346905883\n",
            "  (0, 155094)\t0.042266477874172564\n",
            "  (0, 155002)\t0.029283262324268405\n",
            "  (0, 132162)\t0.056015415411665805\n",
            "  (0, 128457)\t0.05525788035293552\n",
            "  (0, 123132)\t0.02683684553256288\n",
            "  (0, 117808)\t0.03615492190852107\n",
            "  (0, 117504)\t0.03116594152828433\n",
            "  (0, 116190)\t0.046418392113183556\n",
            "  (0, 108192)\t0.03521522806402146\n",
            "  (0, 104688)\t0.04795177236773914\n",
            "  (0, 102981)\t0.00976449473633706\n",
            "  (0, 80477)\t0.016402539098076825\n",
            "  (0, 76882)\t0.009404184911551886\n",
            "  (0, 68359)\t0.06312244732574757\n",
            "  (0, 68358)\t0.05525788035293552\n",
            "  (0, 66216)\t0.06312244732574757\n",
            "  (0, 66196)\t0.03862896644929342\n",
            "  (0, 62285)\t0.030773862507675064\n",
            "  (0, 60131)\t0.01407828761852342\n",
            "  (0, 37197)\t0.04463898584325437\n",
            "  (0, 37192)\t0.04401948888133312\n",
            "  (0, 28422)\t0.060822208943994996\n",
            "  (0, 25366)\t0.010660086450140044\n",
            "KFold(n_splits=20, random_state=0, shuffle=True)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_movies[[\"Avis\"]], df_movies['Polarite'])\n",
        "\n",
        "df_all=df_movies[['Avis','Polarite']]\n",
        "y_all=df_movies['Polarite']\n",
        "x_all=df_movies[\"Avis\"]\n",
        "\n",
        "pipe2_ = make_pipeline(CountVectorizer(ngram_range=(1, 2)),TfidfTransformer())\n",
        "pipe2_.fit(x_all)\n",
        "\n",
        "feat_train2_ = pipe2_.transform(x_all)\n",
        "print(feat_train2_.shape)\n",
        "print(feat_train2_[0])\n",
        "y_train2_=y_all\n",
        "\n",
        "Nb_folds=20\n",
        "k_fold = KFold(n_splits=Nb_folds , shuffle=True, random_state=0)\n",
        "k_fold.get_n_splits(df_all)\n",
        "print(k_fold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9_xzM9AlPRA"
      },
      "source": [
        "### 4.6.1- Création du modèle -->  score poly K-Folds  MNB\n",
        "**Avec 20-folds, on atteint 0.886 (mieux que  Logit)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTHiOtoUlPRA",
        "outputId": "7814bb26-2324-425f-bdfa-398ae7243aba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRACE : Scores de chaque itération :  [0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868, 0.8868]\n",
            "La moyenn :  0.8868000000000003\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "mnb2 = MultinomialNB()\n",
        "Scores=[]\n",
        "\n",
        "for i in range(Nb_folds) :\n",
        "    #print(i, end=' ')\n",
        "    res=next(k_fold.split(feat_train2_), None)\n",
        "    x_train_ = feat_train2_[res[0]]\n",
        "    x_test_ = feat_train2_[res[1]]\n",
        "    y_train_  = y_all.iloc[res[0]]\n",
        "    y_test_ = y_all.iloc[res[1]]\n",
        "    model__ = mnb2.fit(x_train_, y_train_)\n",
        "    predictions_ = mnb2.predict(x_test_)\n",
        "    Scores.append(model__.score(x_test_, y_test_))\n",
        "print('TRACE : Scores de chaque itération : ', Scores)\n",
        "print('La moyenn : ', np.mean(Scores))\n",
        "#mnb.fit(feat_train2, y_train)\n",
        "# mnb.score(feat_test2, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWL16EGflPRA"
      },
      "source": [
        "<font color=\"red\"> __On s'est donc fait une idée du meilleur résultat jsq'à présent !__</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-aFrDvzlPRA"
      },
      "source": [
        "## 4.7- A la recherche d'améliorer les résultats....\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIr7xnDtlPRA"
      },
      "source": [
        "### 4.7.1  Réduction de dimension avec une SVD / ACP\n",
        "On choisit la méthode TruncatedSVD plutôt que l'ACP dont l'implantation ne supporte pas les features creux(sparses)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OytKRKH3lPRA"
      },
      "source": [
        "### 4.7.2- Application du SVD à __Tf__ d'abord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DCZQ2QDlPRA",
        "outputId": "8437057c-157c-4e38-f55f-3200b4225a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Avis'], dtype='object')\n",
            "20379    If we consider three films with a similar subj...\n",
            "19177    Once in a while you get amazed over how BAD a ...\n",
            "6436     I've read countless of posts about this game b...\n",
            "4176     If you have read the books then forget the cha...\n",
            "13689    I had no expectations other than to be enterta...\n",
            "                               ...                        \n",
            "1480     Well I've enjoy this movie, even though someti...\n",
            "24688    I liked this movie a lot. The animation was we...\n",
            "41244    To say that Thunderbirds is a horrid, forced, ...\n",
            "24177    Computer savvy John Light (as John Elias) goes...\n",
            "11574    \"A Slight Case of Murder\" is an excellent TV m...\n",
            "Name: Avis, Length: 37500, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(X_train.keys())\n",
        "print(X_train['Avis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIF9Llb7lPRA",
        "outputId": "66a03307-6d99-45d7-e7cd-7d61e17d8ed1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(40000, 100)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "pipe_svd = make_pipeline(CountVectorizer(), TruncatedSVD(n_components=100))\n",
        "pipe_svd.fit(X_train['Avis'])\n",
        "fit_train_svd = pipe_svd.transform(X_train['Avis'])\n",
        "fit_train_svd.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-54F1UhPlPRB"
      },
      "source": [
        "### 4.7.3-  Application du  RF à ce résultat SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "0Bn0UxwhlPRB",
        "outputId": "5973133c-4fb1-4e72-81a6-0cbff0c0371e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=50)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier(n_estimators=50)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf_svd = RandomForestClassifier(n_estimators=50)\n",
        "clf_svd.fit(fit_train_svd, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYRfLhCHlPRB",
        "outputId": "e8b757b5-b156-4d3c-85c1-854012786e57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7139"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fit_test_svd = pipe_svd.transform(X_test['Avis'])\n",
        "clf_svd.score(fit_test_svd, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voVC0ahwlPRB",
        "outputId": "7adaf668-09ab-41e7-cd82-b766add096a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7944"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_svd = LogisticRegression(solver='lbfgs')\n",
        "lr_svd.fit(fit_train_svd, y_train)\n",
        "lr_svd.score(fit_test_svd, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVH4kDz1lPRB"
      },
      "source": [
        " ## 4.8-Comparaison des méthodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxI6NNBSlPRB"
      },
      "source": [
        "**Repartons de TF-IDF puis SVD puis Logit pour comparer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaxxaZx1lPRB",
        "outputId": "bbf71e64-be68-4cbe-e914-7cbe503b10a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8769"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe_svd_tfidf = make_pipeline(CountVectorizer(),\n",
        "                     TfidfTransformer(),\n",
        "                     TruncatedSVD(n_components=300))\n",
        "pipe_svd_tfidf.fit(X_train['Avis'])\n",
        "fit_train_svd_tfidf = pipe_svd_tfidf.transform(X_train['Avis'])\n",
        "\n",
        "clf_svd_tfidf = LogisticRegression(solver='lbfgs')\n",
        "clf_svd_tfidf.fit(fit_train_svd_tfidf, y_train)\n",
        "\n",
        "fit_test_svd_tfidf = pipe_svd_tfidf.transform(X_test['Avis'])\n",
        "clf_svd_tfidf.score(fit_test_svd_tfidf, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr3uzBKBlPRB"
      },
      "source": [
        "<font color=\"red\"> C'est (un peu) mieux mais cela reste moins bien que le tf-idf sans réduction de dimensions. </font>\n",
        "\n",
        "**Cela veut dire qu'il faut garder davantage de dimensions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLpqk10ClPRB"
      },
      "source": [
        "# 5- word2vec\n",
        "word2vec est une projection (comme ACP/SVD) en ce sens qu'il réduit les dimensions.\n",
        "\n",
        "Une relecture d'ACP et Auto Encoders pour comprendre le lien entre ACP, ACP non linéaire, réseaux de neurones et compression nous fera du bien !\n",
        "\n",
        "__word2vec__ est plus d'une ACP non linéaire car il prend en compte le contexte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK4dlDpVlPRB",
        "outputId": "8dddcc4e-3065-4db1-fc7e-9422dd33a216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting Levenshtein\n",
            "  Downloading Levenshtein-0.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading Levenshtein-0.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.25.0 rapidfuzz-3.7.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# OK  FAIT : !/Users/alexandresaidi/opt/anaconda3/bin/pip3 install gensim\n",
        "# OK  FAIT : !/Users/alexandresaidi/opt/anaconda3/bin/pip3 install p Levenshtein\n",
        "if True :\n",
        "    !pip install gensim\n",
        "    !pip install Levenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upFT7ov4lPRC"
      },
      "source": [
        "La version de gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61oQDAI5lPRC",
        "outputId": "e69f78fc-a339-45b6-c3d3-b0fbe59dae22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyopenssl in /usr/local/lib/python3.10/dist-packages (24.1.0)\n",
            "Requirement already satisfied: cryptography<43,>=41.0.5 in /usr/local/lib/python3.10/dist-packages (from pyopenssl) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<43,>=41.0.5->pyopenssl) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<43,>=41.0.5->pyopenssl) (2.21)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: Levenshtein in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein) (3.7.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pyopenssl\n",
        "!pip install Levenshtein\n",
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1bf1KiulPRC",
        "outputId": "55d45085-8a64-475a-f3d9-c97f11858b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: Levenshtein in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein) (3.7.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeStxA74lPRC",
        "outputId": "d5d733bd-2507-4fe9-fd40-d255c95ee918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.3.2\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "print(gensim.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qnI1jKPlPRC"
      },
      "source": [
        "### Tokenisation du texte des Avis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1wvKLV8lPRC",
        "outputId": "b18d4b9a-f704-44e3-b6ed-c5a9e6a8428c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['this',\n",
              "  'film',\n",
              "  'is',\n",
              "  'one',\n",
              "  'to',\n",
              "  'spend',\n",
              "  'the',\n",
              "  'short',\n",
              "  'while',\n",
              "  'entertainment',\n",
              "  'but',\n",
              "  'who',\n",
              "  'then',\n",
              "  'could',\n",
              "  'be',\n",
              "  'made',\n",
              "  'in',\n",
              "  'nobody',\n",
              "  'country',\n",
              "  'it',\n",
              "  'does',\n",
              "  'not',\n",
              "  'have',\n",
              "  'anything',\n",
              "  'identifies',\n",
              "  'the',\n",
              "  'colombian',\n",
              "  'who',\n",
              "  'looks',\n",
              "  'for',\n",
              "  'the',\n",
              "  'topics',\n",
              "  'of',\n",
              "  'the',\n",
              "  'supposed',\n",
              "  'colombian',\n",
              "  'or',\n",
              "  'socially',\n",
              "  'and',\n",
              "  'politically',\n",
              "  'correct',\n",
              "  'cinema',\n",
              "  'and',\n",
              "  'what',\n",
              "  'it',\n",
              "  'is',\n",
              "  'denominated',\n",
              "  'it',\n",
              "  'jeopardize',\n",
              "  'it',\n",
              "  'is',\n",
              "  'better',\n",
              "  'than',\n",
              "  'it',\n",
              "  'is',\n",
              "  'not',\n",
              "  'going',\n",
              "  'to',\n",
              "  'see',\n",
              "  'this',\n",
              "  'film',\n",
              "  'some',\n",
              "  'to',\n",
              "  'the',\n",
              "  'drug',\n",
              "  'traffic',\n",
              "  'or',\n",
              "  'the',\n",
              "  'fight',\n",
              "  'is',\n",
              "  'no',\n",
              "  'reference',\n",
              "  'either',\n",
              "  'farmer',\n",
              "  'and',\n",
              "  'worker',\n",
              "  'or',\n",
              "  'wing',\n",
              "  'guerrilla',\n",
              "  'and',\n",
              "  'to',\n",
              "  'the',\n",
              "  'kidnapping',\n",
              "  'political',\n",
              "  'br',\n",
              "  'br',\n",
              "  'the',\n",
              "  'corrupt',\n",
              "  'police',\n",
              "  'is',\n",
              "  'a',\n",
              "  'personage',\n",
              "  'who',\n",
              "  'it',\n",
              "  'could',\n",
              "  'be',\n",
              "  'mexican',\n",
              "  'russian',\n",
              "  'italian',\n",
              "  'or',\n",
              "  'chinese',\n",
              "  'and',\n",
              "  'also',\n",
              "  'the',\n",
              "  'personage',\n",
              "  'protagonist',\n",
              "  'an',\n",
              "  'argentine',\n",
              "  'photographer',\n",
              "  'it',\n",
              "  'could',\n",
              "  'be',\n",
              "  'venezuelan',\n",
              "  'peruvian',\n",
              "  'or',\n",
              "  'philippine',\n",
              "  'and',\n",
              "  'the',\n",
              "  'frequent',\n",
              "  'boludo',\n",
              "  'in',\n",
              "  'its',\n",
              "  'parliaments',\n",
              "  'it',\n",
              "  'could',\n",
              "  'be',\n",
              "  'replaced',\n",
              "  'by',\n",
              "  'another',\n",
              "  'word',\n",
              "  'of',\n",
              "  'slang',\n",
              "  'of',\n",
              "  'any',\n",
              "  'other',\n",
              "  'country',\n",
              "  'without',\n",
              "  'the',\n",
              "  'personage',\n",
              "  'or',\n",
              "  'history',\n",
              "  'changes',\n",
              "  'the',\n",
              "  'important',\n",
              "  'thing',\n",
              "  'is',\n",
              "  'that',\n",
              "  'it',\n",
              "  'is',\n",
              "  'a',\n",
              "  'very',\n",
              "  'decently',\n",
              "  'counted',\n",
              "  'history',\n",
              "  'with',\n",
              "  'images',\n",
              "  'and',\n",
              "  'right',\n",
              "  'performances',\n",
              "  'its',\n",
              "  'quality',\n",
              "  'almost',\n",
              "  'is',\n",
              "  'of',\n",
              "  'tv',\n",
              "  'movie',\n",
              "  'but',\n",
              "  'of',\n",
              "  'the',\n",
              "  'good',\n",
              "  'ones',\n",
              "  'that',\n",
              "  'if',\n",
              "  'one',\n",
              "  'film',\n",
              "  'like',\n",
              "  'this',\n",
              "  'if',\n",
              "  'it',\n",
              "  'deserves',\n",
              "  'or',\n",
              "  'does',\n",
              "  'not',\n",
              "  'deserve',\n",
              "  'the',\n",
              "  'support',\n",
              "  'with',\n",
              "  'publics',\n",
              "  'bottoms',\n",
              "  'a',\n",
              "  'little',\n",
              "  'is',\n",
              "  'a',\n",
              "  'classic',\n",
              "  'discussion',\n",
              "  'mamerta'],\n",
              " ['if',\n",
              "  'a',\n",
              "  'more',\n",
              "  'masterful',\n",
              "  'adaptation',\n",
              "  'than',\n",
              "  'this',\n",
              "  'one',\n",
              "  'even',\n",
              "  'existed',\n",
              "  'you',\n",
              "  'need',\n",
              "  'not',\n",
              "  'look',\n",
              "  'for',\n",
              "  'it',\n",
              "  'you',\n",
              "  'will',\n",
              "  'find',\n",
              "  'all',\n",
              "  'and',\n",
              "  'more',\n",
              "  'in',\n",
              "  'this',\n",
              "  'near',\n",
              "  'perfect',\n",
              "  'presentation',\n",
              "  'of',\n",
              "  'charlotte',\n",
              "  'bronte',\n",
              "  's',\n",
              "  'masterpiece',\n",
              "  'br',\n",
              "  'br',\n",
              "  'rarely',\n",
              "  'have',\n",
              "  'i',\n",
              "  'seen',\n",
              "  'a',\n",
              "  'film',\n",
              "  'that',\n",
              "  'would',\n",
              "  'urge',\n",
              "  'me',\n",
              "  'to',\n",
              "  'read',\n",
              "  'the',\n",
              "  'novel',\n",
              "  'on',\n",
              "  'which',\n",
              "  'it',\n",
              "  'was',\n",
              "  'based',\n",
              "  'but',\n",
              "  'i',\n",
              "  'admit',\n",
              "  'to',\n",
              "  'that',\n",
              "  'here',\n",
              "  'although',\n",
              "  'i',\n",
              "  'have',\n",
              "  'not',\n",
              "  'read',\n",
              "  'jane',\n",
              "  'eyre',\n",
              "  'i',\n",
              "  'am',\n",
              "  'convinced',\n",
              "  'that',\n",
              "  'i',\n",
              "  'have',\n",
              "  'missed',\n",
              "  'very',\n",
              "  'little',\n",
              "  'in',\n",
              "  'the',\n",
              "  'way',\n",
              "  'of',\n",
              "  'dialogue',\n",
              "  'and',\n",
              "  'plot',\n",
              "  'or',\n",
              "  'of',\n",
              "  'intensity',\n",
              "  'and',\n",
              "  'emotion',\n",
              "  'i',\n",
              "  'only',\n",
              "  'wish',\n",
              "  'to',\n",
              "  'explore',\n",
              "  'the',\n",
              "  'novel',\n",
              "  'due',\n",
              "  'to',\n",
              "  'the',\n",
              "  'immense',\n",
              "  'curiosity',\n",
              "  'and',\n",
              "  'emotion',\n",
              "  'that',\n",
              "  'this',\n",
              "  'masterpiece',\n",
              "  'has',\n",
              "  'stirred',\n",
              "  'within',\n",
              "  'me',\n",
              "  'br',\n",
              "  'br',\n",
              "  'i',\n",
              "  'need',\n",
              "  'not',\n",
              "  'divulge',\n",
              "  'anything',\n",
              "  'in',\n",
              "  'the',\n",
              "  'way',\n",
              "  'of',\n",
              "  'plot',\n",
              "  'here',\n",
              "  'let',\n",
              "  'me',\n",
              "  'just',\n",
              "  'say',\n",
              "  'this',\n",
              "  'if',\n",
              "  'you',\n",
              "  'are',\n",
              "  'perhaps',\n",
              "  'unsure',\n",
              "  'as',\n",
              "  'to',\n",
              "  'whether',\n",
              "  'you',\n",
              "  'should',\n",
              "  'watch',\n",
              "  'or',\n",
              "  'read',\n",
              "  'the',\n",
              "  'beautiful',\n",
              "  'story',\n",
              "  'that',\n",
              "  'is',\n",
              "  'jane',\n",
              "  'eyre',\n",
              "  'i',\n",
              "  'implore',\n",
              "  'you',\n",
              "  'to',\n",
              "  'doubt',\n",
              "  'no',\n",
              "  'more',\n",
              "  'every',\n",
              "  'atom',\n",
              "  'of',\n",
              "  'might',\n",
              "  'and',\n",
              "  'magic',\n",
              "  'that',\n",
              "  'has',\n",
              "  'reared',\n",
              "  'jane',\n",
              "  'eyre',\n",
              "  'as',\n",
              "  'a',\n",
              "  'popular',\n",
              "  'classic',\n",
              "  'of',\n",
              "  'english',\n",
              "  'literature',\n",
              "  'has',\n",
              "  'successfully',\n",
              "  'been',\n",
              "  'captured',\n",
              "  'in',\n",
              "  'this',\n",
              "  'film',\n",
              "  'br',\n",
              "  'br',\n",
              "  'what',\n",
              "  'bronte',\n",
              "  'did',\n",
              "  'not',\n",
              "  'bring',\n",
              "  'herself',\n",
              "  'clarke',\n",
              "  'and',\n",
              "  'dalton',\n",
              "  'managed',\n",
              "  'to',\n",
              "  'translate',\n",
              "  'in',\n",
              "  'the',\n",
              "  'limelight',\n",
              "  'with',\n",
              "  'stupendous',\n",
              "  'intensity',\n",
              "  'the',\n",
              "  'movie',\n",
              "  's',\n",
              "  'success',\n",
              "  'is',\n",
              "  'no',\n",
              "  'doubt',\n",
              "  'due',\n",
              "  'in',\n",
              "  'no',\n",
              "  'small',\n",
              "  'part',\n",
              "  'to',\n",
              "  'their',\n",
              "  'acting',\n",
              "  'prowess',\n",
              "  'br',\n",
              "  'br',\n",
              "  'love',\n",
              "  'jane',\n",
              "  'eyre',\n",
              "  'or',\n",
              "  'hate',\n",
              "  'her',\n",
              "  'but',\n",
              "  'appreciate',\n",
              "  'the',\n",
              "  'richness',\n",
              "  'the',\n",
              "  'vitality',\n",
              "  'the',\n",
              "  'truth',\n",
              "  'of',\n",
              "  'the',\n",
              "  'story',\n",
              "  'love',\n",
              "  'the',\n",
              "  'characters',\n",
              "  'love',\n",
              "  'the',\n",
              "  'actors',\n",
              "  'all',\n",
              "  'just',\n",
              "  'as',\n",
              "  'you',\n",
              "  'would',\n",
              "  'love',\n",
              "  'what',\n",
              "  'is',\n",
              "  'great',\n",
              "  'in',\n",
              "  'cinema']]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.utils import tokenize\n",
        "Avis = [list(tokenize(s, deacc=True, lower=True)) for s in X_train['Avis']]\n",
        "\n",
        "#  Des exemples :\n",
        "Avis[0:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhrNMle-lPRC"
      },
      "source": [
        "### Application de Wor2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlkMdWAclPRC"
      },
      "source": [
        "**N.B. : Les paramètres d'apprentissage du modèle Word2Vec ne sont pas toujours décrit de façon explicite.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2KFvCG1lPRC",
        "outputId": "c564c3a4-7d45-4a81-b180-e5ad0ef31ab8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "40000"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.models import word2vec\n",
        "#model = word2vec.Word2Vec(Avis, size=300, window=20, min_count=2, workers=1, iter=100)\n",
        "model = word2vec.Word2Vec(Avis,  window=20, min_count=2, workers=1)\n",
        "\n",
        "# Le nombre de tokens\n",
        "model.corpus_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QzQSHW6lPRC"
      },
      "source": [
        "### 5.1 HALT : Save the model !!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmJApRj7lPRD"
      },
      "outputs": [],
      "source": [
        "model.save('trained_word2vec.bin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSs6qFvDlPRD"
      },
      "source": [
        "### 5.2 Vecteur associé aux mots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtlOuKPolPRD"
      },
      "source": [
        "**Vecteur associé au mot after**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jOdHTQblPRD",
        "outputId": "14cda98b-8ef3-4602-bc5d-365617513753"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 8.60458553e-01, -3.86633128e-02,  3.84762979e+00,  1.46056676e+00,\n",
              "        2.01869059e+00, -1.90343726e+00, -1.08113840e-01, -2.24463582e+00,\n",
              "        6.59465432e-01,  8.13851595e-01, -8.81016493e-01, -4.83334482e-01,\n",
              "        1.16241741e+00,  1.01285875e-02,  1.59076750e+00, -5.03030479e-01,\n",
              "       -1.44561005e+00,  1.66797888e+00,  1.76803064e+00, -2.93979931e+00,\n",
              "        6.57167956e-02, -1.88061833e+00,  7.77797639e-01,  3.27448392e+00,\n",
              "       -6.11565709e-01,  1.59340000e+00,  5.99002957e-01,  2.10050797e+00,\n",
              "        3.29899669e+00, -9.21176791e-01,  1.18978846e+00,  3.05247903e+00,\n",
              "       -2.22686708e-01,  4.70438337e+00,  1.57398307e+00,  3.66175461e+00,\n",
              "       -5.04515171e-01,  4.41443641e-03,  4.10552621e-01, -3.65868896e-01,\n",
              "        1.67931104e+00, -2.09015250e+00, -2.52769446e+00,  2.98337102e+00,\n",
              "       -3.97710228e+00, -4.19456363e-01,  7.18656421e-01,  1.87696373e+00,\n",
              "        4.14030504e+00, -2.75122494e-01, -4.08376306e-01,  9.98255610e-01,\n",
              "        5.40415144e+00, -1.46154225e+00, -1.38566554e+00, -1.84695244e-01,\n",
              "       -8.33683908e-02, -3.33854222e+00,  3.63600564e+00,  7.14123964e-01,\n",
              "        3.17542601e+00,  1.80067408e+00, -2.57654262e+00, -4.90973860e-01,\n",
              "        9.89020243e-02,  6.91665471e-01, -4.76742172e+00,  8.70069122e+00,\n",
              "        1.93416798e+00,  1.54127288e+00, -2.29837275e+00,  2.15891433e+00,\n",
              "       -3.17234921e+00,  7.06668496e-01,  4.94887203e-01, -4.52607441e+00,\n",
              "        2.13912010e+00, -4.36554432e+00,  1.66099012e+00,  9.40265581e-02,\n",
              "       -2.65202260e+00, -3.61763430e+00, -1.21559215e+00, -1.12276220e+00,\n",
              "        2.82054394e-01, -3.76735520e+00,  4.41325665e-01, -3.28527570e-01,\n",
              "        3.20780730e+00, -2.16097403e+00, -2.06374907e+00,  1.55737832e-01,\n",
              "        4.91509342e+00, -3.94748330e+00,  6.44886732e+00, -4.19433260e+00,\n",
              "        2.84414148e+00, -5.30961847e+00, -1.02021597e-01, -7.12637854e+00],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = model.wv # .vocab (selon la version de gensim)\n",
        "# w2v.vocab[\"word\"]\n",
        "#list(vocab)#[:5]\n",
        "vocab['after']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL8FZqlvlPRD"
      },
      "source": [
        "**Les dix premières coordonnées du vecteur associé au mot after.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sux7QFfIlPRD",
        "outputId": "ee9d70c7-f9ed-4ecc-94f5-4ca9b18df15a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La taille du vecteur :  (100,)\n",
            "Les 10 première valeurs du vecteur du mot 'after' : \n",
            " [ 0.86045855 -0.03866331  3.8476298   1.4605668   2.0186906  -1.9034373\n",
            " -0.10811384 -2.2446358   0.65946543  0.8138516 ]\n"
          ]
        }
      ],
      "source": [
        "print(\"La taille du vecteur : \", model.wv['after'].shape)\n",
        "print(\"Les 10 première valeurs du vecteur du mot 'after' : \\n\", model.wv['after'][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biS4YenglPRD"
      },
      "source": [
        "**Et si le mot est inconnu**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwPlQnYGlPRD",
        "outputId": "4e15c764-c342-4eca-b31c-b6abe9e473f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.25554246  0.28270817  0.06103024 -0.01009811 -0.16501574 -0.55429\n",
            "  0.33240512  0.40163437 -0.12699422 -0.12471386 -0.01377156 -0.45657822\n",
            "  0.16897778  0.2155556  -0.14254132 -0.08548411  0.17928094 -0.26218417\n",
            " -0.09752951 -0.6386708   0.02870275  0.04298285  0.1638963  -0.5009368\n",
            " -0.16254446 -0.04837751 -0.31141117  0.15062475 -0.35511976 -0.02830146\n",
            "  0.2773115  -0.21794264 -0.00620664 -0.16327257 -0.15407996 -0.0778842\n",
            "  0.3173157  -0.38253856 -0.2138643  -0.21804313 -0.03400102 -0.27014464\n",
            " -0.11004537 -0.19901787  0.04629171 -0.05966508  0.0410932  -0.28364366\n",
            "  0.11089752  0.08270638  0.1785233  -0.19317323 -0.00416213 -0.00655077\n",
            " -0.1005123   0.28516385  0.19572316 -0.05758142  0.02068626  0.22480138\n",
            "  0.19605295  0.10372011  0.09982283  0.07387134 -0.43336898  0.09173945\n",
            " -0.01819439  0.3024729  -0.53619814  0.2828685   0.2914204   0.0586029\n",
            "  0.14124158  0.20854555  0.38892016  0.07074356  0.20755047 -0.19859552\n",
            " -0.16349082 -0.05482773 -0.2865963   0.01042737 -0.40470144  0.3812356\n",
            " -0.36742142 -0.15003844  0.10794177 -0.14552118  0.17916815 -0.0992268\n",
            " -0.08063812  0.09836686  0.32522807  0.11607247  0.43721914 -0.0449409\n",
            "  0.23289022  0.14593601  0.00106782 -0.1492429 ]\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(model.wv['toto'])\n",
        "except KeyError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORu3ykTOlPRD"
      },
      "source": [
        "### 5.3 Similarité entre les mots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW8RIkaTlPRD"
      },
      "source": [
        "**3 mots les plus proches de \"movie\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVmT90CNlPRD",
        "outputId": "2e5347c5-0f93-4942-faad-e90c6b63036a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('film', 0.7510113716125488),\n",
              " ('flick', 0.6092798113822937),\n",
              " ('it', 0.5957990884780884)]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar(positive=['movie'], topn = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUOAzWGZlPRD"
      },
      "source": [
        "**3 mots les plus proches de \"after\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXa0_r_PlPRD",
        "outputId": "b86ffc97-49d6-4864-df74-5710c70956fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('before', 0.7068341374397278),\n",
              " ('later', 0.6106483340263367),\n",
              " ('afterward', 0.6054144501686096)]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar(positive=['after'], topn = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2GX2jPjlPRD"
      },
      "source": [
        "**3 mots les plus proches de \"young\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y37oJ3TlPRD",
        "outputId": "e16169ef-dcee-47aa-c8c2-ed7337708a24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('rich', 0.6653879880905151),\n",
              " ('wealthy', 0.6571557521820068),\n",
              " ('troubled', 0.6470325589179993)]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar(positive=['young'], topn = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHSQ-oA3lPRE"
      },
      "source": [
        "# 6- Vers Doc2Vect (manuel & expérimental)\n",
        "\n",
        "**Pour chaque phrase, on fait la somme des vecteurs associés aux mots qui la composent ou pas si le mot n'est pas dans le vocabulaire.**\n",
        "\n",
        "* Proche de __doc2vect__\n",
        "* Il y a probablement des fonctions déjà prêtes à l'emploi mais la documentation de gensim n'était pas assez explicite\n",
        "\n",
        "Réfs :\n",
        "\n",
        "Efficient Estimation of Word Representations in Vector Space     \n",
        "\n",
        "Distributed Representations of Words and Phrases and their Compositionality)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq-RkHFilPRE"
      },
      "source": [
        "**NB :  :  il existe des fonctions équivalentes !!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFaO9P1_lPRE",
        "outputId": "17fd7d63-3b02-45f3-c117-6066a262f021"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(40000, 100)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_vect(word, model):\n",
        "    try:\n",
        "        return model.wv[word]\n",
        "    except KeyError:\n",
        "        return np.zeros((model.vector_size,))\n",
        "# Somme\n",
        "def sum_vectors(phrase, model):\n",
        "    return sum(get_vect(w, model) for w in phrase)\n",
        "\n",
        "def word2vec_features(X, model):\n",
        "    feats = np.vstack([sum_vectors(p, model) for p in X])\n",
        "    return feats\n",
        "\n",
        "wv_train_feat = word2vec_features(X_train[\"Avis\"], model)\n",
        "wv_train_feat.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uGkdADTlPRE"
      },
      "source": [
        "### 6.1 Logit sur word2vect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "nritkcajlPRE",
        "outputId": "d6f778d1-62ab-4ec9-acc6-fab158c4c1fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clfwv = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "clfwv.fit(wv_train_feat, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KXqM4yglPRE"
      },
      "outputs": [],
      "source": [
        "wv_test_feat = word2vec_features(X_test[\"Avis\"], model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bphFtJXBlPRE",
        "outputId": "69523d96-5f1a-4c30-f007-dc92d598a9fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6219"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clfwv.score(wv_test_feat, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Fwrao0lPRE"
      },
      "source": [
        "**NB**\n",
        "\n",
        "La performance est bien moindre et encore  moindre que la performance obtenue avec l'ACP.\n",
        "Il faudrait 'tuner' les hyperparamètres de l'apprentissage ou réutiliser un model appris sur un corpus similaire aux données initiales mais bien plus grand.\n",
        "\n",
        "On peut constater que la fonction de similarités ne retourne pas des résultat très intéressants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jBLW7fUlPRE",
        "outputId": "657b66e8-5a84-4f07-c2e2-0bb133c81d21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['_',\n",
              " '__',\n",
              " '___',\n",
              " '____',\n",
              " '_____',\n",
              " '______',\n",
              " '________',\n",
              " '_____________________________________',\n",
              " '__________________________________________________________________',\n",
              " '_a']"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.index_to_key[:10]\n",
        "# words = list(sorted(model.wv))\n",
        "words=sorted(model.wv.index_to_key)\n",
        "words[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pOlxwVRlPRE"
      },
      "source": [
        "### Similarité deux à deux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "x_i11HCAlPRE",
        "outputId": "ddb65130-927a-412e-d565-0e4376ae276a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"w1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"_____\",\n          \"before\",\n          \"______\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"____\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.10075435787439346,\n          -0.0011148122139275074,\n          0.1907828152179718\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"_____\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.0,\n          0.014441467821598053,\n          0.29307878017425537\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"______\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.29307878017425537,\n          -0.18834412097930908,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"after\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.02632695436477661,\n          0.7068341374397278,\n          -0.059771765023469925\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"before\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.014441467821598053,\n          1.0,\n          -0.18834412097930908\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1bc5825b-7588-453c-9194-dee0f0b84230\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>w2</th>\n",
              "      <th>____</th>\n",
              "      <th>_____</th>\n",
              "      <th>______</th>\n",
              "      <th>after</th>\n",
              "      <th>before</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>w1</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>____</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.100754</td>\n",
              "      <td>0.190783</td>\n",
              "      <td>0.033343</td>\n",
              "      <td>-0.001115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>_____</th>\n",
              "      <td>0.100754</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.293079</td>\n",
              "      <td>0.026327</td>\n",
              "      <td>0.014441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>______</th>\n",
              "      <td>0.190783</td>\n",
              "      <td>0.293079</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.059772</td>\n",
              "      <td>-0.188344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>after</th>\n",
              "      <td>0.033343</td>\n",
              "      <td>0.026327</td>\n",
              "      <td>-0.059772</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.706834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>before</th>\n",
              "      <td>-0.001115</td>\n",
              "      <td>0.014441</td>\n",
              "      <td>-0.188344</td>\n",
              "      <td>0.706834</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1bc5825b-7588-453c-9194-dee0f0b84230')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1bc5825b-7588-453c-9194-dee0f0b84230 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1bc5825b-7588-453c-9194-dee0f0b84230');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-05493399-ae1b-44ed-9385-5617c4a98fb8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-05493399-ae1b-44ed-9385-5617c4a98fb8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-05493399-ae1b-44ed-9385-5617c4a98fb8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "w2          ____     _____    ______     after    before\n",
              "w1                                                      \n",
              "____    1.000000  0.100754  0.190783  0.033343 -0.001115\n",
              "_____   0.100754  1.000000  0.293079  0.026327  0.014441\n",
              "______  0.190783  0.293079  1.000000 -0.059772 -0.188344\n",
              "after   0.033343  0.026327 -0.059772  1.000000  0.706834\n",
              "before -0.001115  0.014441 -0.188344  0.706834  1.000000"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subset = ['after', 'before', words[3], words[4], words[5]]\n",
        "rows = []\n",
        "for w in subset:\n",
        "    for ww in subset:\n",
        "        rows.append(dict(w1=w, w2=ww, d=model.wv.similarity(w, ww)))\n",
        "import pandas\n",
        "df=pandas.DataFrame(rows)\n",
        "df\n",
        "df.pivot_table(index=\"w1\", columns='w2', values= \"d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6OX6cCJlPRF"
      },
      "source": [
        "Un autre exemple de pivot_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2vec de Google "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Essayer d'utilliser les résultats Word2Vect de Google (attention BD un peu grande), appliquer word2vect et comparer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG0qddd6lPRF",
        "outputId": "12c595d0-8e58-49cc-af33-b845f2f6ac6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
            "  \"class\": algorithms.Blowfish,\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# chemin par le chemin du .model\n",
        "model_path = './model/word2vec-google-news-300.model'\n",
        "\n",
        "# Charger le modèle\n",
        "model = KeyedVectors.load(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformation des avis en vecteurs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Définition de la fonction pour transformer les avis en vecteurs\n",
        "def review_to_vec(review, model):\n",
        "    words = review.split()\n",
        "    word_vecs = [model[word] for word in words if word in model.key_to_index]\n",
        "    if len(word_vecs) > 0:\n",
        "        return np.mean(word_vecs, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# Application de la transformation aux données d'entraînement et de test\n",
        "X_train_vec = X_train['Avis'].apply(lambda x: review_to_vec(x, model))\n",
        "X_test_vec = X_test['Avis'].apply(lambda x: review_to_vec(x, model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conversion des résultats en format utilisable par scikit-learn\n",
        "X_train_vec = np.vstack(X_train_vec.values)\n",
        "X_test_vec = np.vstack(X_test_vec.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2vec + Regression logistique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit du modèle de classification\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_vec, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L'exactitude du modèle sur l'ensemble de test est : 0.8336\n"
          ]
        }
      ],
      "source": [
        "# Évaluation du modèle\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"L'exactitude du modèle sur l'ensemble de test est : {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En appliquons une cross validation à  un modèle de régression logistique avec le modèle word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Précision pour chaque fold: [0.83075  0.835    0.836125 0.8295   0.836375]\n",
            "Précision moyenne: 0.83355\n",
            "Écart type de la précision: 0.0028620359885927274\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression()\n",
        "scores = cross_val_score(clf, X_train_vec, y_train, cv=5)  \n",
        "print(\"Précision pour chaque fold:\", scores)\n",
        "print(\"Précision moyenne:\", scores.mean())\n",
        "print(\"Écart type de la précision:\", scores.std())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2vec + Forêts aléatoires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L'exactitude du modèle sur l'ensemble de test est : 0.7934\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train_vec, y_train)\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "# Évaluation du modèle\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"L'exactitude du modèle sur l'ensemble de test est : {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2vec + Machines à vecteurs de support (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L'exactitude du modèle sur l'ensemble de test est : 0.8477\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "clf = SVC()\n",
        "clf.fit(X_train_vec, y_train)\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "# Évaluation du modèle\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"L'exactitude du modèle sur l'ensemble de test est : {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word2vec + Réseaux de neurones artificiels (ANN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L'exactitude du modèle sur l'ensemble de test est : 0.841\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tariq\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier()\n",
        "clf.fit(X_train_vec, y_train)\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "# Évaluation du modèle\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"L'exactitude du modèle sur l'ensemble de test est : {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Résumé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voici un résumé  des scores des différents modèles de classification appliqué à la base **movie_reviews** :\n",
        "\n",
        "| Modèles                                                 | score |\n",
        "|--------------------------------------------------------|-------------|\n",
        "| DecisionTreeClassifier (arbre de décision)                  | 0.7291     |\n",
        "| Random Forest                 |0.8357                         |\n",
        "| Modèle linéaire (LogisticRegression)                   | 0.8379      |\n",
        "| Evaluation : ROC de RF                | AUC = 0.8379     |\n",
        "| Multinomial Bayesian (MNB)                             | 0.8338      |\n",
        "| RandomForest                                           | 0.8396      |\n",
        "| RandomForestClassifier (n_estimators=150, max_depth=90)| 0.8461      |\n",
        "| RandomForestClassifier (n_estimators=100, max_depth=70)| 0.8471      |\n",
        "| Gradient Boost | 0.5688     |\n",
        "| SVM linéaire avec OneVsRestClassifier                  | 0.8968      |\n",
        "| LogisticRegression avec données lemmatisées (bi-gramme)| 0.8987      |\n",
        "| LogisticRegression avec Cross-validation  les données de base            | 0.9162      |\n",
        "| MultinomialNB sur les données de base                  | 0.8862      |\n",
        "| MNB avec Validation Croisée (XV)                       | 0.8868      |\n",
        "| RandomForestClassifier avec SVD                        | 0.7139      |\n",
        "| LogisticRegression avec SVD                            | 0.7944      |\n",
        "| TF-IDF -> SVD -> LogisticRegression                    | 0.8769      |\n",
        "| LogisticRegression sur word2vect  (model gensim)                     | 0.6219      |\n",
        "| LogisticRegression sur word2vect (modele pré-entrainé de google)    | 0.8336      |\n",
        "| RandomForest sur word2vect (modele pré-entrainé de google)    | 0.7934      |\n",
        "| SVM sur word2vect (modele pré-entrainé de google)    | 0.8477      |\n",
        "| MLPClassifier sur word2vect (modele pré-entrainé de google)    | 0.841      |\n",
        "\n",
        "Nous remarquons que :\n",
        "\n",
        "| Description          | Modèle                                     | Performance |\n",
        "|----------------------|--------------------------------------------|-------------|\n",
        "| Performance maximale | LogisticRegression avec Cross-validation les données de base   | **0.9162**      |\n",
        "| Performance minimale | LogisticRegression sur word2vect (model gensim)          | **0.6219**      |\n",
        "\n",
        "\n",
        "Voici quelques observations et commentaires sur les résultats :\n",
        "\n",
        "- **Haute performance avec la régression logistique :** Il est remarquable que la régression logistique, en particulier avec la validation croisée et les données de base, obtienne les meilleures performances (0.9162). Cela souligne l'efficacité de la régression logistique pour les tâches de classification textuelle, surtout lorsqu'elle est combinée avec des techniques de validation robustes.\n",
        "\n",
        "- **L'importance de la validation croisée :** L'amélioration notable des scores lors de l'utilisation de la validation croisée (cross-validation) indique son importance pour obtenir une estimation plus fiable et robuste de la performance du modèle.\n",
        "\n",
        "- **Les modèles basés sur des arbres :** Les Random Forests montrent de bonnes performances, en particulier avec des hyperparamètres ajustés (n_estimators et max_depth). Cela illustre l'importance de l'ajustement des hyperparamètres dans l'amélioration des performances des modèles.\n",
        "\n",
        "- **Le SVM linéaire se démarque :** Le SVM linéaire avec OneVsRestClassifier obtient un score élevé (0.8968), ce qui met en évidence sa capacité à gérer des tâches de classification multiclasse efficacement.\n",
        "\n",
        "- **Difficultés avec Gradient Boosting :** Le Gradient Boosting a montré une performance relativement faible (0.5688). Cela peut indiquer que le modèle est soit surajusté, soit sous-ajusté, ou que l'ensemble de données n'est pas adapté pour ce type de modèle dans sa forme actuelle.\n",
        "\n",
        "- **Word2Vec et performances :** L'utilisation de word2vec (en particulier le modèle pré-entraîné de Google) a donné des résultats mixtes. Bien que meilleure que le modèle généré avec Gensim, sa performance reste inférieure à celle de la régression logistique avec des données lemmatisées ou des features traditionnels comme TF-IDF. Cela peut suggérer que pour cette tâche spécifique, les caractéristiques contextuelles capturées par Word2Vec ne sont pas aussi cruciales que les caractéristiques sémantiques ou syntaxiques plus directes.\n",
        "\n",
        "- **Performance minimale :** La performance la plus faible obtenue avec la régression logistique sur les vecteurs Word2Vec (0.6219) suggère que le modèle n'a pas réussi à capturer suffisamment bien les nuances sémantiques ou que le processus de vectorisation n'a pas été optimal.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "toc-showcode": false
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
